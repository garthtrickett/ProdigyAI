{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 0, 3)"
     ]
    }
   ],
   "source": [
    "print(\"script initiated\")\n",
    "import time\n",
    "very_start = time.time()\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import tables as tb\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from multiprocessing import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)  # don't use scientific notati\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "# from hanging_threads import start_monitoring\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler, StandardScaler\n",
    "import keras\n",
    "import third_party_libraries.finance_ml\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab as ml\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab.labeling.labeling as labeling\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab.sample_weights.attribution as attribution\n",
    "import third_party_libraries.snippets as snp\n",
    "from third_party_libraries.finance_ml.stats.vol import *\n",
    "\n",
    "from library.core import *\n",
    "# monitoring_thread = start_monitoring(seconds_frozen=360, test_interval=100)\n",
    "# import googlecloudprofiler\n",
    "# try:\n",
    "#     googlecloudprofiler.start(\n",
    "#         service=\"preemp-cpu-big-full-jeff_in-max\",\n",
    "#         # verbose is the logging level. 0-error, 1-warning, 2-info,\n",
    "#         # 3-debug. It defaults to 0 (error) if not set.\n",
    "#         verbose=3,\n",
    "#     )\n",
    "# except (ValueError, NotImplementedError) as exc:\n",
    "#     print(exc)  # Handle errors here\n",
    "\n",
    "arg_parse_stage = None\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--is_finished\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    args = parser.parse_args()\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "try:\n",
    "    with open(path_adjust + \"temp/data_name_gpu.txt\", \"r\") as text_file:\n",
    "        gpu_file_name = text_file.read()\n",
    "        stage = 2\n",
    "except:\n",
    "    stage = 1\n",
    "\n",
    "side = None\n",
    "\n",
    "with open(path_adjust + \"temp/is_script_finished.txt\", \"w+\") as text_file:\n",
    "    text_file.write(\"start_script_time\" + str(very_start))\n",
    "\n",
    "if arg_parse_stage == 1:\n",
    "    stage = int(args.stage)\n",
    "\n",
    "print(\"the stage\" + str(stage))\n",
    "# Overide model and stage for testing\n",
    "\n",
    "model = \"two_model\"\n",
    "stage = 1\n",
    "print(\"the overidden stage\" + str(stage))\n",
    "\n",
    "if stage == 2:\n",
    "    # size\n",
    "    h5f = h5py.File(\"data/gpu_output/\" + gpu_file_name + \".h5\", \"r\")\n",
    "    X = h5f[\"X\"][:]\n",
    "    P = h5f[\"P\"][:]\n",
    "    sample_weights = h5f[\"sample_weights\"][:]\n",
    "    sampled_idx_epoch = h5f[\"sampled_idx_epoch\"][:]\n",
    "    h5f.close()\n",
    "    data = pq.read_pandas(\"data/gpu_output/\" + gpu_file_name +\n",
    "                          \"_data.parquet\").to_pandas()\n",
    "    X_for_all_labels = data.dropna(subset=[\"bins\"])\n",
    "    sampled_idx = pd.DatetimeIndex(sampled_idx_epoch)\n",
    "    X_for_all_labels[\"predicted_bins\"] = P\n",
    "    side = X_for_all_labels[\"predicted_bins\"]\n",
    "    # Could use the probabilities (instead of [1,0,0] use [0.2,0.55,0.25]\n",
    "\n",
    "yaml_path = path_adjust + \"yaml/preprocessing.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "yaml_path = path_adjust + \"yaml/preprocessing.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yaml': '../yaml/preprocessing.yaml',\n",
      " 'params': {'window_length': {'desc': 'Length of the input feature window',\n",
      "   'value': 200},\n",
      "  'profit_taking_multiplier': {'desc': 'Vertical Barrier profit taking multiplier',\n",
      "   'value': 1},\n",
      "  'stop_loss_multiplier': {'desc': 'Vertical Barrier profit taking multiplier',\n",
      "   'value': 1},\n",
      "  'minimum_return': {'desc': 'Amount of return chosen to consider it a profitable trade',\n",
      "   'value': '0.001 * 1 / 23'}}}"
     ]
    }
   ],
   "source": [
    "config_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dictionary.params.minimum_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yaml': '../yaml/preprocessing.yaml',\n",
      " 'params': {'window_length': {'desc': 'Length of the input feature window',\n",
      "   'value': 200},\n",
      "  'profit_taking_multiplier': {'desc': 'Vertical Barrier profit taking multiplier',\n",
      "   'value': 1},\n",
      "  'stop_loss_multiplier': {'desc': 'Vertical Barrier profit taking multiplier',\n",
      "   'value': 1},\n",
      "  'minimum_return': {'desc': 'Amount of return chosen to consider it a profitable trade',\n",
      "   'value': '0.001 * 1 / 23'}}}"
     ]
    }
   ],
   "source": [
    "config_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'desc': 'Amount of return chosen to consider it a profitable trade',\n",
      " 'value': '0.001 * 1 / 23'}"
     ]
    }
   ],
   "source": [
    "config_dictionary['params']['minimum_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dictionary['params']['minimum_return'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    dir=\"~/ProdigyAI/\",\n",
    "    project=\"prodigyai\",\n",
    "    config=config_dictionary,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/garthtrickett/prodigyai/runs/2mak5z83"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    dir=\"~/ProdigyAI/\",\n",
    "    project=\"prodigyai\",\n",
    "    config=config_dictionary,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<wandb.wandb_config.Config at 0x7f97840ba510>"
     ]
    }
   ],
   "source": [
    "wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.minimum_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__',\n",
      " '__delattr__',\n",
      " '__dict__',\n",
      " '__dir__',\n",
      " '__doc__',\n",
      " '__eq__',\n",
      " '__format__',\n",
      " '__ge__',\n",
      " '__getattr__',\n",
      " '__getattribute__',\n",
      " '__getitem__',\n",
      " '__gt__',\n",
      " '__hash__',\n",
      " '__init__',\n",
      " '__init_subclass__',\n",
      " '__le__',\n",
      " '__lt__',\n",
      " '__module__',\n",
      " '__ne__',\n",
      " '__new__',\n",
      " '__reduce__',\n",
      " '__reduce_ex__',\n",
      " '__repr__',\n",
      " '__setattr__',\n",
      " '__setitem__',\n",
      " '__sizeof__',\n",
      " '__str__',\n",
      " '__subclasshook__',\n",
      " '__weakref__',\n",
      " '_config_path',\n",
      " '_descriptions',\n",
      " '_items',\n",
      " '_load_defaults',\n",
      " '_load_file',\n",
      " '_load_values',\n",
      " '_load_wandb',\n",
      " '_run_dir',\n",
      " '_sanitize',\n",
      " '_sanitize_val',\n",
      " '_set_wandb',\n",
      " '_telemetry_update',\n",
      " '_update',\n",
      " '_wandb_dir',\n",
      " 'as_dict',\n",
      " 'desc',\n",
      " 'from_environment_or_defaults',\n",
      " 'get',\n",
      " 'keys',\n",
      " 'load_json',\n",
      " 'persist',\n",
      " 'set_run_dir',\n",
      " 'setdefault',\n",
      " 'setdefaults',\n",
      " 'update',\n",
      " 'user_items']"
     ]
    }
   ],
   "source": [
    "dir(wandb.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'window_length': {'desc': 'Length of the input feature window', 'value': 200},\n",
      " 'profit_taking_multiplier': {'desc': 'Vertical Barrier profit taking multiplier',\n",
      "  'value': 1},\n",
      " 'stop_loss_multiplier': {'desc': 'Vertical Barrier profit taking multiplier',\n",
      "  'value': 1},\n",
      " 'minimum_return': {'desc': 'Amount of return chosen to consider it a profitable trade',\n",
      "  'value': '0.001 * 1 / 23'}}"
     ]
    }
   ],
   "source": [
    "wandb.config.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.params.minimum_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.params.minimum_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'desc': 'Amount of return chosen to consider it a profitable trade',\n",
      " 'value': '0.001 * 1 / 23'}"
     ]
    }
   ],
   "source": [
    "wandb.config.params['minimum_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.window_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/garthtrickett/prodigyai/runs/1vud8k2s"
     ]
    }
   ],
   "source": [
    "yaml_path = path_adjust + \"yaml/tabl.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "wandb.init(\n",
    "    dir=\"~/ProdigyAI/\",\n",
    "    project=\"prodigyai\",\n",
    "    config=config_dictionary,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': {'desc': 'Number of epochs to train over', 'value': 100},\n",
      " 'batch_size': {'desc': 'Size of each mini-batch', 'value': 32}}"
     ]
    }
   ],
   "source": [
    "wandb.config.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.params.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32"
     ]
    }
   ],
   "source": [
    "wandb.config.params['batch_size']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dictionary['params']['minimum_return']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dictionary['params']['minimum_return']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': {'desc': 'Number of epochs to train over', 'value': 100},\n",
      " 'batch_size': {'desc': 'Size of each mini-batch', 'value': 32}}"
     ]
    }
   ],
   "source": [
    "wandb.config['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'window_length': {'desc': 'Length of the input feature window', 'value': 200},\n",
      " 'profit_taking_multiplier': {'desc': 'Vertical Barrier profit taking multiplier',\n",
      "  'value': 1},\n",
      " 'stop_loss_multiplier': {'desc': 'Vertical Barrier profit taking multiplier',\n",
      "  'value': 1},\n",
      " 'minimum_return': {'desc': 'Amount of return chosen to consider it a profitable trade',\n",
      "  'value': '0.001 * 1 / 23'}}"
     ]
    }
   ],
   "source": [
    "yaml_path = path_adjust + \"yaml/preprocessing.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "wandb.init(\n",
    "    dir=\"~/ProdigyAI/\",\n",
    "    project=\"prodigyai\",\n",
    "    config=config_dictionary,\n",
    ")\n",
    "\n",
    "wandb.config['params']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

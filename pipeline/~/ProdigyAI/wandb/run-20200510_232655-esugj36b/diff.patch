diff --git a/pipeline/preprocessing.py b/pipeline/preprocessing.py
index 932ae0b..714d325 100644
--- a/pipeline/preprocessing.py
+++ b/pipeline/preprocessing.py
@@ -136,54 +136,67 @@ if stage == 2:
     side = X_for_all_labels["predicted_bins"]
     # Could use the probabilities (instead of [1,0,0] use [0.2,0.55,0.25]
 
+import yaml
+import wandb
+
+yaml_path = path_adjust + "yaml/preprocessing.yaml"
+with open(yaml_path) as file:
+    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
+
+config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
+
+wandb.init(
+    dir="~/ProdigyAI/",
+    project="prodigyai",
+    config=config_dictionary,
+)
+
+minimum_return = eval(wandb.config['params']['minimum_return']['value'])
+vertical_barrier_seconds = eval(
+    wandb.config['params']['vertical_barrier_seconds']['value'])
+
 # Parameters
 parameters = dict()
-model_arch = "TABL"
-parameters["arch"] = model_arch
-parameters["name"] = model
-parameters["WL"] = 200  # WINDOW LONG
-parameters["pt"] = 1
-parameters["sl"] = 1
-parameters["min_ret"] = 0.001 * 1 / 23
-parameters["vbs"] = round(1 / 2,
-                          3)  # Increasing this decreases vertical touches
-parameters["head"] = 1000  # take only first x number of rows 0 means of
-parameters["skip"] = 0  # sample every n'th row if skip > 0
-# get even classes at fraction = 0.8 so the training set is balanced then set to 1
-# 3 million rows is about the limit of before it starts taking ages / memory maxing
-parameters["vol_max"] = (
-    parameters["min_ret"] + 0.00000002
+wandb.config['params']['head'][
+    'value'] = 1000  # take only first x number of rows 0 means of
+volume_max = (
+    minimum_return + wandb.config['params']['vol_max_modifier']['value']
 )  # The higher this is the more an increase in volatility requries an increase
 # in return to be considered buy/sell (Increasing this increases end barrier vertical touches)
-parameters["vol_min"] = parameters["min_ret"] + 0.00000001
+volume_min = minimum_return + wandb.config['params']['vol_min_modifier'][
+    'value']
 
-parameters["filter"] = "none"
+filter_type = wandb.config['params']['filter_type']['value']
 
-if parameters["filter"] == "cm":
-    parameters["cm_vol_mod"] = 500
+if filter_type == "cm":
+    cusum_filter_vol_modifier = wandb.config['params'][
+        'cusum_filter_volume_modifier']['value']
 else:
-    parameters["cm_vol_mod"] = 0
+    cusum_filter_vol_modifier = 0
 
-parameters["sw"] = "on"  # sample weights
-parameters["fd"] = "off"
+use_sample_weights = wandb.config['params']['use_sample_weights']['value']
+use_sample_weights = wandb.config['params']['use_fractional_differentiation'][
+    'value']
 
-parameters["input"] = "obook"
+input_type = wandb.config['params']['input_type']['value']
 
-parameters["ntb"] = True  # non time bars
+# parameters["ntb"] = True  # non time bars
 
-if parameters["ntb"] == True:
-    # Pick whether you want to add in the time since last bar input feature
-    # time since last bar column
-    parameters["tslbc"] = True  # time since last bar column
-else:
-    # Pick whether you want to add in the volume input feature
-    parameters["vbc"] = True  # volume bar column
+# if parameters["ntb"] == True:
+#     # Pick whether you want to add in the time since last bar input feature
+#     # time since last bar column
+#     parameters["tslbc"] = True  # time since last bar column
+# else:
+#     # Pick whether you want to add in the volume input feature
+#     parameters["vbc"] = True  # volume bar column
 
 # Create the txt file string
-parameter_string = "&".join("{}{}{}".format(key, "=", val)
-                            for key, val in parameters.items())
+parameter_string = wandb.run.id
 
-pt_sl = [parameters["pt"], parameters["sl"]]
+pt_sl = [
+    wandb.config['params']['profit_taking_multiplier']['value'],
+    wandb.config['params']['stop_loss_multiplier']['value']
+]
 cpus = cpu_count() - 1
 
 regenerate_features_and_labels = True
@@ -192,10 +205,10 @@ if regenerate_features_and_labels == True:
     if stage == 1:
         # Side
         print("starting data load")
-        head = parameters["head"]
+        head = wandb.config['params']['head']['value']
 
         # # read parquet file of dollar bars
-        if parameters["input"] == "bars":
+        if input_type == "bars":
             # Mlfinlab bars
             data = pq.read_pandas(
                 path_adjust + "data/bars/"
@@ -216,7 +229,7 @@ if regenerate_features_and_labels == True:
             data.index = pd.to_datetime(data.index, infer_datetime_format=True)
 
         # read parquet file of raw ticks
-        if parameters["input"] == "ticks":
+        if input_type == "ticks":
             data = pq.read_pandas(
                 path_adjust + "data/bars/" +
                 "btcusdt_agg_trades_raw_tick_data.parquet").to_pandas()
@@ -238,11 +251,7 @@ if regenerate_features_and_labels == True:
             # Should do something else than drop the duplicates (maybe trades doesnt have duplicate indexes rather than aggtrades)
             data = data.loc[~data.index.duplicated(keep="first")]
 
-            # skip most rows if this is > 1
-            if parameters["skip"] > 0:
-                data = data.iloc[::parameters["skip"], :]
-
-        if parameters["input"] == "obook":
+        if input_type == "orderbook":
             with open(path_adjust + "temp/orderbook_data_name.txt",
                       "r") as text_file:
                 orderbook_preprocessed_file_name = text_file.read()
@@ -275,7 +284,7 @@ if regenerate_features_and_labels == True:
         # duplicate_fast_search(data.index.duplicated())
 
         # Fractional differentiation
-        if parameters["fd"] == "on":
+        if use_sample_weights == "on":
             data_series = data["close"].to_frame()
             # # generate 100 points
             # nsample = 1000
@@ -313,16 +322,16 @@ if regenerate_features_and_labels == True:
 
         start = time.time()
         volatility_level_array = volatility_levels_numba(
-            np.ascontiguousarray(data.close.values), parameters["WL"])
+            np.ascontiguousarray(data.close.values),
+            wandb.config['params']['window_length']['value'])
         data["window_volatility_level"] = volatility_level_array
 
         # Should adjust the max value
         # To get more vertical touches we can
         # either increase vol_max or
         # decrease the window seconds
-        scaler = MinMaxScaler(
-            feature_range=(parameters["vol_min"],
-                           parameters["vol_max"]))  # normalization
+        scaler = MinMaxScaler(feature_range=(volume_min,
+                                             volume_max))  # normalization
 
         normed_window_volatility_level = scaler.fit_transform(
             data[["window_volatility_level"]])
@@ -338,7 +347,7 @@ if regenerate_features_and_labels == True:
         close_np_array, close_index_np_array = pandas_series_to_numba_ready_np_arrays(
             close_copy)
 
-        volatility_threshold = volatility_threshold * parameters["cm_vol_mod"]
+        volatility_threshold = volatility_threshold * cusum_filter_vol_modifier
         print("data_len = " + str(len(data)))
         start = time.time()
         sampled_idx = filter_events(
@@ -346,7 +355,7 @@ if regenerate_features_and_labels == True:
             close_np_array,
             close_index_np_array,
             volatility_threshold,
-            parameters["filter"],
+            filter_type,
         )
         print("sampled_idx_len = " + str(len(sampled_idx)))
         end = time.time()
@@ -356,7 +365,8 @@ if regenerate_features_and_labels == True:
         # size
         start = time.time()
         volatility_level_array = volatility_levels_numba(
-            data.close.values, parameters["WL"])
+            data.close.values,
+            wandb.config['params']['window_length']['value'])
         data["window_volatility_level"] = volatility_level_array
 
     # This code runs for both first and second stage preprocessing
@@ -364,7 +374,7 @@ if regenerate_features_and_labels == True:
     vertical_barrier_timestamps = ml.labeling.add_vertical_barrier(
         t_events=sampled_idx,
         close=data["close"],
-        num_seconds=parameters["vbs"])
+        num_seconds=vertical_barrier_seconds)
     end = time.time()
     print("vertical barrier" + str(end - start))
 
@@ -376,7 +386,7 @@ if regenerate_features_and_labels == True:
         t_events=sampled_idx,
         pt_sl=pt_sl,
         target=data["window_volatility_level"],
-        min_ret=parameters["min_ret"],
+        min_ret=minimum_return,
         num_threads=cpus * 2,
         vertical_barrier_times=vertical_barrier_timestamps,
         side_prediction=side,
@@ -533,26 +543,27 @@ if stage == 1:
     ### ORDERBOOK VOLUME DATA
     volumes_for_all_labels = volumes.loc[data.close.index]
 
-    ## TRADE DATA
-    input_features_trade = []
-    close_array = data.close.values
-    input_features_trade.append(close_array)
+    # ## TRADE DATA
+    # input_features_trade = []
+    # close_array = data.close.values
+    # input_features_trade.append(close_array)
 
-    if parameters["ntb"] == False and parameters["vbc"] == True:
-        volume_array = data.volume.values
-        input_features_trade.append(volume_array)
-    if parameters["ntb"] == True and parameters["tslbc"] == True:
-        time_since_last_bar_array = data.time_since_last_bar.values
-        input_features_trade.append(time_since_last_bar_array)
+    # if parameters["ntb"] == False and parameters["vbc"] == True:
+    #     volume_array = data.volume.values
+    #     input_features_trade.append(volume_array)
+    # if parameters["ntb"] == True and parameters["tslbc"] == True:
+    #     time_since_last_bar_array = data.time_since_last_bar.values
+    #     input_features_trade.append(time_since_last_bar_array)
 
     end_time = time.time()
     print(end_time - start_time)
 
-    # min max limits
+    # Type of scaling to apply
+    scaling_type = wandb.config['params']['scaling_type']['value']
 
-    minimum = -1
-    maximum = 1
-    scaling_type = "z_score"
+    # min max limits
+    minimum = wandb.config['params']['scaling_maximum']['value']
+    maximum = wandb.config['params']['scaling_minimum']['value']
 
     ### Split intothe training/validation/test sets
     print("splitting into train/va/test sets start")
@@ -573,11 +584,13 @@ if stage == 1:
 
     volumes_for_all_labels_train = volumes_for_all_labels.iloc[
         train_close_array_integer_index[0] -
-        parameters["WL"]:train_close_array_integer_index[-1] + 2]
+        wandb.config['params']['window_length']['value']:
+        train_close_array_integer_index[-1] + 2]
 
     close_index_array_train = close_index_array[
         train_close_array_integer_index[0] -
-        parameters["WL"]:train_close_array_integer_index[-1] + 2]
+        wandb.config['params']['window_length']['value']:
+        train_close_array_integer_index[-1] + 2]
 
     end_time = time.time()
 
@@ -623,7 +636,7 @@ if stage == 1:
     # print("Make window started")
     # start_time = time.time()
 
-    # padding = parameters["WL"] * 2
+    # padding = wandb.config['params']['window_length']['value'] * 2
 
     # split_by = 100000
 
@@ -652,7 +665,7 @@ if stage == 1:
     #         len(prices_for_window_index_array_train[start_index:end_index]),
     #         input_features_normalized_train[:, close_and_input_start_index:
     #                                         close_and_input_end_index],
-    #         parameters["WL"],
+    #         wandb.config['params']['window_length']['value'],
     #         model_arch,
     #     )
 
@@ -697,11 +710,13 @@ if stage == 1:
 
     volumes_for_all_labels_val = volumes_for_all_labels.iloc[
         val_close_array_integer_index[0] -
-        parameters["WL"]:val_close_array_integer_index[-1] + 2]
+        wandb.config['params']['window_length']['value']:
+        val_close_array_integer_index[-1] + 2]
 
     close_index_array_val = close_index_array[
         val_close_array_integer_index[0] -
-        parameters["WL"]:val_close_array_integer_index[-1] + 2]
+        wandb.config['params']['window_length']['value']:
+        val_close_array_integer_index[-1] + 2]
 
     input_features_val = make_input_features_from_orderbook_data(
         volumes_for_all_labels_val)
@@ -721,7 +736,7 @@ if stage == 1:
     #     #     prices_for_window_index_array_val,
     #     #     close_index_array_val,
     #     #     input_features_normalized_val,
-    #     #     parameters["WL"],
+    #     #     wandb.config['params']['window_length']['value'],
     #     #     model_arch,
     #     # )
 
@@ -736,11 +751,13 @@ if stage == 1:
 
     volumes_for_all_labels_test = volumes_for_all_labels.iloc[
         test_close_array_integer_index[0] -
-        parameters["WL"]:test_close_array_integer_index[-1] + 2]
+        wandb.config['params']['window_length']['value']:
+        test_close_array_integer_index[-1] + 2]
 
     close_index_array_test = close_index_array[
         test_close_array_integer_index[0] -
-        parameters["WL"]:test_close_array_integer_index[-1] + 2]
+        wandb.config['params']['window_length']['value']:
+        test_close_array_integer_index[-1] + 2]
 
     input_features_test = make_input_features_from_orderbook_data(
         volumes_for_all_labels_test)
@@ -761,7 +778,7 @@ if stage == 1:
 #     #     prices_for_window_index_array_test,
 #     #     close_index_array_test,
 #     #     input_features_normalized_test,
-#     #     parameters["WL"],
+#     #     wandb.config['params']['window_length']['value'],
 #     #     model_arch,
 #     # )
 
@@ -782,8 +799,9 @@ ax = plt.gca()
 
 data.iloc[window_index - 200:window_index + 200].plot(y="close",
                                                       use_index=True)
-plot_window_and_touch_and_label(window_index, parameters["WL"], data,
-                                triple_barrier_events, labels)
+plot_window_and_touch_and_label(
+    window_index, wandb.config['params']['window_length']['value'], data,
+    triple_barrier_events, labels)
 
 data.iloc[window_index - 10:window_index + 30]
 
@@ -837,9 +855,9 @@ print("writing train/val/test to .h5 files finished taking " +
 #     h5f.create_dataset("P", data=P)
 # h5f.create_dataset("y", data=y)
 # h5f.create_dataset("sampled_idx_epoch", data=sampled_idx_epoch)
-# if parameters["sw"] == "on":
+# if use_sample_weights == "on":
 #     h5f.create_dataset("sample_weights", data=sample_weights)
-# elif parameters["sw"] == "off":
+# elif use_sample_weights == "off":
 #     h5f.create_dataset("sample_weights", data=np.zeros(1))
 # h5f.close()
 
diff --git a/temp/cp_end.ckpt b/temp/cp_end.ckpt
index c1c1d9e..e682868 100644
Binary files a/temp/cp_end.ckpt and b/temp/cp_end.ckpt differ
diff --git a/temp/is_script_finished.txt b/temp/is_script_finished.txt
index 40ca0d3..7370f48 100644
--- a/temp/is_script_finished.txt
+++ b/temp/is_script_finished.txt
@@ -1 +1 @@
-full_script_time64.39192986488342
\ No newline at end of file
+start_script_time1589153215.1770627
\ No newline at end of file
Submodule third_party_libraries/TABL 7f25314..df3e92d:
diff --git a/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc
new file mode 100644
index 0000000..12f31e7
Binary files /dev/null and b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc differ
diff --git a/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc
new file mode 100644
index 0000000..d8950e3
Binary files /dev/null and b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc differ

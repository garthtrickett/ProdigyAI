{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 0, 3)"
     ]
    }
   ],
   "source": [
    "print(\"script initiated\")\n",
    "import time\n",
    "very_start = time.time()\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import tables as tb\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from multiprocessing import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)  # don't use scientific notati\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "# from hanging_threads import start_monitoring\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler, StandardScaler\n",
    "import keras\n",
    "import third_party_libraries.finance_ml\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab as ml\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab.labeling.labeling as labeling\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab.sample_weights.attribution as attribution\n",
    "import third_party_libraries.snippets as snp\n",
    "from third_party_libraries.finance_ml.stats.vol import *\n",
    "\n",
    "from library.core import *\n",
    "# monitoring_thread = start_monitoring(seconds_frozen=360, test_interval=100)\n",
    "# import googlecloudprofiler\n",
    "# try:\n",
    "#     googlecloudprofiler.start(\n",
    "#         service=\"preemp-cpu-big-full-jeff_in-max\",\n",
    "#         # verbose is the logging level. 0-error, 1-warning, 2-info,\n",
    "#         # 3-debug. It defaults to 0 (error) if not set.\n",
    "#         verbose=3,\n",
    "#     )\n",
    "# except (ValueError, NotImplementedError) as exc:\n",
    "#     print(exc)  # Handle errors here\n",
    "\n",
    "arg_parse_stage = None\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--is_finished\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    args = parser.parse_args()\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "try:\n",
    "    with open(path_adjust + \"temp/data_name_gpu.txt\", \"r\") as text_file:\n",
    "        gpu_file_name = text_file.read()\n",
    "        stage = 2\n",
    "except:\n",
    "    stage = 1\n",
    "\n",
    "side = None\n",
    "\n",
    "with open(path_adjust + \"temp/is_script_finished.txt\", \"w+\") as text_file:\n",
    "    text_file.write(\"start_script_time\" + str(very_start))\n",
    "\n",
    "if arg_parse_stage == 1:\n",
    "    stage = int(args.stage)\n",
    "\n",
    "print(\"the stage\" + str(stage))\n",
    "# Overide model and stage for testing\n",
    "\n",
    "model = \"two_model\"\n",
    "stage = 1\n",
    "print(\"the overidden stage\" + str(stage))\n",
    "\n",
    "if stage == 2:\n",
    "    # size\n",
    "    h5f = h5py.File(\"data/gpu_output/\" + gpu_file_name + \".h5\", \"r\")\n",
    "    X = h5f[\"X\"][:]\n",
    "    P = h5f[\"P\"][:]\n",
    "    sample_weights = h5f[\"sample_weights\"][:]\n",
    "    sampled_idx_epoch = h5f[\"sampled_idx_epoch\"][:]\n",
    "    h5f.close()\n",
    "    data = pq.read_pandas(\"data/gpu_output/\" + gpu_file_name +\n",
    "                          \"_data.parquet\").to_pandas()\n",
    "    X_for_all_labels = data.dropna(subset=[\"bins\"])\n",
    "    sampled_idx = pd.DatetimeIndex(sampled_idx_epoch)\n",
    "    X_for_all_labels[\"predicted_bins\"] = P\n",
    "    side = X_for_all_labels[\"predicted_bins\"]\n",
    "    # Could use the probabilities (instead of [1,0,0] use [0.2,0.55,0.25]\n",
    "\n",
    "import yaml\n",
    "import wandb\n",
    "yaml_path = path_adjust + \"yaml/preprocessing.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "wandb.init(\n",
    "    dir=\"~/ProdigyAI/\",\n",
    "    project=\"prodigyai\",\n",
    "    config=config_dictionary,\n",
    ")\n",
    "\n",
    "minimum_return = eval(wandb.config['params']['minimum_return']['value'])\n",
    "\n",
    "vertical_barrier_seconds = eval(\n",
    "    wandb.config['params']['vertical_barrier_seconds']['value'])\n",
    "# Parameters\n",
    "\n",
    "parameters = dict()\n",
    "\n",
    "wandb.config['params']['head'][\n",
    "    'value'] = 1000  # take only first x number of rows 0 means of\n",
    "\n",
    "volume_max = (\n",
    "    minimum_return + wandb.config['params']['vol_max_modifier']['value']\n",
    ")  # The higher this is the more an increase in volatility requries an increase\n",
    "# in return to be considered buy/sell (Increasing this increases end barrier vertical touches)\n",
    "\n",
    "volume_min = minimum_return + wandb.config['params']['vol_min_modifier'][\n",
    "    'value']\n",
    "\n",
    "filter_type = wandb.config['params']['filter_type']['value']\n",
    "\n",
    "if filter_type == \"cm\":\n",
    "    cusum_filter_vol_modifier = wandb.config['params'][\n",
    "        'cusum_filter_volume_modifier']['value']\n",
    "else:\n",
    "    cusum_filter_vol_modifier = 0\n",
    "\n",
    "use_sample_weights = wandb.config['params']['use_sample_weights']['value']\n",
    "\n",
    "use_sample_weights = wandb.config['params']['use_fractional_differentiation'][\n",
    "    'value']\n",
    "\n",
    "input_type = wandb.config['params']['input_type']['value']\n",
    "# parameters[\"ntb\"] = True  # non time bars\n",
    "# if parameters[\"ntb\"] == True:\n",
    "#     # Pick whether you want to add in the time since last bar input feature\n",
    "#     # time since last bar column\n",
    "#     parameters[\"tslbc\"] = True  # time since last bar column\n",
    "# else:\n",
    "#     # Pick whether you want to add in the volume input feature\n",
    "#     parameters[\"vbc\"] = True  # volume bar column\n",
    "# Create the txt file string\n",
    "\n",
    "parameter_string = \"&\".join(\"{}{}{}\".format(key, \"=\", val)\n",
    "                            for key, val in parameters.items())\n",
    "\n",
    "pt_sl = [\n",
    "    wandb.config['params']['profit_taking_multiplier']['value'],\n",
    "    wandb.config['params']['stop_loss_multiplier']['value']\n",
    "]\n",
    "\n",
    "cpus = cpu_count() - 1\n",
    "regenerate_features_and_labels = True\n",
    "\n",
    "if regenerate_features_and_labels == True:\n",
    "    # READ THE DATA\n",
    "    if stage == 1:\n",
    "        # Side\n",
    "        print(\"starting data load\")\n",
    "        head = wandb.config['params']['head']['value']\n",
    "        # # read parquet file of dollar bars\n",
    "        if input_type == \"bars\":\n",
    "            # Mlfinlab bars\n",
    "            data = pq.read_pandas(\n",
    "                path_adjust + \"data/bars/\"\n",
    "                \"btcusdt_agg_trades_50_volume_bars.parquet\").to_pandas()\n",
    "            data = data.drop(columns=[\n",
    "                \"open\",\n",
    "                \"high\",\n",
    "                \"low\",\n",
    "                # \"volume\",\n",
    "                \"seconds_since_last_bar\",\n",
    "            ])\n",
    "            # 1 min ohlcv ready made bars\n",
    "            # data = pq.read_pandas(\"data/bars/BTCUSDT_1m.parquet\").to_pandas()\n",
    "            # data[\"date_time\"] = pd.to_datetime(data[\"date_time\"], unit='ms')\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "            data = data.set_index(\"date_time\")\n",
    "            data.index = pd.to_datetime(data.index, infer_datetime_format=True)\n",
    "        # read parquet file of raw ticks\n",
    "        if input_type == \"ticks\":\n",
    "            data = pq.read_pandas(\n",
    "                path_adjust + \"data/bars/\" +\n",
    "                \"btcusdt_agg_trades_raw_tick_data.parquet\").to_pandas()\n",
    "            data = data.rename(columns={\n",
    "                \"date\": \"date_time\",\n",
    "                \"price\": \"close\",\n",
    "                \"volume\": \"volume\"\n",
    "            })\n",
    "            data = data.drop(columns=[\"volume\"])\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "            data = data.set_index(\"date_time\")\n",
    "            print(\"converting index to date_time\")\n",
    "            data.index = pd.to_datetime(data.index,\n",
    "                                        format=\"%m/%d/%Y %H:%M:%S.%f\")\n",
    "            print(\"index converted\")\n",
    "            # Should do something else than drop the duplicates (maybe trades doesnt have duplicate indexes rather than aggtrades)\n",
    "            data = data.loc[~data.index.duplicated(keep=\"first\")]\n",
    "        if input_type == \"orderbook\":\n",
    "            with open(path_adjust + \"temp/orderbook_data_name.txt\",\n",
    "                      \"r\") as text_file:\n",
    "                orderbook_preprocessed_file_name = text_file.read()\n",
    "            h5f = h5py.File(\n",
    "                path_adjust + \"data/orderbook_preprocessed/\" +\n",
    "                orderbook_preprocessed_file_name + \".h5\",\n",
    "                \"r\",\n",
    "            )\n",
    "            volumes_index_as_epoch = h5f[\"volumes_index_as_epoch\"][:]\n",
    "            volumes_np_array = h5f[\"volumes_np_array\"][:]\n",
    "            df_index_as_epoch = h5f[\"df_index_as_epoch\"][:]\n",
    "            df_np_array = h5f[\"df_np_array\"][:]\n",
    "            h5f.close()\n",
    "            volumes = pd.DataFrame(data=volumes_np_array,\n",
    "                                   index=volumes_index_as_epoch)\n",
    "            volumes.index = pd.to_datetime(volumes.index, unit=\"ms\")\n",
    "            data = pd.DataFrame(data=df_np_array, index=df_index_as_epoch)\n",
    "            data.index = pd.to_datetime(data.index, unit=\"ms\")\n",
    "            data.columns = [\"close\"]\n",
    "            data.index.name = \"date_time\"\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "        print(\"data load finished\")\n",
    "        # Checking for duplicates\n",
    "        # duplicate_fast_search(data.index.duplicated())\n",
    "        # Fractional differentiation\n",
    "        if use_sample_weights == \"on\":\n",
    "            data_series = data[\"close\"].to_frame()\n",
    "            # # generate 100 points\n",
    "            # nsample = 1000\n",
    "            # ## simulate a simple sinusoidal function\n",
    "            # x1 = np.linspace(0, 10, nsample)\n",
    "            # y = pd.Series(1*np.sin(2.0 * x1 + .5))\n",
    "            # y.plot()\n",
    "            # c_constant = 1.\n",
    "            # y_shifted = (y + c_constant).cumsum().rename('Shifted_series').to_frame()\n",
    "            # y_shifted.plot()\n",
    "            # df = y_shifted\n",
    "            # # df=(df-df.mean())/df.std()\n",
    "            # df['Shifted_series'][1:] = np.diff(df['Shifted_series'].values)\n",
    "            # df['Shifted_series'].plot()\n",
    "            kwargs = None\n",
    "            # data_series = np.log(data_series)  ## is it good to log this?\n",
    "            frac_diff_series, d = get_opt_d(  # reduces the number of rows and ends up with less vertical barriers touched\n",
    "                data_series,\n",
    "                ds=None,\n",
    "                maxlag=None,  # If we use raw tick data need at least head > 8000\n",
    "                thres=1e-5,\n",
    "                max_size=10000,\n",
    "                p_thres=1e-2,\n",
    "                autolag=None,\n",
    "                verbose=1,\n",
    "            )\n",
    "            data[\"close\"] = frac_diff_series\n",
    "            data = data.dropna(subset=[\"close\"])\n",
    "        data[\"window_volatility_level\"] = np.nan\n",
    "        start = time.time()\n",
    "        volatility_level_array = volatility_levels_numba(\n",
    "            np.ascontiguousarray(data.close.values),\n",
    "            wandb.config['params']['window_length']['value'])\n",
    "        data[\"window_volatility_level\"] = volatility_level_array\n",
    "        # Should adjust the max value\n",
    "        # To get more vertical touches we can\n",
    "        # either increase vol_max or\n",
    "        # decrease the window seconds\n",
    "        scaler = MinMaxScaler(feature_range=(volume_min,\n",
    "                                             volume_max))  # normalization\n",
    "        normed_window_volatility_level = scaler.fit_transform(\n",
    "            data[[\"window_volatility_level\"]])\n",
    "        data[\"window_volatility_level\"] = normed_window_volatility_level  #\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "        # CUSUM FILTER\n",
    "        volatility_threshold = data[\"window_volatility_level\"].mean()\n",
    "        close_copy = data.dropna().close.copy(deep=True)\n",
    "        close_np_array, close_index_np_array = pandas_series_to_numba_ready_np_arrays(\n",
    "            close_copy)\n",
    "        volatility_threshold = volatility_threshold * cusum_filter_vol_modifier\n",
    "        print(\"data_len = \" + str(len(data)))\n",
    "        start = time.time()\n",
    "        sampled_idx = filter_events(\n",
    "            data,\n",
    "            close_np_array,\n",
    "            close_index_np_array,\n",
    "            volatility_threshold,\n",
    "            filter_type,\n",
    "        )\n",
    "        print(\"sampled_idx_len = \" + str(len(sampled_idx)))\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "    if stage == 2:\n",
    "        # size\n",
    "        start = time.time()\n",
    "        volatility_level_array = volatility_levels_numba(\n",
    "            data.close.values,\n",
    "            wandb.config['params']['window_length']['value'])\n",
    "        data[\"window_volatility_level\"] = volatility_level_array\n",
    "    # This code runs for both first and second stage preprocessing\n",
    "    start = time.time()\n",
    "    vertical_barrier_timestamps = ml.labeling.add_vertical_barrier(\n",
    "        t_events=sampled_idx,\n",
    "        close=data[\"close\"],\n",
    "        num_seconds=vertical_barrier_seconds)\n",
    "    end = time.time()\n",
    "    print(\"vertical barrier\" + str(end - start))\n",
    "    start = time.time()\n",
    "    print(\"Getting triple barrier events\")\n",
    "    triple_barrier_events = ml.labeling.get_events(\n",
    "        close=data[\"close\"],\n",
    "        t_events=sampled_idx,\n",
    "        pt_sl=pt_sl,\n",
    "        target=data[\"window_volatility_level\"],\n",
    "        min_ret=minimum_return,\n",
    "        num_threads=cpus * 2,\n",
    "        vertical_barrier_times=vertical_barrier_timestamps,\n",
    "        side_prediction=side,\n",
    "        split_by=\n",
    "        100  # maybe we want this as large as we can while still fitting in ram\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"triple_barrier_events finished taking\" + str(end - start))\n",
    "    very_end = time.time()\n",
    "    with open(path_adjust + \"temp/is_script_finished.txt\", \"w+\") as text_file:\n",
    "        text_file.write(\"full_script_time\" + str(very_end - very_start))\n",
    "    start_time = time.time()\n",
    "    print(\"Returning Bins\")\n",
    "    labels = ml.labeling.get_bins(triple_barrier_events, data[\"close\"])\n",
    "    labels = ml.labeling.drop_labels(labels)\n",
    "    label_counts = labels.bin.value_counts()\n",
    "    print(\"label_counts\" + str(label_counts))\n",
    "    end_time = time.time()\n",
    "    print(\"returning bins finished taking\" + str(end_time - start_time))\n",
    "    # unique, counts = np.unique(y, return_counts=True)\n",
    "    sampled_idx_epoch = sampled_idx.astype(np.int64)\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/sampled_idx_epoch.h5\",\n",
    "        \"w\")\n",
    "    h5f.create_dataset(\"sampled_idx_epoch\", data=sampled_idx_epoch)\n",
    "    h5f.close()\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(labels)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/labels.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(data)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/data.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(triple_barrier_events)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/triple_barrier_events.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "else:\n",
    "    labels = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/labels.parquet\").to_pandas()\n",
    "    data = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/data.parquet\").to_pandas()\n",
    "    triple_barrier_events = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/triple_barrier_events.parquet\"\n",
    "    ).to_pandas()\n",
    "    with open(path_adjust + \"temp/orderbook_data_name.txt\", \"r\") as text_file:\n",
    "        orderbook_preprocessed_file_name = text_file.read()\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/orderbook_preprocessed/\" +\n",
    "        orderbook_preprocessed_file_name + \".h5\", \"r\")\n",
    "    volumes_index_as_epoch = h5f[\"volumes_index_as_epoch\"][:]\n",
    "    volumes_np_array = h5f[\"volumes_np_array\"][:]\n",
    "    h5f.close()\n",
    "    volumes = pd.DataFrame(data=volumes_np_array, index=volumes_index_as_epoch)\n",
    "    volumes.index = pd.to_datetime(volumes.index, unit=\"ms\")\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/sampled_idx_epoch.h5\",\n",
    "        \"r\")\n",
    "    sampled_idx_epoch = h5f[\"sampled_idx_epoch\"][:]\n",
    "    h5f.close()\n",
    "    sampled_idx = pd.DatetimeIndex(sampled_idx_epoch)\n",
    "\n",
    "y_dataframe = labels[\"bin\"]\n",
    "data[\"bins\"] = labels[\"bin\"]\n",
    "y = np.asarray(y_dataframe)\n",
    "end = time.time()\n",
    "y = keras.utils.to_categorical(y, num_classes=3)\n",
    "\n",
    "if stage == 1:\n",
    "    start_time = time.time()\n",
    "    # side\n",
    "    X_for_all_labels = data.loc[labels.index, :]\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    # ### FOR HIGHWAY RNN\n",
    "    # X = np.asarray(volumes.loc[labels.index, :])\n",
    "    # h5f = h5py.File(path_adjust + \"data/preprocessed/\" + parameter_string + \".h5\", \"w\")\n",
    "    # h5f.create_dataset(\"X\", data=X)\n",
    "    # h5f.create_dataset(\"y\", data=y)\n",
    "    # h5f.close()\n",
    "    # X = []\n",
    "    start_time = time.time()\n",
    "    prices_for_window = data.loc[X_for_all_labels.index]\n",
    "    prices_for_window_index = prices_for_window.index.astype(np.int64)\n",
    "    prices_for_window_index_array = np.asarray(prices_for_window_index)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    start_time = time.time()\n",
    "    close_index = data.close.index.astype(np.int64)\n",
    "    close_index_array = np.asarray(close_index)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    start_time = time.time()\n",
    "    # Make a new column time since last bar\n",
    "    unindexed_data = data.reset_index()\n",
    "    unindexed_data[\"shifted_date_time\"] = unindexed_data[\"date_time\"].shift(1)\n",
    "    unindexed_data[\"time_since_last_bar\"] = (unindexed_data[\"date_time\"].sub(\n",
    "        unindexed_data[\"shifted_date_time\"], axis=0).dt.seconds)\n",
    "    unindexed_data = unindexed_data.set_index(\"date_time\")\n",
    "    data[\"time_since_last_bar\"] = unindexed_data[\"time_since_last_bar\"]\n",
    "    data[\"time_since_last_bar\"].iloc[0] = 0\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    start_time = time.time()\n",
    "    ### ORDERBOOK VOLUME DATA\n",
    "    volumes_for_all_labels = volumes.loc[data.close.index]\n",
    "    # ## TRADE DATA\n",
    "    # input_features_trade = []\n",
    "    # close_array = data.close.values\n",
    "    # input_features_trade.append(close_array)\n",
    "    # if parameters[\"ntb\"] == False and parameters[\"vbc\"] == True:\n",
    "    #     volume_array = data.volume.values\n",
    "    #     input_features_trade.append(volume_array)\n",
    "    # if parameters[\"ntb\"] == True and parameters[\"tslbc\"] == True:\n",
    "    #     time_since_last_bar_array = data.time_since_last_bar.values\n",
    "    #     input_features_trade.append(time_since_last_bar_array)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    # Type of scaling to apply\n",
    "    scaling_type = wandb.config['params']['scaling_type']['value']\n",
    "    # min max limits\n",
    "    minimum = wandb.config['params']['scaling_maximum']['value']\n",
    "    maximum = wandb.config['params']['scaling_minimum']['value']\n",
    "    ### Split intothe training/validation/test sets\n",
    "    print(\"splitting into train/va/test sets start\")\n",
    "    start_time = time.time()\n",
    "    prices_for_window_index_array_train_and_val = prices_for_window_index_array[:round(\n",
    "        len(prices_for_window_index_array) * 0.8)]\n",
    "    y_train_and_val = y[:round(len(y) * 0.8)]\n",
    "    prices_for_window_index_array_train = prices_for_window_index_array_train_and_val[:round(\n",
    "        len(prices_for_window_index_array_train_and_val) * 0.8)]\n",
    "    y_train = y_train_and_val[:round(len(y_train_and_val) * 0.8)]\n",
    "    train_close_array_integer_index = np.nonzero(\n",
    "        np.in1d(close_index_array, prices_for_window_index_array_train))[0]\n",
    "    volumes_for_all_labels_train = volumes_for_all_labels.iloc[\n",
    "        train_close_array_integer_index[0] -\n",
    "        wandb.config['params']['window_length']['value']:\n",
    "        train_close_array_integer_index[-1] + 2]\n",
    "    close_index_array_train = close_index_array[\n",
    "        train_close_array_integer_index[0] -\n",
    "        wandb.config['params']['window_length']['value']:\n",
    "        train_close_array_integer_index[-1] + 2]\n",
    "    end_time = time.time()\n",
    "    print(\"splitting into train/va/test sets finished\" +\n",
    "          str(end_time - start_time))\n",
    "    print(\"Make input features from orderbook data started\")\n",
    "    start_time = time.time()\n",
    "    # MAKE WINDOW FROM INPUTS\n",
    "    input_features_train = make_input_features_from_orderbook_data(\n",
    "        volumes_for_all_labels_train)\n",
    "    end_time = time.time()\n",
    "    print(\"Make input features from orderbook data started \" +\n",
    "          str(end_time - start_time))\n",
    "    print(\"Get train scalers started\")\n",
    "    start_time = time.time()\n",
    "    ### 2: Apply the normalisation based on the training fit scalars (-1,1 vs 0,1 min max test both)\n",
    "    maxes_or_means_np_array_train, mins_or_stds_np_array_train = get_fit_scalars(\n",
    "        scaling_type, input_features_train)\n",
    "    end_time = time.time()\n",
    "    print(\"Get train scalers finished\" + str(end_time - start_time))\n",
    "    print(\"Norm train data started\")\n",
    "    start_time = time.time()\n",
    "    # Norm train\n",
    "    input_features_normalized_train = scale_input_features(\n",
    "        scaling_type,\n",
    "        maxes_or_means_np_array_train,\n",
    "        mins_or_stds_np_array_train,\n",
    "        input_features_train,\n",
    "        minimum,\n",
    "        maximum,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(\"Get train scalers finished\" + str(end_time - start_time))\n",
    "    # print(\"Make window started\")\n",
    "    # start_time = time.time()\n",
    "    # padding = wandb.config['params']['window_length']['value'] * 2\n",
    "    # split_by = 100000\n",
    "    # number_of_splits = len(prices_for_window_index_array_train) // split_by\n",
    "    # for i in range(number_of_splits):\n",
    "    #     print(i)\n",
    "    #     start_time = time.time()\n",
    "    #     if i == 0:\n",
    "    #         start_index = None  # 0\n",
    "    #         end_index = (i + 1) * split_by\n",
    "    #         close_and_input_start_index = start_index\n",
    "    #         close_and_input_end_index = end_index + (padding)\n",
    "    #     elif i < number_of_splits - 1:\n",
    "    #         start_index = i * split_by\n",
    "    #         end_index = (i + 1) * split_by\n",
    "    #         close_and_input_start_index = start_index - (padding)\n",
    "    #         close_and_input_end_index = end_index + (padding)\n",
    "    #     elif i == number_of_splits - 1:\n",
    "    #         start_index = i * split_by\n",
    "    #         end_index = None  # -1\n",
    "    #         close_and_input_start_index = start_index - (padding)\n",
    "    #         close_and_input_end_index = end_index\n",
    "    #     # Window train\n",
    "    #     X_train_section = make_window_multivariate_numba(\n",
    "    #         len(prices_for_window_index_array_train[start_index:end_index]),\n",
    "    #         input_features_normalized_train[:, close_and_input_start_index:\n",
    "    #                                         close_and_input_end_index],\n",
    "    #         wandb.config['params']['window_length']['value'],\n",
    "    #         model_arch,\n",
    "    #     )\n",
    "    #     print(X_train_section.shape)\n",
    "    #     hdf5_epath = path_adjust + \"data/preprocessed/X_and_y.h5\"\n",
    "    #     if os.path.exists(hdf5_epath) == False or i == 0:\n",
    "    #         h5f = tb.open_file(hdf5_epath, mode=\"a\")\n",
    "    #         dataGroup = h5f.create_group(h5f.root, \"MyData\")\n",
    "    #         h5f.create_earray(dataGroup,\n",
    "    #                           \"X_train_section\",\n",
    "    #                           obj=X_train_section)\n",
    "    #         h5f.close()\n",
    "    #     else:\n",
    "    #         h5f = tb.open_file(hdf5_epath, mode=\"r+\")\n",
    "    #         h5f.root.MyData.X_train_section.append(X_train_section)\n",
    "    #         h5f.close()\n",
    "    #     # h5f = h5py.File(path_adjust + \"data/preprocessed/X_and_y.h5\", \"w\")\n",
    "    #     # h5f.create_dataset(\"X_train_section\", data=X_train_section)\n",
    "    #     # h5f.close()\n",
    "    #     end_time = time.time()\n",
    "    #     print(str(i) + \" finished taking \" + str(end_time - start_time))\n",
    "    # end_time = time.time()\n",
    "    # print(\"Make window finished\" + str(end_time - start_time))\n",
    "    #     # import pdb\n",
    "    #     # pdb.set_trace()\n",
    "    # Val\n",
    "    prices_for_window_index_array_val = prices_for_window_index_array_train_and_val[\n",
    "        round(len(prices_for_window_index_array_train_and_val) * 0.8):]\n",
    "    y_val = y_train_and_val[round(len(y_train_and_val) * 0.8):]\n",
    "    val_close_array_integer_index = np.nonzero(\n",
    "        np.in1d(close_index_array, prices_for_window_index_array_val))[0]\n",
    "    volumes_for_all_labels_val = volumes_for_all_labels.iloc[\n",
    "        val_close_array_integer_index[0] -\n",
    "        wandb.config['params']['window_length']['value']:\n",
    "        val_close_array_integer_index[-1] + 2]\n",
    "    close_index_array_val = close_index_array[\n",
    "        val_close_array_integer_index[0] -\n",
    "        wandb.config['params']['window_length']['value']:\n",
    "        val_close_array_integer_index[-1] + 2]\n",
    "    input_features_val = make_input_features_from_orderbook_data(\n",
    "        volumes_for_all_labels_val)\n",
    "    # Norm val\n",
    "    input_features_normalized_val = scale_input_features(\n",
    "        scaling_type,\n",
    "        maxes_or_means_np_array_train,\n",
    "        mins_or_stds_np_array_train,\n",
    "        input_features_val,\n",
    "        minimum,\n",
    "        maximum,\n",
    "    )\n",
    "    #     # # Make window val\n",
    "    #     # X_val = make_window_multivariate_numba(\n",
    "    #     #     prices_for_window_index_array_val,\n",
    "    #     #     close_index_array_val,\n",
    "    #     #     input_features_normalized_val,\n",
    "    #     #     wandb.config['params']['window_length']['value'],\n",
    "    #     #     model_arch,\n",
    "    #     # )\n",
    "    # Test\n",
    "    prices_for_window_index_array_test = prices_for_window_index_array[\n",
    "        round(len(prices_for_window_index_array) * 0.8):]\n",
    "    y_test = y[round(len(y) * 0.8):]\n",
    "    test_close_array_integer_index = np.nonzero(\n",
    "        np.in1d(close_index_array, prices_for_window_index_array_test))[0]\n",
    "    volumes_for_all_labels_test = volumes_for_all_labels.iloc[\n",
    "        test_close_array_integer_index[0] -\n",
    "        wandb.config['params']['window_length']['value']:\n",
    "        test_close_array_integer_index[-1] + 2]\n",
    "    close_index_array_test = close_index_array[\n",
    "        test_close_array_integer_index[0] -\n",
    "        wandb.config['params']['window_length']['value']:\n",
    "        test_close_array_integer_index[-1] + 2]\n",
    "    input_features_test = make_input_features_from_orderbook_data(\n",
    "        volumes_for_all_labels_test)\n",
    "    # Norm test\n",
    "    input_features_normalized_test = scale_input_features(\n",
    "        scaling_type,\n",
    "        maxes_or_means_np_array_train,\n",
    "        mins_or_stds_np_array_train,\n",
    "        input_features_test,\n",
    "        minimum,\n",
    "        maximum,\n",
    "    )\n",
    "#     # # Window test\n",
    "#     # # TABL\n",
    "#     # X_test = make_window_multivariate_numba(\n",
    "#     #     prices_for_window_index_array_test,\n",
    "#     #     close_index_array_test,\n",
    "#     #     input_features_normalized_test,\n",
    "#     #     wandb.config['params']['window_length']['value'],\n",
    "#     #     model_arch,\n",
    "#     # )\n",
    "#     start = time.time()\n",
    "#     end = time.time()\n",
    "#     print(\"numba make window time\" + str(end - start))\n",
    "#     start = time.time()\n",
    "\n",
    "print(\"plotting started\")\n",
    "# plot the whole sequence\n",
    "\n",
    "ax = plt.gca()\n",
    "data.plot(y=\"close\", use_index=True)\n",
    "window_index = 500\n",
    "ax = plt.gca()\n",
    "\n",
    "data.iloc[window_index - 200:window_index + 200].plot(y=\"close\",\n",
    "                                                      use_index=True)\n",
    "\n",
    "plot_window_and_touch_and_label(\n",
    "    window_index, wandb.config['params']['window_length']['value'], data,\n",
    "    triple_barrier_events, labels)\n",
    "\n",
    "data.iloc[window_index - 10:window_index + 30]\n",
    "\n",
    "print(\"plotting finished\")\n",
    "# Sample Weights\n",
    "# if stage == 1:\n",
    "#     weights = attribution.get_weights_by_return(triple_barrier_events.dropna(),\n",
    "#                                                 data.close,\n",
    "#                                                 num_threads=5)\n",
    "#     sample_weights = np.asarray(weights)\n",
    "#     sample_weights = sample_weights.reshape(len(sample_weights))\n",
    "#     sampled_idx_epoch = sampled_idx.astype(np.int64)  #\n",
    "\n",
    "if stage == 2:\n",
    "    # size\n",
    "    parameter_string = parameter_string + \"second_stage\"\n",
    "\n",
    "print(\"writing train/val/test to .h5 files starting\")\n",
    "\n",
    "start = time.time()\n",
    "# Writing preprocessed X,y\n",
    "\n",
    "h5f = h5py.File(path_adjust + \"data/preprocessed/\" + parameter_string + \".h5\",\n",
    "                \"w\")\n",
    "\n",
    "h5f.create_dataset(\"prices_for_window_index_array_train\",\n",
    "                   data=prices_for_window_index_array_train)\n",
    "\n",
    "h5f.create_dataset(\"prices_for_window_index_array_val\",\n",
    "                   data=prices_for_window_index_array_val)\n",
    "\n",
    "h5f.create_dataset(\"prices_for_window_index_array_test\",\n",
    "                   data=prices_for_window_index_array_test)\n",
    "\n",
    "h5f.create_dataset(\"close_index_array_train\", data=close_index_array_train)\n",
    "h5f.create_dataset(\"close_index_array_val\", data=close_index_array_val)\n",
    "h5f.create_dataset(\"close_index_array_test\", data=close_index_array_test)\n",
    "\n",
    "h5f.create_dataset(\"input_features_normalized_train\",\n",
    "                   data=input_features_normalized_train)\n",
    "\n",
    "h5f.create_dataset(\"input_features_normalized_val\",\n",
    "                   data=input_features_normalized_val)\n",
    "\n",
    "h5f.create_dataset(\"input_features_normalized_test\",\n",
    "                   data=input_features_normalized_test)\n",
    "\n",
    "h5f.create_dataset(\"y_train\", data=y_train)\n",
    "h5f.create_dataset(\"y_val\", data=y_val)\n",
    "h5f.create_dataset(\"y_test\", data=y_test)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"writing train/val/test to .h5 files finished taking \" +\n",
    "      str(end_time - start_time))\n",
    "# if stage == 2:\n",
    "#     # size\n",
    "#     h5f.create_dataset(\"P\", data=P)\n",
    "# h5f.create_dataset(\"y\", data=y)\n",
    "# h5f.create_dataset(\"sampled_idx_epoch\", data=sampled_idx_epoch)\n",
    "# if use_sample_weights == \"on\":\n",
    "#     h5f.create_dataset(\"sample_weights\", data=sample_weights)\n",
    "# elif use_sample_weights == \"off\":\n",
    "#     h5f.create_dataset(\"sample_weights\", data=np.zeros(1))\n",
    "# h5f.close()\n",
    "# # save data dataframe\n",
    "# table = pa.Table.from_pandas(data)\n",
    "# pq.write_table(\n",
    "#     table,\n",
    "#     path_adjust + \"data/preprocessed/\" + parameter_string + \"_data.parquet\",\n",
    "#     use_dictionary=True,\n",
    "#     compression=\"snappy\",\n",
    "#     use_deprecated_int96_timestamps=True,\n",
    "# )\n",
    "\n",
    "with open(path_adjust + \"temp/data_name.txt\", \"w+\") as text_file:\n",
    "    text_file.write(parameter_string)\n",
    "\n",
    "very_end = time.time()\n",
    "print(\"full_script_time\" + str(very_end - very_start))\n",
    "\n",
    "print(parameter_string + \".h5\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'trim-snowflake-30'"
     ]
    }
   ],
   "source": [
    "wandb.run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'25yocwa5'"
     ]
    }
   ],
   "source": [
    "wandb.run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"script initiated\")\n",
    "import time\n",
    "very_start = time.time()\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import tables as tb\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from multiprocessing import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)  # don't use scientific notati\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "# from hanging_threads import start_monitoring\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler, StandardScaler\n",
    "import keras\n",
    "import third_party_libraries.finance_ml\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab as ml\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab.labeling.labeling as labeling\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab.sample_weights.attribution as attribution\n",
    "import third_party_libraries.snippets as snp\n",
    "from third_party_libraries.finance_ml.stats.vol import *\n",
    "\n",
    "from library.core import *\n",
    "# monitoring_thread = start_monitoring(seconds_frozen=360, test_interval=100)\n",
    "# import googlecloudprofiler\n",
    "# try:\n",
    "#     googlecloudprofiler.start(\n",
    "#         service=\"preemp-cpu-big-full-jeff_in-max\",\n",
    "#         # verbose is the logging level. 0-error, 1-warning, 2-info,\n",
    "#         # 3-debug. It defaults to 0 (error) if not set.\n",
    "#         verbose=3,\n",
    "#     )\n",
    "# except (ValueError, NotImplementedError) as exc:\n",
    "#     print(exc)  # Handle errors here\n",
    "\n",
    "arg_parse_stage = None\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--is_finished\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    args = parser.parse_args()\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "try:\n",
    "    with open(path_adjust + \"temp/data_name_gpu.txt\", \"r\") as text_file:\n",
    "        gpu_file_name = text_file.read()\n",
    "        stage = 2\n",
    "except:\n",
    "    stage = 1\n",
    "\n",
    "side = None\n",
    "\n",
    "with open(path_adjust + \"temp/is_script_finished.txt\", \"w+\") as text_file:\n",
    "    text_file.write(\"start_script_time\" + str(very_start))\n",
    "\n",
    "if arg_parse_stage == 1:\n",
    "    stage = int(args.stage)\n",
    "\n",
    "print(\"the stage\" + str(stage))\n",
    "# Overide model and stage for testing\n",
    "\n",
    "model = \"two_model\"\n",
    "stage = 1\n",
    "print(\"the overidden stage\" + str(stage))\n",
    "\n",
    "if stage == 2:\n",
    "    # size\n",
    "    h5f = h5py.File(\"data/gpu_output/\" + gpu_file_name + \".h5\", \"r\")\n",
    "    X = h5f[\"X\"][:]\n",
    "    P = h5f[\"P\"][:]\n",
    "    sample_weights = h5f[\"sample_weights\"][:]\n",
    "    sampled_idx_epoch = h5f[\"sampled_idx_epoch\"][:]\n",
    "    h5f.close()\n",
    "    data = pq.read_pandas(\"data/gpu_output/\" + gpu_file_name +\n",
    "                          \"_data.parquet\").to_pandas()\n",
    "    X_for_all_labels = data.dropna(subset=[\"bins\"])\n",
    "    sampled_idx = pd.DatetimeIndex(sampled_idx_epoch)\n",
    "    X_for_all_labels[\"predicted_bins\"] = P\n",
    "    side = X_for_all_labels[\"predicted_bins\"]\n",
    "    # Could use the probabilities (instead of [1,0,0] use [0.2,0.55,0.25]\n",
    "\n",
    "import yaml\n",
    "import wandb\n",
    "yaml_path = path_adjust + \"yaml/preprocessing.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "wandb.init(\n",
    "    dir=\"~/ProdigyAI/\",\n",
    "    project=\"prodigyai\",\n",
    "    config=config_dictionary,\n",
    ")\n",
    "\n",
    "minimum_return = eval(wandb.config['params']['minimum_return']['value'])\n",
    "\n",
    "vertical_barrier_seconds = eval(\n",
    "    wandb.config['params']['vertical_barrier_seconds']['value'])\n",
    "# Parameters\n",
    "\n",
    "parameters = dict()\n",
    "\n",
    "wandb.config['params']['head'][\n",
    "    'value'] = 1000  # take only first x number of rows 0 means of\n",
    "\n",
    "volume_max = (\n",
    "    minimum_return + wandb.config['params']['vol_max_modifier']['value']\n",
    ")  # The higher this is the more an increase in volatility requries an increase\n",
    "# in return to be considered buy/sell (Increasing this increases end barrier vertical touches)\n",
    "\n",
    "volume_min = minimum_return + wandb.config['params']['vol_min_modifier'][\n",
    "    'value']\n",
    "\n",
    "filter_type = wandb.config['params']['filter_type']['value']\n",
    "\n",
    "if filter_type == \"cm\":\n",
    "    cusum_filter_vol_modifier = wandb.config['params'][\n",
    "        'cusum_filter_volume_modifier']['value']\n",
    "else:\n",
    "    cusum_filter_vol_modifier = 0\n",
    "\n",
    "use_sample_weights = wandb.config['params']['use_sample_weights']['value']\n",
    "\n",
    "use_sample_weights = wandb.config['params']['use_fractional_differentiation'][\n",
    "    'value']\n",
    "\n",
    "input_type = wandb.config['params']['input_type']['value']\n",
    "# parameters[\"ntb\"] = True  # non time bars\n",
    "# if parameters[\"ntb\"] == True:\n",
    "#     # Pick whether you want to add in the time since last bar input feature\n",
    "#     # time since last bar column\n",
    "#     parameters[\"tslbc\"] = True  # time since last bar column\n",
    "# else:\n",
    "#     # Pick whether you want to add in the volume input feature\n",
    "#     parameters[\"vbc\"] = True  # volume bar column\n",
    "# Create the txt file string\n",
    "\n",
    "parameter_string = wandb.run.id\n",
    "\n",
    "pt_sl = [\n",
    "    wandb.config['params']['profit_taking_multiplier']['value'],\n",
    "    wandb.config['params']['stop_loss_multiplier']['value']\n",
    "]\n",
    "\n",
    "cpus = cpu_count() - 1\n",
    "regenerate_features_and_labels = True\n",
    "\n",
    "if regenerate_features_and_labels == True:\n",
    "    # READ THE DATA\n",
    "    if stage == 1:\n",
    "        # Side\n",
    "        print(\"starting data load\")\n",
    "        head = wandb.config['params']['head']['value']\n",
    "        # # read parquet file of dollar bars\n",
    "        if input_type == \"bars\":\n",
    "            # Mlfinlab bars\n",
    "            data = pq.read_pandas(\n",
    "                path_adjust + \"data/bars/\"\n",
    "                \"btcusdt_agg_trades_50_volume_bars.parquet\").to_pandas()\n",
    "            data = data.drop(columns=[\n",
    "                \"open\",\n",
    "                \"high\",\n",
    "                \"low\",\n",
    "                # \"volume\",\n",
    "                \"seconds_since_last_bar\",\n",
    "            ])\n",
    "            # 1 min ohlcv ready made bars\n",
    "            # data = pq.read_pandas(\"data/bars/BTCUSDT_1m.parquet\").to_pandas()\n",
    "            # data[\"date_time\"] = pd.to_datetime(data[\"date_time\"], unit='ms')\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "            data = data.set_index(\"date_time\")\n",
    "            data.index = pd.to_datetime(data.index, infer_datetime_format=True)\n",
    "        # read parquet file of raw ticks\n",
    "        if input_type == \"ticks\":\n",
    "            data = pq.read_pandas(\n",
    "                path_adjust + \"data/bars/\" +\n",
    "                \"btcusdt_agg_trades_raw_tick_data.parquet\").to_pandas()\n",
    "            data = data.rename(columns={\n",
    "                \"date\": \"date_time\",\n",
    "                \"price\": \"close\",\n",
    "                \"volume\": \"volume\"\n",
    "            })\n",
    "            data = data.drop(columns=[\"volume\"])\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "            data = data.set_index(\"date_time\")\n",
    "            print(\"converting index to date_time\")\n",
    "            data.index = pd.to_datetime(data.index,\n",
    "                                        format=\"%m/%d/%Y %H:%M:%S.%f\")\n",
    "            print(\"index converted\")\n",
    "            # Should do something else than drop the duplicates (maybe trades doesnt have duplicate indexes rather than aggtrades)\n",
    "            data = data.loc[~data.index.duplicated(keep=\"first\")]\n",
    "        if input_type == \"orderbook\":\n",
    "            with open(path_adjust + \"temp/orderbook_data_name.txt\",\n",
    "                      \"r\") as text_file:\n",
    "                orderbook_preprocessed_file_name = text_file.read()\n",
    "            h5f = h5py.File(\n",
    "                path_adjust + \"data/orderbook_preprocessed/\" +\n",
    "                orderbook_preprocessed_file_name + \".h5\",\n",
    "                \"r\",\n",
    "            )\n",
    "            volumes_index_as_epoch = h5f[\"volumes_index_as_epoch\"][:]\n",
    "            volumes_np_array = h5f[\"volumes_np_array\"][:]\n",
    "            df_index_as_epoch = h5f[\"df_index_as_epoch\"][:]\n",
    "            df_np_array = h5f[\"df_np_array\"][:]\n",
    "            h5f.close()\n",
    "            volumes = pd.DataFrame(data=volumes_np_array,\n",
    "                                   index=volumes_index_as_epoch)\n",
    "            volumes.index = pd.to_datetime(volumes.index, unit=\"ms\")\n",
    "            data = pd.DataFrame(data=df_np_array, index=df_index_as_epoch)\n",
    "            data.index = pd.to_datetime(data.index, unit=\"ms\")\n",
    "            data.columns = [\"close\"]\n",
    "            data.index.name = \"date_time\"\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "        print(\"data load finished\")\n",
    "        # Checking for duplicates\n",
    "        # duplicate_fast_search(data.index.duplicated())\n",
    "        # Fractional differentiation\n",
    "        if use_sample_weights == \"on\":\n",
    "            data_series = data[\"close\"].to_frame()\n",
    "            # # generate 100 points\n",
    "            # nsample = 1000\n",
    "            # ## simulate a simple sinusoidal function\n",
    "            # x1 = np.linspace(0, 10, nsample)\n",
    "            # y = pd.Series(1*np.sin(2.0 * x1 + .5))\n",
    "            # y.plot()\n",
    "            # c_constant = 1.\n",
    "            # y_shifted = (y + c_constant).cumsum().rename('Shifted_series').to_frame()\n",
    "            # y_shifted.plot()\n",
    "            # df = y_shifted\n",
    "            # # df=(df-df.mean())/df.std()\n",
    "            # df['Shifted_series'][1:] = np.diff(df['Shifted_series'].values)\n",
    "            # df['Shifted_series'].plot()\n",
    "            kwargs = None\n",
    "            # data_series = np.log(data_series)  ## is it good to log this?\n",
    "            frac_diff_series, d = get_opt_d(  # reduces the number of rows and ends up with less vertical barriers touched\n",
    "                data_series,\n",
    "                ds=None,\n",
    "                maxlag=None,  # If we use raw tick data need at least head > 8000\n",
    "                thres=1e-5,\n",
    "                max_size=10000,\n",
    "                p_thres=1e-2,\n",
    "                autolag=None,\n",
    "                verbose=1,\n",
    "            )\n",
    "            data[\"close\"] = frac_diff_series\n",
    "            data = data.dropna(subset=[\"close\"])\n",
    "        data[\"window_volatility_level\"] = np.nan\n",
    "        start = time.time()\n",
    "        volatility_level_array = volatility_levels_numba(\n",
    "            np.ascontiguousarray(data.close.values),\n",
    "            wandb.config['params']['window_length']['value'])\n",
    "        data[\"window_volatility_level\"] = volatility_level_array\n",
    "        # Should adjust the max value\n",
    "        # To get more vertical touches we can\n",
    "        # either increase vol_max or\n",
    "        # decrease the window seconds\n",
    "        scaler = MinMaxScaler(feature_range=(volume_min,\n",
    "                                             volume_max))  # normalization\n",
    "        normed_window_volatility_level = scaler.fit_transform(\n",
    "            data[[\"window_volatility_level\"]])\n",
    "        data[\"window_volatility_level\"] = normed_window_volatility_level  #\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "        # CUSUM FILTER\n",
    "        volatility_threshold = data[\"window_volatility_level\"].mean()\n",
    "        close_copy = data.dropna().close.copy(deep=True)\n",
    "        close_np_array, close_index_np_array = pandas_series_to_numba_ready_np_arrays(\n",
    "            close_copy)\n",
    "        volatility_threshold = volatility_threshold * cusum_filter_vol_modifier\n",
    "        print(\"data_len = \" + str(len(data)))\n",
    "        start = time.time()\n",
    "        sampled_idx = filter_events(\n",
    "            data,\n",
    "            close_np_array,\n",
    "            close_index_np_array,\n",
    "            volatility_threshold,\n",
    "            filter_type,\n",
    "        )\n",
    "        print(\"sampled_idx_len = \" + str(len(sampled_idx)))\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "    if stage == 2:\n",
    "        # size\n",
    "        start = time.time()\n",
    "        volatility_level_array = volatility_levels_numba(\n",
    "            data.close.values,\n",
    "            wandb.config['params']['window_length']['value'])\n",
    "        data[\"window_volatility_level\"] = volatility_level_array\n",
    "    # This code runs for both first and second stage preprocessing\n",
    "    start = time.time()\n",
    "    vertical_barrier_timestamps = ml.labeling.add_vertical_barrier(\n",
    "        t_events=sampled_idx,\n",
    "        close=data[\"close\"],\n",
    "        num_seconds=vertical_barrier_seconds)\n",
    "    end = time.time()\n",
    "    print(\"vertical barrier\" + str(end - start))\n",
    "    start = time.time()\n",
    "    print(\"Getting triple barrier events\")\n",
    "    triple_barrier_events = ml.labeling.get_events(\n",
    "        close=data[\"close\"],\n",
    "        t_events=sampled_idx,\n",
    "        pt_sl=pt_sl,\n",
    "        target=data[\"window_volatility_level\"],\n",
    "        min_ret=minimum_return,\n",
    "        num_threads=cpus * 2,\n",
    "        vertical_barrier_times=vertical_barrier_timestamps,\n",
    "        side_prediction=side,\n",
    "        split_by=\n",
    "        100  # maybe we want this as large as we can while still fitting in ram\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"triple_barrier_events finished taking\" + str(end - start))\n",
    "    very_end = time.time()\n",
    "    with open(path_adjust + \"temp/is_script_finished.txt\", \"w+\") as text_file:\n",
    "        text_file.write(\"full_script_time\" + str(very_end - very_start))\n",
    "    start_time = time.time()\n",
    "    print(\"Returning Bins\")\n",
    "    labels = ml.labeling.get_bins(triple_barrier_events, data[\"close\"])\n",
    "    labels = ml.labeling.drop_labels(labels)\n",
    "    label_counts = labels.bin.value_counts()\n",
    "    print(\"label_counts\" + str(label_counts))\n",
    "    end_time = time.time()\n",
    "    print(\"returning bins finished taking\" + str(end_time - start_time))\n",
    "    # unique, counts = np.unique(y, return_counts=True)\n",
    "    sampled_idx_epoch = sampled_idx.astype(np.int64)\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/sampled_idx_epoch.h5\",\n",
    "        \"w\")\n",
    "    h5f.create_dataset(\"sampled_idx_epoch\", data=sampled_idx_epoch)\n",
    "    h5f.close()\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(labels)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/labels.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(data)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/data.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(triple_barrier_events)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/triple_barrier_events.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "else:\n",
    "    labels = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/labels.parquet\").to_pandas()\n",
    "    data = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/data.parquet\").to_pandas()\n",
    "    triple_barrier_events = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/triple_barrier_events.parquet\"\n",
    "    ).to_pandas()\n",
    "    with open(path_adjust + \"temp/orderbook_data_name.txt\", \"r\") as text_file:\n",
    "        orderbook_preprocessed_file_name = text_file.read()\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/orderbook_preprocessed/\" +\n",
    "        orderbook_preprocessed_file_name + \".h5\", \"r\")\n",
    "    volumes_index_as_epoch = h5f[\"volumes_index_as_epoch\"][:]\n",
    "    volumes_np_array = h5f[\"volumes_np_array\"][:]\n",
    "    h5f.close()\n",
    "    volumes = pd.DataFrame(data=volumes_np_array, index=volumes_index_as_epoch)\n",
    "    volumes.index = pd.to_datetime(volumes.index, unit=\"ms\")\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/sampled_idx_epoch.h5\",\n",
    "        \"r\")\n",
    "    sampled_idx_epoch = h5f[\"sampled_idx_epoch\"][:]\n",
    "    h5f.close()\n",
    "    sampled_idx = pd.DatetimeIndex(sampled_idx_epoch)\n",
    "\n",
    "y_dataframe = labels[\"bin\"]\n",
    "data[\"bins\"] = labels[\"bin\"]\n",
    "y = np.asarray(y_dataframe)\n",
    "end = time.time()\n",
    "y = keras.utils.to_categorical(y, num_classes=3)\n",
    "\n",
    "if stage == 1:\n",
    "    start_time = time.time()\n",
    "    # side\n",
    "    X_for_all_labels = data.loc[labels.index, :]\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    # ### FOR HIGHWAY RNN\n",
    "    # X = np.asarray(volumes.loc[labels.index, :])\n",
    "    # h5f = h5py.File(path_adjust + \"data/preprocessed/\" + parameter_string + \".h5\", \"w\")\n",
    "    # h5f.create_dataset(\"X\", data=X)\n",
    "    # h5f.create_dataset(\"y\", data=y)\n",
    "    # h5f.close()\n",
    "    # X = []\n",
    "    start_time = time.time()\n",
    "    prices_for_window = data.loc[X_for_all_labels.index]\n",
    "    prices_for_window_index = prices_for_window.index.astype(np.int64)\n",
    "    prices_for_window_index_array = np.asarray(prices_for_window_index)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    start_time = time.time()\n",
    "    close_index = data.close.index.astype(np.int64)\n",
    "    close_index_array = np.asarray(close_index)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    start_time = time.time()\n",
    "    # Make a new column time since last bar\n",
    "    unindexed_data = data.reset_index()\n",
    "    unindexed_data[\"shifted_date_time\"] = unindexed_data[\"date_time\"].shift(1)\n",
    "    unindexed_data[\"time_since_last_bar\"] = (unindexed_data[\"date_time\"].sub(\n",
    "        unindexed_data[\"shifted_date_time\"], axis=0).dt.seconds)\n",
    "    unindexed_data = unindexed_data.set_index(\"date_time\")\n",
    "    data[\"time_since_last_bar\"] = unindexed_data[\"time_since_last_bar\"]\n",
    "    data[\"time_since_last_bar\"].iloc[0] = 0\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    start_time = time.time()\n",
    "    ### ORDERBOOK VOLUME DATA\n",
    "    volumes_for_all_labels = volumes.loc[data.close.index]\n",
    "    # ## TRADE DATA\n",
    "    # input_features_trade = []\n",
    "    # close_array = data.close.values\n",
    "    # input_features_trade.append(close_array)\n",
    "    # if parameters[\"ntb\"] == False and parameters[\"vbc\"] == True:\n",
    "    #     volume_array = data.volume.values\n",
    "    #     input_features_trade.append(volume_array)\n",
    "    # if parameters[\"ntb\"] == True and parameters[\"tslbc\"] == True:\n",
    "    #     time_since_last_bar_array = data.time_since_last_bar.values\n",
    "    #     input_features_trade.append(time_since_last_bar_array)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    # Type of scaling to apply\n",
    "    scaling_type = wandb.config['params']['scaling_type']['value']\n",
    "    # min max limits\n",
    "    minimum = wandb.config['params']['scaling_maximum']['value']\n",
    "    maximum = wandb.config['params']['scaling_minimum']['value']\n",
    "    ### Split intothe training/validation/test sets\n",
    "    print(\"splitting into train/va/test sets start\")\n",
    "    start_time = time.time()\n",
    "    prices_for_window_index_array_train_and_val = prices_for_window_index_array[:round(\n",
    "        len(prices_for_window_index_array) * 0.8)]\n",
    "    y_train_and_val = y[:round(len(y) * 0.8)]\n",
    "    prices_for_window_index_array_train = prices_for_window_index_array_train_and_val[:round(\n",
    "        len(prices_for_window_index_array_train_and_val) * 0.8)]\n",
    "    y_train = y_train_and_val[:round(len(y_train_and_val) * 0.8)]\n",
    "    train_close_array_integer_index = np.nonzero(\n",
    "        np.in1d(close_index_array, prices_for_window_index_array_train))[0]\n",
    "    volumes_for_all_labels_train = volumes_for_all_labels.iloc[\n",
    "        train_close_array_integer_index[0] -\n",
    "        wandb.config['params']['window_length']['value']:\n",
    "        train_close_array_integer_index[-1] + 2]\n",
    "    close_index_array_train = close_index_array[\n",
    "        train_close_array_integer_index[0] -\n",
    "        wandb.config['params']['window_length']['value']:\n",
    "        train_close_array_integer_index[-1] + 2]\n",
    "    end_time = time.time()\n",
    "    print(\"splitting into train/va/test sets finished\" +\n",
    "          str(end_time - start_time))\n",
    "    print(\"Make input features from orderbook data started\")\n",
    "    start_time = time.time()\n",
    "    # MAKE WINDOW FROM INPUTS\n",
    "    input_features_train = make_input_features_from_orderbook_data(\n",
    "        volumes_for_all_labels_train)\n",
    "    end_time = time.time()\n",
    "    print(\"Make input features from orderbook data started \" +\n",
    "          str(end_time - start_time))\n",
    "    print(\"Get train scalers started\")\n",
    "    start_time = time.time()\n",
    "    ### 2: Apply the normalisation based on the training fit scalars (-1,1 vs 0,1 min max test both)\n",
    "    maxes_or_means_np_array_train, mins_or_stds_np_array_train = get_fit_scalars(\n",
    "        scaling_type, input_features_train)\n",
    "    end_time = time.time()\n",
    "    print(\"Get train scalers finished\" + str(end_time - start_time))\n",
    "    print(\"Norm train data started\")\n",
    "    start_time = time.time()\n",
    "    # Norm train\n",
    "    input_features_normalized_train = scale_input_features(\n",
    "        scaling_type,\n",
    "        maxes_or_means_np_array_train,\n",
    "        mins_or_stds_np_array_train,\n",
    "        input_features_train,\n",
    "        minimum,\n",
    "        maximum,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(\"Get train scalers finished\" + str(end_time - start_time))\n",
    "    # print(\"Make window started\")\n",
    "    # start_time = time.time()\n",
    "    # padding = wandb.config['params']['window_length']['value'] * 2\n",
    "    # split_by = 100000\n",
    "    # number_of_splits = len(prices_for_window_index_array_train) // split_by\n",
    "    # for i in range(number_of_splits):\n",
    "    #     print(i)\n",
    "    #     start_time = time.time()\n",
    "    #     if i == 0:\n",
    "    #         start_index = None  # 0\n",
    "    #         end_index = (i + 1) * split_by\n",
    "    #         close_and_input_start_index = start_index\n",
    "    #         close_and_input_end_index = end_index + (padding)\n",
    "    #     elif i < number_of_splits - 1:\n",
    "    #         start_index = i * split_by\n",
    "    #         end_index = (i + 1) * split_by\n",
    "    #         close_and_input_start_index = start_index - (padding)\n",
    "    #         close_and_input_end_index = end_index + (padding)\n",
    "    #     elif i == number_of_splits - 1:\n",
    "    #         start_index = i * split_by\n",
    "    #         end_index = None  # -1\n",
    "    #         close_and_input_start_index = start_index - (padding)\n",
    "    #         close_and_input_end_index = end_index\n",
    "    #     # Window train\n",
    "    #     X_train_section = make_window_multivariate_numba(\n",
    "    #         len(prices_for_window_index_array_train[start_index:end_index]),\n",
    "    #         input_features_normalized_train[:, close_and_input_start_index:\n",
    "    #                                         close_and_input_end_index],\n",
    "    #         wandb.config['params']['window_length']['value'],\n",
    "    #         model_arch,\n",
    "    #     )\n",
    "    #     print(X_train_section.shape)\n",
    "    #     hdf5_epath = path_adjust + \"data/preprocessed/X_and_y.h5\"\n",
    "    #     if os.path.exists(hdf5_epath) == False or i == 0:\n",
    "    #         h5f = tb.open_file(hdf5_epath, mode=\"a\")\n",
    "    #         dataGroup = h5f.create_group(h5f.root, \"MyData\")\n",
    "    #         h5f.create_earray(dataGroup,\n",
    "    #                           \"X_train_section\",\n",
    "    #                           obj=X_train_section)\n",
    "    #         h5f.close()\n",
    "    #     else:\n",
    "    #         h5f = tb.open_file(hdf5_epath, mode=\"r+\")\n",
    "    #         h5f.root.MyData.X_train_section.append(X_train_section)\n",
    "    #         h5f.close()\n",
    "    #     # h5f = h5py.File(path_adjust + \"data/preprocessed/X_and_y.h5\", \"w\")\n",
    "    #     # h5f.create_dataset(\"X_train_section\", data=X_train_section)\n",
    "    #     # h5f.close()\n",
    "    #     end_time = time.time()\n",
    "    #     print(str(i) + \" finished taking \" + str(end_time - start_time))\n",
    "    # end_time = time.time()\n",
    "    # print(\"Make window finished\" + str(end_time - start_time))\n",
    "    #     # import pdb\n",
    "    #     # pdb.set_trace()\n",
    "    # Val\n",
    "    prices_for_window_index_array_val = prices_for_window_index_array_train_and_val[\n",
    "        round(len(prices_for_window_index_array_train_and_val) * 0.8):]\n",
    "    y_val = y_train_and_val[round(len(y_train_and_val) * 0.8):]\n",
    "    val_close_array_integer_index = np.nonzero(\n",
    "        np.in1d(close_index_array, prices_for_window_index_array_val))[0]\n",
    "    volumes_for_all_labels_val = volumes_for_all_labels.iloc[\n",
    "        val_close_array_integer_index[0] -\n",
    "        wandb.config['params']['window_length']['value']:\n",
    "        val_close_array_integer_index[-1] + 2]\n",
    "    close_index_array_val = close_index_array[\n",
    "        val_close_array_integer_index[0] -\n",
    "        wandb.config['params']['window_length']['value']:\n",
    "        val_close_array_integer_index[-1] + 2]\n",
    "    input_features_val = make_input_features_from_orderbook_data(\n",
    "        volumes_for_all_labels_val)\n",
    "    # Norm val\n",
    "    input_features_normalized_val = scale_input_features(\n",
    "        scaling_type,\n",
    "        maxes_or_means_np_array_train,\n",
    "        mins_or_stds_np_array_train,\n",
    "        input_features_val,\n",
    "        minimum,\n",
    "        maximum,\n",
    "    )\n",
    "    #     # # Make window val\n",
    "    #     # X_val = make_window_multivariate_numba(\n",
    "    #     #     prices_for_window_index_array_val,\n",
    "    #     #     close_index_array_val,\n",
    "    #     #     input_features_normalized_val,\n",
    "    #     #     wandb.config['params']['window_length']['value'],\n",
    "    #     #     model_arch,\n",
    "    #     # )\n",
    "    # Test\n",
    "    prices_for_window_index_array_test = prices_for_window_index_array[\n",
    "        round(len(prices_for_window_index_array) * 0.8):]\n",
    "    y_test = y[round(len(y) * 0.8):]\n",
    "    test_close_array_integer_index = np.nonzero(\n",
    "        np.in1d(close_index_array, prices_for_window_index_array_test))[0]\n",
    "    volumes_for_all_labels_test = volumes_for_all_labels.iloc[\n",
    "        test_close_array_integer_index[0] -\n",
    "        wandb.config['params']['window_length']['value']:\n",
    "        test_close_array_integer_index[-1] + 2]\n",
    "    close_index_array_test = close_index_array[\n",
    "        test_close_array_integer_index[0] -\n",
    "        wandb.config['params']['window_length']['value']:\n",
    "        test_close_array_integer_index[-1] + 2]\n",
    "    input_features_test = make_input_features_from_orderbook_data(\n",
    "        volumes_for_all_labels_test)\n",
    "    # Norm test\n",
    "    input_features_normalized_test = scale_input_features(\n",
    "        scaling_type,\n",
    "        maxes_or_means_np_array_train,\n",
    "        mins_or_stds_np_array_train,\n",
    "        input_features_test,\n",
    "        minimum,\n",
    "        maximum,\n",
    "    )\n",
    "#     # # Window test\n",
    "#     # # TABL\n",
    "#     # X_test = make_window_multivariate_numba(\n",
    "#     #     prices_for_window_index_array_test,\n",
    "#     #     close_index_array_test,\n",
    "#     #     input_features_normalized_test,\n",
    "#     #     wandb.config['params']['window_length']['value'],\n",
    "#     #     model_arch,\n",
    "#     # )\n",
    "#     start = time.time()\n",
    "#     end = time.time()\n",
    "#     print(\"numba make window time\" + str(end - start))\n",
    "#     start = time.time()\n",
    "\n",
    "print(\"plotting started\")\n",
    "# plot the whole sequence\n",
    "\n",
    "ax = plt.gca()\n",
    "data.plot(y=\"close\", use_index=True)\n",
    "window_index = 500\n",
    "ax = plt.gca()\n",
    "\n",
    "data.iloc[window_index - 200:window_index + 200].plot(y=\"close\",\n",
    "                                                      use_index=True)\n",
    "\n",
    "plot_window_and_touch_and_label(\n",
    "    window_index, wandb.config['params']['window_length']['value'], data,\n",
    "    triple_barrier_events, labels)\n",
    "\n",
    "data.iloc[window_index - 10:window_index + 30]\n",
    "\n",
    "print(\"plotting finished\")\n",
    "# Sample Weights\n",
    "# if stage == 1:\n",
    "#     weights = attribution.get_weights_by_return(triple_barrier_events.dropna(),\n",
    "#                                                 data.close,\n",
    "#                                                 num_threads=5)\n",
    "#     sample_weights = np.asarray(weights)\n",
    "#     sample_weights = sample_weights.reshape(len(sample_weights))\n",
    "#     sampled_idx_epoch = sampled_idx.astype(np.int64)  #\n",
    "\n",
    "if stage == 2:\n",
    "    # size\n",
    "    parameter_string = parameter_string + \"second_stage\"\n",
    "\n",
    "print(\"writing train/val/test to .h5 files starting\")\n",
    "\n",
    "start = time.time()\n",
    "# Writing preprocessed X,y\n",
    "\n",
    "h5f = h5py.File(path_adjust + \"data/preprocessed/\" + parameter_string + \".h5\",\n",
    "                \"w\")\n",
    "\n",
    "h5f.create_dataset(\"prices_for_window_index_array_train\",\n",
    "                   data=prices_for_window_index_array_train)\n",
    "\n",
    "h5f.create_dataset(\"prices_for_window_index_array_val\",\n",
    "                   data=prices_for_window_index_array_val)\n",
    "\n",
    "h5f.create_dataset(\"prices_for_window_index_array_test\",\n",
    "                   data=prices_for_window_index_array_test)\n",
    "\n",
    "h5f.create_dataset(\"close_index_array_train\", data=close_index_array_train)\n",
    "h5f.create_dataset(\"close_index_array_val\", data=close_index_array_val)\n",
    "h5f.create_dataset(\"close_index_array_test\", data=close_index_array_test)\n",
    "\n",
    "h5f.create_dataset(\"input_features_normalized_train\",\n",
    "                   data=input_features_normalized_train)\n",
    "\n",
    "h5f.create_dataset(\"input_features_normalized_val\",\n",
    "                   data=input_features_normalized_val)\n",
    "\n",
    "h5f.create_dataset(\"input_features_normalized_test\",\n",
    "                   data=input_features_normalized_test)\n",
    "\n",
    "h5f.create_dataset(\"y_train\", data=y_train)\n",
    "h5f.create_dataset(\"y_val\", data=y_val)\n",
    "h5f.create_dataset(\"y_test\", data=y_test)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"writing train/val/test to .h5 files finished taking \" +\n",
    "      str(end_time - start_time))\n",
    "# if stage == 2:\n",
    "#     # size\n",
    "#     h5f.create_dataset(\"P\", data=P)\n",
    "# h5f.create_dataset(\"y\", data=y)\n",
    "# h5f.create_dataset(\"sampled_idx_epoch\", data=sampled_idx_epoch)\n",
    "# if use_sample_weights == \"on\":\n",
    "#     h5f.create_dataset(\"sample_weights\", data=sample_weights)\n",
    "# elif use_sample_weights == \"off\":\n",
    "#     h5f.create_dataset(\"sample_weights\", data=np.zeros(1))\n",
    "# h5f.close()\n",
    "# # save data dataframe\n",
    "# table = pa.Table.from_pandas(data)\n",
    "# pq.write_table(\n",
    "#     table,\n",
    "#     path_adjust + \"data/preprocessed/\" + parameter_string + \"_data.parquet\",\n",
    "#     use_dictionary=True,\n",
    "#     compression=\"snappy\",\n",
    "#     use_deprecated_int96_timestamps=True,\n",
    "# )\n",
    "\n",
    "with open(path_adjust + \"temp/data_name.txt\", \"w+\") as text_file:\n",
    "    text_file.write(parameter_string)\n",
    "\n",
    "very_end = time.time()\n",
    "print(\"full_script_time\" + str(very_end - very_start))\n",
    "\n",
    "print(parameter_string + \".h5\")\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

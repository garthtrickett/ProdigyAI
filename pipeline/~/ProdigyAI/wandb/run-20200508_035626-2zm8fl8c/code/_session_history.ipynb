{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/garthtrickett/prodigyai/runs/1xl0is2l"
     ]
    }
   ],
   "source": [
    "print(\"script initiated\")\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from numba import njit, prange\n",
    "import tensorflow as tf\n",
    "\n",
    "import math\n",
    "# Init wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"prodigyai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/garthtrickett/ProdigyAI/pipeline/wandb/run-20200508_034406-1xl0is2l'"
     ]
    }
   ],
   "source": [
    "wandb.run.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.dir = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run(dir='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/garthtrickett/ProdigyAI/pipeline/wandb/run-20200508_034406-1xl0is2l'"
     ]
    }
   ],
   "source": [
    "wandb.run._dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run._dir = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'test'"
     ]
    }
   ],
   "source": [
    "wandb.run.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/garthtrickett/prodigyai/runs/1xl0is2l"
     ]
    }
   ],
   "source": [
    "wandb.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__',\n",
      " '__delattr__',\n",
      " '__dict__',\n",
      " '__dir__',\n",
      " '__doc__',\n",
      " '__enter__',\n",
      " '__eq__',\n",
      " '__exit__',\n",
      " '__format__',\n",
      " '__ge__',\n",
      " '__getattribute__',\n",
      " '__gt__',\n",
      " '__hash__',\n",
      " '__init__',\n",
      " '__init_subclass__',\n",
      " '__le__',\n",
      " '__lt__',\n",
      " '__module__',\n",
      " '__ne__',\n",
      " '__new__',\n",
      " '__reduce__',\n",
      " '__reduce_ex__',\n",
      " '__repr__',\n",
      " '__setattr__',\n",
      " '__sizeof__',\n",
      " '__str__',\n",
      " '__subclasshook__',\n",
      " '__weakref__',\n",
      " '_add_singleton',\n",
      " '_add_viz',\n",
      " '_api',\n",
      " '_dir',\n",
      " '_events',\n",
      " '_generate_query_string',\n",
      " '_history',\n",
      " '_history_added',\n",
      " '_init_jupyter_agent',\n",
      " '_jupyter_agent',\n",
      " '_load_entity',\n",
      " '_meta',\n",
      " '_mkdir',\n",
      " '_name',\n",
      " '_name_and_description',\n",
      " '_run_manager',\n",
      " '_stop_jupyter_agent',\n",
      " '_summary',\n",
      " 'api',\n",
      " 'args',\n",
      " 'auto_project_name',\n",
      " 'close_files',\n",
      " 'config',\n",
      " 'config_static',\n",
      " 'description',\n",
      " 'description_path',\n",
      " 'dir',\n",
      " 'enable_logging',\n",
      " 'entity',\n",
      " 'events',\n",
      " 'from_directory',\n",
      " 'from_environment_or_defaults',\n",
      " 'get_project_url',\n",
      " 'get_sweep_url',\n",
      " 'get_url',\n",
      " 'group',\n",
      " 'has_events',\n",
      " 'has_history',\n",
      " 'has_summary',\n",
      " 'history',\n",
      " 'host',\n",
      " 'id',\n",
      " 'job_type',\n",
      " 'log',\n",
      " 'log_fname',\n",
      " 'mode',\n",
      " 'name',\n",
      " 'notes',\n",
      " 'path',\n",
      " 'pid',\n",
      " 'program',\n",
      " 'project',\n",
      " 'project_name',\n",
      " 'resume',\n",
      " 'resumed',\n",
      " 'save',\n",
      " 'send_message',\n",
      " 'set_environment',\n",
      " 'socket',\n",
      " 'step',\n",
      " 'storage_id',\n",
      " 'summary',\n",
      " 'sweep_id',\n",
      " 'tags',\n",
      " 'upload_debug',\n",
      " 'wandb_dir',\n",
      " 'watch']"
     ]
    }
   ],
   "source": [
    "dir(wandb.run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.wandb_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.wandb_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"script initiated\")\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from numba import njit, prange\n",
    "import tensorflow as tf\n",
    "\n",
    "import math\n",
    "# Init wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"prodigyai\")\n",
    "wandb.run._dir = '/home/garthtrickett/ProdigyAI/wandb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/garthtrickett/ProdigyAI/wandb'"
     ]
    }
   ],
   "source": [
    "wandb.run.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/garthtrickett/prodigyai/runs/kbdn484p"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"prodigyai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/garthtrickett/ProdigyAI/pipeline/wandb/run-20200508_034913-kbdn484p'"
     ]
    }
   ],
   "source": [
    "wandb.run._dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['',\n",
      " 'home',\n",
      " 'garthtrickett',\n",
      " 'ProdigyAI',\n",
      " 'pipeline',\n",
      " 'wandb',\n",
      " 'run-20200508_034913-kbdn484p']"
     ]
    }
   ],
   "source": [
    "wandb.run.dir.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/garthtrickett/ProdigyAI/wandb/run-20200508_034913-kbdn484p'"
     ]
    }
   ],
   "source": [
    "\"/home/garthtrickett/ProdigyAI/wandb/\" + wandb.run.dir.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run._dir = \"/home/garthtrickett/ProdigyAI/wandb/\" + wandb.run.dir.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"prodigyai\")\n",
    "wandb.run._dir = \"~/ProdigyAI/wandb/\" + wandb.run.dir.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'~/ProdigyAI/wandb/run-20200508_035220-2xxb4xgc'"
     ]
    }
   ],
   "source": [
    "wandb.run._dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"script initiated\")\n",
    "import time\n",
    "very_start = time.time()\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import tables as tb\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from multiprocessing import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)  # don't use scientific notati\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "# from hanging_threads import start_monitoring\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler, StandardScaler\n",
    "import keras\n",
    "import third_party_libraries.finance_ml\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab as ml\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab.labeling.labeling as labeling\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab.sample_weights.attribution as attribution\n",
    "import third_party_libraries.snippets as snp\n",
    "from third_party_libraries.finance_ml.stats.vol import *\n",
    "\n",
    "from library.core import *\n",
    "# monitoring_thread = start_monitoring(seconds_frozen=360, test_interval=100)\n",
    "# import googlecloudprofiler\n",
    "# try:\n",
    "#     googlecloudprofiler.start(\n",
    "#         service=\"preemp-cpu-big-full-jeff_in-max\",\n",
    "#         # verbose is the logging level. 0-error, 1-warning, 2-info,\n",
    "#         # 3-debug. It defaults to 0 (error) if not set.\n",
    "#         verbose=3,\n",
    "#     )\n",
    "# except (ValueError, NotImplementedError) as exc:\n",
    "#     print(exc)  # Handle errors here\n",
    "\n",
    "arg_parse_stage = None\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--is_finished\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    args = parser.parse_args()\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "try:\n",
    "    with open(path_adjust + \"temp/data_name_gpu.txt\", \"r\") as text_file:\n",
    "        gpu_file_name = text_file.read()\n",
    "        stage = 2\n",
    "except:\n",
    "    stage = 1\n",
    "\n",
    "side = None\n",
    "\n",
    "with open(path_adjust + \"temp/is_script_finished.txt\", \"w+\") as text_file:\n",
    "    text_file.write(\"start_script_time\" + str(very_start))\n",
    "\n",
    "if arg_parse_stage == 1:\n",
    "    stage = int(args.stage)\n",
    "\n",
    "print(\"the stage\" + str(stage))\n",
    "# Overide model and stage for testing\n",
    "\n",
    "model = \"two_model\"\n",
    "stage = 1\n",
    "print(\"the overidden stage\" + str(stage))\n",
    "\n",
    "if stage == 2:\n",
    "    # size\n",
    "    h5f = h5py.File(\"data/gpu_output/\" + gpu_file_name + \".h5\", \"r\")\n",
    "    X = h5f[\"X\"][:]\n",
    "    P = h5f[\"P\"][:]\n",
    "    sample_weights = h5f[\"sample_weights\"][:]\n",
    "    sampled_idx_epoch = h5f[\"sampled_idx_epoch\"][:]\n",
    "    h5f.close()\n",
    "    data = pq.read_pandas(\"data/gpu_output/\" + gpu_file_name +\n",
    "                          \"_data.parquet\").to_pandas()\n",
    "    X_for_all_labels = data.dropna(subset=[\"bins\"])\n",
    "    sampled_idx = pd.DatetimeIndex(sampled_idx_epoch)\n",
    "    X_for_all_labels[\"predicted_bins\"] = P\n",
    "    side = X_for_all_labels[\"predicted_bins\"]\n",
    "    # Could use the probabilities (instead of [1,0,0] use [0.2,0.55,0.25]\n",
    "# Parameters\n",
    "\n",
    "parameters = dict()\n",
    "model_arch = \"TABL\"\n",
    "parameters[\"arch\"] = model_arch\n",
    "parameters[\"name\"] = model\n",
    "parameters[\"WL\"] = 200  # WINDOW LONG\n",
    "parameters[\"pt\"] = 1\n",
    "parameters[\"sl\"] = 1\n",
    "parameters[\"min_ret\"] = 0.001 * 1 / 23\n",
    "\n",
    "parameters[\"vbs\"] = round(1 / 2,\n",
    "                          3)  # Increasing this decreases vertical touches\n",
    "\n",
    "parameters[\"head\"] = 1000  # take only first x number of rows 0 means of\n",
    "\n",
    "parameters[\"skip\"] = 0  # sample every n'th row if skip > 0\n",
    "# get even classes at fraction = 0.8 so the training set is balanced then set to 1\n",
    "# 3 million rows is about the limit of before it starts taking ages / memory maxing\n",
    "\n",
    "parameters[\"vol_max\"] = (\n",
    "    parameters[\"min_ret\"] + 0.00000002\n",
    ")  # The higher this is the more an increase in volatility requries an increase\n",
    "# in return to be considered buy/sell (Increasing this increases end barrier vertical touches)\n",
    "\n",
    "parameters[\"vol_min\"] = parameters[\"min_ret\"] + 0.00000001\n",
    "parameters[\"filter\"] = \"none\"\n",
    "\n",
    "if parameters[\"filter\"] == \"cm\":\n",
    "    parameters[\"cm_vol_mod\"] = 500\n",
    "else:\n",
    "    parameters[\"cm_vol_mod\"] = 0\n",
    "\n",
    "parameters[\"sw\"] = \"on\"  # sample weights\n",
    "parameters[\"fd\"] = \"off\"\n",
    "parameters[\"input\"] = \"obook\"\n",
    "parameters[\"ntb\"] = True  # non time bars\n",
    "\n",
    "if parameters[\"ntb\"] == True:\n",
    "    # Pick whether you want to add in the time since last bar input feature\n",
    "    # time since last bar column\n",
    "    parameters[\"tslbc\"] = True  # time since last bar column\n",
    "else:\n",
    "    # Pick whether you want to add in the volume input feature\n",
    "    parameters[\"vbc\"] = True  # volume bar column\n",
    "# Create the txt file string\n",
    "\n",
    "parameter_string = \"&\".join(\"{}{}{}\".format(key, \"=\", val)\n",
    "                            for key, val in parameters.items())\n",
    "\n",
    "pt_sl = [parameters[\"pt\"], parameters[\"sl\"]]\n",
    "cpus = cpu_count() - 1\n",
    "regenerate_features_and_labels = True\n",
    "\n",
    "if regenerate_features_and_labels == True:\n",
    "    # READ THE DATA\n",
    "    if stage == 1:\n",
    "        # Side\n",
    "        print(\"starting data load\")\n",
    "        head = parameters[\"head\"]\n",
    "        # # read parquet file of dollar bars\n",
    "        if parameters[\"input\"] == \"bars\":\n",
    "            # Mlfinlab bars\n",
    "            data = pq.read_pandas(\n",
    "                path_adjust + \"data/bars/\"\n",
    "                \"btcusdt_agg_trades_50_volume_bars.parquet\").to_pandas()\n",
    "            data = data.drop(columns=[\n",
    "                \"open\",\n",
    "                \"high\",\n",
    "                \"low\",\n",
    "                # \"volume\",\n",
    "                \"seconds_since_last_bar\",\n",
    "            ])\n",
    "            # 1 min ohlcv ready made bars\n",
    "            # data = pq.read_pandas(\"data/bars/BTCUSDT_1m.parquet\").to_pandas()\n",
    "            # data[\"date_time\"] = pd.to_datetime(data[\"date_time\"], unit='ms')\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "            data = data.set_index(\"date_time\")\n",
    "            data.index = pd.to_datetime(data.index, infer_datetime_format=True)\n",
    "        # read parquet file of raw ticks\n",
    "        if parameters[\"input\"] == \"ticks\":\n",
    "            data = pq.read_pandas(\n",
    "                path_adjust + \"data/bars/\" +\n",
    "                \"btcusdt_agg_trades_raw_tick_data.parquet\").to_pandas()\n",
    "            data = data.rename(columns={\n",
    "                \"date\": \"date_time\",\n",
    "                \"price\": \"close\",\n",
    "                \"volume\": \"volume\"\n",
    "            })\n",
    "            data = data.drop(columns=[\"volume\"])\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "            data = data.set_index(\"date_time\")\n",
    "            print(\"converting index to date_time\")\n",
    "            data.index = pd.to_datetime(data.index,\n",
    "                                        format=\"%m/%d/%Y %H:%M:%S.%f\")\n",
    "            print(\"index converted\")\n",
    "            # Should do something else than drop the duplicates (maybe trades doesnt have duplicate indexes rather than aggtrades)\n",
    "            data = data.loc[~data.index.duplicated(keep=\"first\")]\n",
    "            # skip most rows if this is > 1\n",
    "            if parameters[\"skip\"] > 0:\n",
    "                data = data.iloc[::parameters[\"skip\"], :]\n",
    "        if parameters[\"input\"] == \"obook\":\n",
    "            with open(path_adjust + \"temp/orderbook_data_name.txt\",\n",
    "                      \"r\") as text_file:\n",
    "                orderbook_preprocessed_file_name = text_file.read()\n",
    "            h5f = h5py.File(\n",
    "                path_adjust + \"data/orderbook_preprocessed/\" +\n",
    "                orderbook_preprocessed_file_name + \".h5\",\n",
    "                \"r\",\n",
    "            )\n",
    "            volumes_index_as_epoch = h5f[\"volumes_index_as_epoch\"][:]\n",
    "            volumes_np_array = h5f[\"volumes_np_array\"][:]\n",
    "            df_index_as_epoch = h5f[\"df_index_as_epoch\"][:]\n",
    "            df_np_array = h5f[\"df_np_array\"][:]\n",
    "            h5f.close()\n",
    "            volumes = pd.DataFrame(data=volumes_np_array,\n",
    "                                   index=volumes_index_as_epoch)\n",
    "            volumes.index = pd.to_datetime(volumes.index, unit=\"ms\")\n",
    "            data = pd.DataFrame(data=df_np_array, index=df_index_as_epoch)\n",
    "            data.index = pd.to_datetime(data.index, unit=\"ms\")\n",
    "            data.columns = [\"close\"]\n",
    "            data.index.name = \"date_time\"\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "        print(\"data load finished\")\n",
    "        # Checking for duplicates\n",
    "        # duplicate_fast_search(data.index.duplicated())\n",
    "        # Fractional differentiation\n",
    "        if parameters[\"fd\"] == \"on\":\n",
    "            data_series = data[\"close\"].to_frame()\n",
    "            # # generate 100 points\n",
    "            # nsample = 1000\n",
    "            # ## simulate a simple sinusoidal function\n",
    "            # x1 = np.linspace(0, 10, nsample)\n",
    "            # y = pd.Series(1*np.sin(2.0 * x1 + .5))\n",
    "            # y.plot()\n",
    "            # c_constant = 1.\n",
    "            # y_shifted = (y + c_constant).cumsum().rename('Shifted_series').to_frame()\n",
    "            # y_shifted.plot()\n",
    "            # df = y_shifted\n",
    "            # # df=(df-df.mean())/df.std()\n",
    "            # df['Shifted_series'][1:] = np.diff(df['Shifted_series'].values)\n",
    "            # df['Shifted_series'].plot()\n",
    "            kwargs = None\n",
    "            # data_series = np.log(data_series)  ## is it good to log this?\n",
    "            frac_diff_series, d = get_opt_d(  # reduces the number of rows and ends up with less vertical barriers touched\n",
    "                data_series,\n",
    "                ds=None,\n",
    "                maxlag=None,  # If we use raw tick data need at least head > 8000\n",
    "                thres=1e-5,\n",
    "                max_size=10000,\n",
    "                p_thres=1e-2,\n",
    "                autolag=None,\n",
    "                verbose=1,\n",
    "            )\n",
    "            data[\"close\"] = frac_diff_series\n",
    "            data = data.dropna(subset=[\"close\"])\n",
    "        data[\"window_volatility_level\"] = np.nan\n",
    "        start = time.time()\n",
    "        volatility_level_array = volatility_levels_numba(\n",
    "            np.ascontiguousarray(data.close.values), parameters[\"WL\"])\n",
    "        data[\"window_volatility_level\"] = volatility_level_array\n",
    "        # Should adjust the max value\n",
    "        # To get more vertical touches we can\n",
    "        # either increase vol_max or\n",
    "        # decrease the window seconds\n",
    "        scaler = MinMaxScaler(\n",
    "            feature_range=(parameters[\"vol_min\"],\n",
    "                           parameters[\"vol_max\"]))  # normalization\n",
    "        normed_window_volatility_level = scaler.fit_transform(\n",
    "            data[[\"window_volatility_level\"]])\n",
    "        data[\"window_volatility_level\"] = normed_window_volatility_level  #\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "        # CUSUM FILTER\n",
    "        volatility_threshold = data[\"window_volatility_level\"].mean()\n",
    "        close_copy = data.dropna().close.copy(deep=True)\n",
    "        close_np_array, close_index_np_array = pandas_series_to_numba_ready_np_arrays(\n",
    "            close_copy)\n",
    "        volatility_threshold = volatility_threshold * parameters[\"cm_vol_mod\"]\n",
    "        print(\"data_len = \" + str(len(data)))\n",
    "        start = time.time()\n",
    "        sampled_idx = filter_events(\n",
    "            data,\n",
    "            close_np_array,\n",
    "            close_index_np_array,\n",
    "            volatility_threshold,\n",
    "            parameters[\"filter\"],\n",
    "        )\n",
    "        print(\"sampled_idx_len = \" + str(len(sampled_idx)))\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "    if stage == 2:\n",
    "        # size\n",
    "        start = time.time()\n",
    "        volatility_level_array = volatility_levels_numba(\n",
    "            data.close.values, parameters[\"WL\"])\n",
    "        data[\"window_volatility_level\"] = volatility_level_array\n",
    "    # This code runs for both first and second stage preprocessing\n",
    "    start = time.time()\n",
    "    vertical_barrier_timestamps = ml.labeling.add_vertical_barrier(\n",
    "        t_events=sampled_idx,\n",
    "        close=data[\"close\"],\n",
    "        num_seconds=parameters[\"vbs\"])\n",
    "    end = time.time()\n",
    "    print(\"vertical barrier\" + str(end - start))\n",
    "    start = time.time()\n",
    "    print(\"Getting triple barrier events\")\n",
    "    triple_barrier_events = ml.labeling.get_events(\n",
    "        close=data[\"close\"],\n",
    "        t_events=sampled_idx,\n",
    "        pt_sl=pt_sl,\n",
    "        target=data[\"window_volatility_level\"],\n",
    "        min_ret=parameters[\"min_ret\"],\n",
    "        num_threads=cpus * 2,\n",
    "        vertical_barrier_times=vertical_barrier_timestamps,\n",
    "        side_prediction=side,\n",
    "        split_by=\n",
    "        100  # maybe we want this as large as we can while still fitting in ram\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"triple_barrier_events finished taking\" + str(end - start))\n",
    "    very_end = time.time()\n",
    "    with open(path_adjust + \"temp/is_script_finished.txt\", \"w+\") as text_file:\n",
    "        text_file.write(\"full_script_time\" + str(very_end - very_start))\n",
    "    start_time = time.time()\n",
    "    print(\"Returning Bins\")\n",
    "    labels = ml.labeling.get_bins(triple_barrier_events, data[\"close\"])\n",
    "    labels = ml.labeling.drop_labels(labels)\n",
    "    label_counts = labels.bin.value_counts()\n",
    "    print(\"label_counts\" + str(label_counts))\n",
    "    end_time = time.time()\n",
    "    print(\"returning bins finished taking\" + str(end_time - start_time))\n",
    "    # unique, counts = np.unique(y, return_counts=True)\n",
    "    sampled_idx_epoch = sampled_idx.astype(np.int64)\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/sampled_idx_epoch.h5\",\n",
    "        \"w\")\n",
    "    h5f.create_dataset(\"sampled_idx_epoch\", data=sampled_idx_epoch)\n",
    "    h5f.close()\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(labels)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/labels.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(data)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/data.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(triple_barrier_events)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/triple_barrier_events.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "else:\n",
    "    labels = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/labels.parquet\").to_pandas()\n",
    "    data = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/data.parquet\").to_pandas()\n",
    "    triple_barrier_events = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/triple_barrier_events.parquet\"\n",
    "    ).to_pandas()\n",
    "    with open(path_adjust + \"temp/orderbook_data_name.txt\", \"r\") as text_file:\n",
    "        orderbook_preprocessed_file_name = text_file.read()\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/orderbook_preprocessed/\" +\n",
    "        orderbook_preprocessed_file_name + \".h5\", \"r\")\n",
    "    volumes_index_as_epoch = h5f[\"volumes_index_as_epoch\"][:]\n",
    "    volumes_np_array = h5f[\"volumes_np_array\"][:]\n",
    "    h5f.close()\n",
    "    volumes = pd.DataFrame(data=volumes_np_array, index=volumes_index_as_epoch)\n",
    "    volumes.index = pd.to_datetime(volumes.index, unit=\"ms\")\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/sampled_idx_epoch.h5\",\n",
    "        \"r\")\n",
    "    sampled_idx_epoch = h5f[\"sampled_idx_epoch\"][:]\n",
    "    h5f.close()\n",
    "    sampled_idx = pd.DatetimeIndex(sampled_idx_epoch)\n",
    "# y_dataframe = labels[\"bin\"]\n",
    "# data[\"bins\"] = labels[\"bin\"]\n",
    "# y = np.asarray(y_dataframe)\n",
    "# end = time.time()\n",
    "# y = keras.utils.to_categorical(y, num_classes=3)\n",
    "# if stage == 1:\n",
    "#     start_time = time.time()\n",
    "#     # side\n",
    "#     X_for_all_labels = data.loc[labels.index, :]\n",
    "#     end_time = time.time()\n",
    "#     print(end_time - start_time)\n",
    "#     # ### FOR HIGHWAY RNN\n",
    "#     # X = np.asarray(volumes.loc[labels.index, :])\n",
    "#     # h5f = h5py.File(path_adjust + \"data/preprocessed/\" + parameter_string + \".h5\", \"w\")\n",
    "#     # h5f.create_dataset(\"X\", data=X)\n",
    "#     # h5f.create_dataset(\"y\", data=y)\n",
    "#     # h5f.close()\n",
    "#     # X = []\n",
    "#     start_time = time.time()\n",
    "#     prices_for_window = data.loc[X_for_all_labels.index]\n",
    "#     prices_for_window_index = prices_for_window.index.astype(np.int64)\n",
    "#     prices_for_window_index_array = np.asarray(prices_for_window_index)\n",
    "#     end_time = time.time()\n",
    "#     print(end_time - start_time)\n",
    "#     start_time = time.time()\n",
    "#     close_index = data.close.index.astype(np.int64)\n",
    "#     close_index_array = np.asarray(close_index)\n",
    "#     end_time = time.time()\n",
    "#     print(end_time - start_time)\n",
    "#     start_time = time.time()\n",
    "#     # Make a new column time since last bar\n",
    "#     unindexed_data = data.reset_index()\n",
    "#     unindexed_data[\"shifted_date_time\"] = unindexed_data[\"date_time\"].shift(1)\n",
    "#     unindexed_data[\"time_since_last_bar\"] = (unindexed_data[\"date_time\"].sub(\n",
    "#         unindexed_data[\"shifted_date_time\"], axis=0).dt.seconds)\n",
    "#     unindexed_data = unindexed_data.set_index(\"date_time\")\n",
    "#     data[\"time_since_last_bar\"] = unindexed_data[\"time_since_last_bar\"]\n",
    "#     data[\"time_since_last_bar\"].iloc[0] = 0\n",
    "#     end_time = time.time()\n",
    "#     print(end_time - start_time)\n",
    "#     start_time = time.time()\n",
    "#     ### ORDERBOOK VOLUME DATA\n",
    "#     volumes_for_all_labels = volumes.loc[data.close.index]\n",
    "#     ## TRADE DATA\n",
    "#     input_features_trade = []\n",
    "#     close_array = data.close.values\n",
    "#     input_features_trade.append(close_array)\n",
    "#     if parameters[\"ntb\"] == False and parameters[\"vbc\"] == True:\n",
    "#         volume_array = data.volume.values\n",
    "#         input_features_trade.append(volume_array)\n",
    "#     if parameters[\"ntb\"] == True and parameters[\"tslbc\"] == True:\n",
    "#         time_since_last_bar_array = data.time_since_last_bar.values\n",
    "#         input_features_trade.append(time_since_last_bar_array)\n",
    "#     end_time = time.time()\n",
    "#     print(end_time - start_time)\n",
    "#     # min max limits\n",
    "#     minimum = -1\n",
    "#     maximum = 1\n",
    "#     scaling_type = \"z_score\"\n",
    "#     ### Split intothe training/validation/test sets\n",
    "#     print(\"splitting into train/va/test sets start\")\n",
    "#     start_time = time.time()\n",
    "#     prices_for_window_index_array_train_and_val = prices_for_window_index_array[:round(\n",
    "#         len(prices_for_window_index_array) * 0.8)]\n",
    "#     y_train_and_val = y[:round(len(y) * 0.8)]\n",
    "#     prices_for_window_index_array_train = prices_for_window_index_array_train_and_val[:round(\n",
    "#         len(prices_for_window_index_array_train_and_val) * 0.8)]\n",
    "#     y_train = y_train_and_val[:round(len(y_train_and_val) * 0.8)]\n",
    "#     train_close_array_integer_index = np.nonzero(\n",
    "#         np.in1d(close_index_array, prices_for_window_index_array_train))[0]\n",
    "#     volumes_for_all_labels_train = volumes_for_all_labels.iloc[\n",
    "#         train_close_array_integer_index[0] -\n",
    "#         parameters[\"WL\"]:train_close_array_integer_index[-1] + 2]\n",
    "#     close_index_array_train = close_index_array[\n",
    "#         train_close_array_integer_index[0] -\n",
    "#         parameters[\"WL\"]:train_close_array_integer_index[-1] + 2]\n",
    "#     end_time = time.time()\n",
    "#     print(\"splitting into train/va/test sets finished\" +\n",
    "#           str(end_time - start_time))\n",
    "#     print(\"Make input features from orderbook data started\")\n",
    "#     start_time = time.time()\n",
    "#     # MAKE WINDOW FROM INPUTS\n",
    "#     input_features_train = make_input_features_from_orderbook_data(\n",
    "#         volumes_for_all_labels_train)\n",
    "#     end_time = time.time()\n",
    "#     print(\"Make input features from orderbook data started \" +\n",
    "#           str(end_time - start_time))\n",
    "#     print(\"Get train scalers started\")\n",
    "#     start_time = time.time()\n",
    "#     ### 2: Apply the normalisation based on the training fit scalars (-1,1 vs 0,1 min max test both)\n",
    "#     maxes_or_means_np_array_train, mins_or_stds_np_array_train = get_fit_scalars(\n",
    "#         scaling_type, input_features_train)\n",
    "#     end_time = time.time()\n",
    "#     print(\"Get train scalers finished\" + str(end_time - start_time))\n",
    "#     print(\"Norm train data started\")\n",
    "#     start_time = time.time()\n",
    "#     # Norm train\n",
    "#     input_features_normalized_train = scale_input_features(\n",
    "#         scaling_type,\n",
    "#         maxes_or_means_np_array_train,\n",
    "#         mins_or_stds_np_array_train,\n",
    "#         input_features_train,\n",
    "#         minimum,\n",
    "#         maximum,\n",
    "#     )\n",
    "#     end_time = time.time()\n",
    "#     print(\"Get train scalers finished\" + str(end_time - start_time))\n",
    "#     # print(\"Make window started\")\n",
    "#     # start_time = time.time()\n",
    "#     # padding = parameters[\"WL\"] * 2\n",
    "#     # split_by = 100000\n",
    "#     # number_of_splits = len(prices_for_window_index_array_train) // split_by\n",
    "#     # for i in range(number_of_splits):\n",
    "#     #     print(i)\n",
    "#     #     start_time = time.time()\n",
    "#     #     if i == 0:\n",
    "#     #         start_index = None  # 0\n",
    "#     #         end_index = (i + 1) * split_by\n",
    "#     #         close_and_input_start_index = start_index\n",
    "#     #         close_and_input_end_index = end_index + (padding)\n",
    "#     #     elif i < number_of_splits - 1:\n",
    "#     #         start_index = i * split_by\n",
    "#     #         end_index = (i + 1) * split_by\n",
    "#     #         close_and_input_start_index = start_index - (padding)\n",
    "#     #         close_and_input_end_index = end_index + (padding)\n",
    "#     #     elif i == number_of_splits - 1:\n",
    "#     #         start_index = i * split_by\n",
    "#     #         end_index = None  # -1\n",
    "#     #         close_and_input_start_index = start_index - (padding)\n",
    "#     #         close_and_input_end_index = end_index\n",
    "#     #     # Window train\n",
    "#     #     X_train_section = make_window_multivariate_numba(\n",
    "#     #         len(prices_for_window_index_array_train[start_index:end_index]),\n",
    "#     #         input_features_normalized_train[:, close_and_input_start_index:\n",
    "#     #                                         close_and_input_end_index],\n",
    "#     #         parameters[\"WL\"],\n",
    "#     #         model_arch,\n",
    "#     #     )\n",
    "#     #     print(X_train_section.shape)\n",
    "#     #     hdf5_epath = path_adjust + \"data/preprocessed/X_and_y.h5\"\n",
    "#     #     if os.path.exists(hdf5_epath) == False or i == 0:\n",
    "#     #         h5f = tb.open_file(hdf5_epath, mode=\"a\")\n",
    "#     #         dataGroup = h5f.create_group(h5f.root, \"MyData\")\n",
    "#     #         h5f.create_earray(dataGroup,\n",
    "#     #                           \"X_train_section\",\n",
    "#     #                           obj=X_train_section)\n",
    "#     #         h5f.close()\n",
    "#     #     else:\n",
    "#     #         h5f = tb.open_file(hdf5_epath, mode=\"r+\")\n",
    "#     #         h5f.root.MyData.X_train_section.append(X_train_section)\n",
    "#     #         h5f.close()\n",
    "#     #     # h5f = h5py.File(path_adjust + \"data/preprocessed/X_and_y.h5\", \"w\")\n",
    "#     #     # h5f.create_dataset(\"X_train_section\", data=X_train_section)\n",
    "#     #     # h5f.close()\n",
    "#     #     end_time = time.time()\n",
    "#     #     print(str(i) + \" finished taking \" + str(end_time - start_time))\n",
    "#     # end_time = time.time()\n",
    "#     # print(\"Make window finished\" + str(end_time - start_time))\n",
    "#     #     # import pdb\n",
    "#     #     # pdb.set_trace()\n",
    "#     # Val\n",
    "#     prices_for_window_index_array_val = prices_for_window_index_array_train_and_val[\n",
    "#         round(len(prices_for_window_index_array_train_and_val) * 0.8):]\n",
    "#     y_val = y_train_and_val[round(len(y_train_and_val) * 0.8):]\n",
    "#     val_close_array_integer_index = np.nonzero(\n",
    "#         np.in1d(close_index_array, prices_for_window_index_array_val))[0]\n",
    "#     volumes_for_all_labels_val = volumes_for_all_labels.iloc[\n",
    "#         val_close_array_integer_index[0] -\n",
    "#         parameters[\"WL\"]:val_close_array_integer_index[-1] + 2]\n",
    "#     close_index_array_val = close_index_array[\n",
    "#         val_close_array_integer_index[0] -\n",
    "#         parameters[\"WL\"]:val_close_array_integer_index[-1] + 2]\n",
    "#     input_features_val = make_input_features_from_orderbook_data(\n",
    "#         volumes_for_all_labels_val)\n",
    "#     # Norm val\n",
    "#     input_features_normalized_val = scale_input_features(\n",
    "#         scaling_type,\n",
    "#         maxes_or_means_np_array_train,\n",
    "#         mins_or_stds_np_array_train,\n",
    "#         input_features_val,\n",
    "#         minimum,\n",
    "#         maximum,\n",
    "#     )\n",
    "#     #     # # Make window val\n",
    "#     #     # X_val = make_window_multivariate_numba(\n",
    "#     #     #     prices_for_window_index_array_val,\n",
    "#     #     #     close_index_array_val,\n",
    "#     #     #     input_features_normalized_val,\n",
    "#     #     #     parameters[\"WL\"],\n",
    "#     #     #     model_arch,\n",
    "#     #     # )\n",
    "#     # Test\n",
    "#     prices_for_window_index_array_test = prices_for_window_index_array[\n",
    "#         round(len(prices_for_window_index_array) * 0.8):]\n",
    "#     y_test = y[round(len(y) * 0.8):]\n",
    "#     test_close_array_integer_index = np.nonzero(\n",
    "#         np.in1d(close_index_array, prices_for_window_index_array_test))[0]\n",
    "#     volumes_for_all_labels_test = volumes_for_all_labels.iloc[\n",
    "#         test_close_array_integer_index[0] -\n",
    "#         parameters[\"WL\"]:test_close_array_integer_index[-1] + 2]\n",
    "#     close_index_array_test = close_index_array[\n",
    "#         test_close_array_integer_index[0] -\n",
    "#         parameters[\"WL\"]:test_close_array_integer_index[-1] + 2]\n",
    "#     input_features_test = make_input_features_from_orderbook_data(\n",
    "#         volumes_for_all_labels_test)\n",
    "#     # Norm test\n",
    "#     input_features_normalized_test = scale_input_features(\n",
    "#         scaling_type,\n",
    "#         maxes_or_means_np_array_train,\n",
    "#         mins_or_stds_np_array_train,\n",
    "#         input_features_test,\n",
    "#         minimum,\n",
    "#         maximum,\n",
    "#     )\n",
    "# #     # # Window test\n",
    "# #     # # TABL\n",
    "# #     # X_test = make_window_multivariate_numba(\n",
    "# #     #     prices_for_window_index_array_test,\n",
    "# #     #     close_index_array_test,\n",
    "# #     #     input_features_normalized_test,\n",
    "# #     #     parameters[\"WL\"],\n",
    "# #     #     model_arch,\n",
    "# #     # )\n",
    "# #     start = time.time()\n",
    "# #     end = time.time()\n",
    "# #     print(\"numba make window time\" + str(end - start))\n",
    "# #     start = time.time()\n",
    "# print(\"plotting started\")\n",
    "# # plot the whole sequence\n",
    "# ax = plt.gca()\n",
    "# data.plot(y=\"close\", use_index=True)\n",
    "# window_index = 500\n",
    "# ax = plt.gca()\n",
    "# data.iloc[window_index - 200:window_index + 200].plot(y=\"close\",\n",
    "#                                                       use_index=True)\n",
    "# plot_window_and_touch_and_label(window_index, parameters[\"WL\"], data,\n",
    "#                                 triple_barrier_events, labels)\n",
    "# data.iloc[window_index - 10:window_index + 30]\n",
    "# print(\"plotting finished\")\n",
    "# # Sample Weights\n",
    "# # if stage == 1:\n",
    "# #     weights = attribution.get_weights_by_return(triple_barrier_events.dropna(),\n",
    "# #                                                 data.close,\n",
    "# #                                                 num_threads=5)\n",
    "# #     sample_weights = np.asarray(weights)\n",
    "# #     sample_weights = sample_weights.reshape(len(sample_weights))\n",
    "# #     sampled_idx_epoch = sampled_idx.astype(np.int64)  #\n",
    "# if stage == 2:\n",
    "#     # size\n",
    "#     parameter_string = parameter_string + \"second_stage\"\n",
    "# print(\"writing train/val/test to .h5 files starting\")\n",
    "# start = time.time()\n",
    "# # Writing preprocessed X,y\n",
    "# h5f = h5py.File(path_adjust + \"data/preprocessed/\" + parameter_string + \".h5\",\n",
    "#                 \"w\")\n",
    "# h5f.create_dataset(\"prices_for_window_index_array_train\",\n",
    "#                    data=prices_for_window_index_array_train)\n",
    "# h5f.create_dataset(\"prices_for_window_index_array_val\",\n",
    "#                    data=prices_for_window_index_array_val)\n",
    "# h5f.create_dataset(\"prices_for_window_index_array_test\",\n",
    "#                    data=prices_for_window_index_array_test)\n",
    "# h5f.create_dataset(\"close_index_array_train\", data=close_index_array_train)\n",
    "# h5f.create_dataset(\"close_index_array_val\", data=close_index_array_val)\n",
    "# h5f.create_dataset(\"close_index_array_test\", data=close_index_array_test)\n",
    "# h5f.create_dataset(\"input_features_normalized_train\",\n",
    "#                    data=input_features_normalized_train)\n",
    "# h5f.create_dataset(\"input_features_normalized_val\",\n",
    "#                    data=input_features_normalized_val)\n",
    "# h5f.create_dataset(\"input_features_normalized_test\",\n",
    "#                    data=input_features_normalized_test)\n",
    "# h5f.create_dataset(\"y_train\", data=y_train)\n",
    "# h5f.create_dataset(\"y_val\", data=y_val)\n",
    "# h5f.create_dataset(\"y_test\", data=y_test)\n",
    "# end_time = time.time()\n",
    "# print(\"writing train/val/test to .h5 files finished taking \" +\n",
    "#       str(end_time - start_time))\n",
    "# # if stage == 2:\n",
    "# #     # size\n",
    "# #     h5f.create_dataset(\"P\", data=P)\n",
    "# # h5f.create_dataset(\"y\", data=y)\n",
    "# # h5f.create_dataset(\"sampled_idx_epoch\", data=sampled_idx_epoch)\n",
    "# # if parameters[\"sw\"] == \"on\":\n",
    "# #     h5f.create_dataset(\"sample_weights\", data=sample_weights)\n",
    "# # elif parameters[\"sw\"] == \"off\":\n",
    "# #     h5f.create_dataset(\"sample_weights\", data=np.zeros(1))\n",
    "# # h5f.close()\n",
    "# # # save data dataframe\n",
    "# # table = pa.Table.from_pandas(data)\n",
    "# # pq.write_table(\n",
    "# #     table,\n",
    "# #     path_adjust + \"data/preprocessed/\" + parameter_string + \"_data.parquet\",\n",
    "# #     use_dictionary=True,\n",
    "# #     compression=\"snappy\",\n",
    "# #     use_deprecated_int96_timestamps=True,\n",
    "# # )\n",
    "# with open(path_adjust + \"temp/data_name.txt\", \"w+\") as text_file:\n",
    "#     text_file.write(parameter_string)\n",
    "# very_end = time.time()\n",
    "# print(\"full_script_time\" + str(very_end - very_start))\n",
    "# print(parameter_string + \".h5\")\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dataframe = labels[\"bin\"]\n",
    "data[\"bins\"] = labels[\"bin\"]\n",
    "y = np.asarray(y_dataframe)\n",
    "end = time.time()\n",
    "y = keras.utils.to_categorical(y, num_classes=3)\n",
    "\n",
    "if stage == 1:\n",
    "    start_time = time.time()\n",
    "    # side\n",
    "    X_for_all_labels = data.loc[labels.index, :]\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    # ### FOR HIGHWAY RNN\n",
    "    # X = np.asarray(volumes.loc[labels.index, :])\n",
    "    # h5f = h5py.File(path_adjust + \"data/preprocessed/\" + parameter_string + \".h5\", \"w\")\n",
    "    # h5f.create_dataset(\"X\", data=X)\n",
    "    # h5f.create_dataset(\"y\", data=y)\n",
    "    # h5f.close()\n",
    "    # X = []\n",
    "    start_time = time.time()\n",
    "    prices_for_window = data.loc[X_for_all_labels.index]\n",
    "    prices_for_window_index = prices_for_window.index.astype(np.int64)\n",
    "    prices_for_window_index_array = np.asarray(prices_for_window_index)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    start_time = time.time()\n",
    "    close_index = data.close.index.astype(np.int64)\n",
    "    close_index_array = np.asarray(close_index)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    start_time = time.time()\n",
    "    # Make a new column time since last bar\n",
    "    unindexed_data = data.reset_index()\n",
    "    unindexed_data[\"shifted_date_time\"] = unindexed_data[\"date_time\"].shift(1)\n",
    "    unindexed_data[\"time_since_last_bar\"] = (unindexed_data[\"date_time\"].sub(\n",
    "        unindexed_data[\"shifted_date_time\"], axis=0).dt.seconds)\n",
    "    unindexed_data = unindexed_data.set_index(\"date_time\")\n",
    "    data[\"time_since_last_bar\"] = unindexed_data[\"time_since_last_bar\"]\n",
    "    data[\"time_since_last_bar\"].iloc[0] = 0\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    start_time = time.time()\n",
    "    ### ORDERBOOK VOLUME DATA\n",
    "    volumes_for_all_labels = volumes.loc[data.close.index]\n",
    "    ## TRADE DATA\n",
    "    input_features_trade = []\n",
    "    close_array = data.close.values\n",
    "    input_features_trade.append(close_array)\n",
    "    if parameters[\"ntb\"] == False and parameters[\"vbc\"] == True:\n",
    "        volume_array = data.volume.values\n",
    "        input_features_trade.append(volume_array)\n",
    "    if parameters[\"ntb\"] == True and parameters[\"tslbc\"] == True:\n",
    "        time_since_last_bar_array = data.time_since_last_bar.values\n",
    "        input_features_trade.append(time_since_last_bar_array)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    # min max limits\n",
    "    minimum = -1\n",
    "    maximum = 1\n",
    "    scaling_type = \"z_score\"\n",
    "    ### Split intothe training/validation/test sets\n",
    "    print(\"splitting into train/va/test sets start\")\n",
    "    start_time = time.time()\n",
    "    prices_for_window_index_array_train_and_val = prices_for_window_index_array[:round(\n",
    "        len(prices_for_window_index_array) * 0.8)]\n",
    "    y_train_and_val = y[:round(len(y) * 0.8)]\n",
    "    prices_for_window_index_array_train = prices_for_window_index_array_train_and_val[:round(\n",
    "        len(prices_for_window_index_array_train_and_val) * 0.8)]\n",
    "    y_train = y_train_and_val[:round(len(y_train_and_val) * 0.8)]\n",
    "    train_close_array_integer_index = np.nonzero(\n",
    "        np.in1d(close_index_array, prices_for_window_index_array_train))[0]\n",
    "    volumes_for_all_labels_train = volumes_for_all_labels.iloc[\n",
    "        train_close_array_integer_index[0] -\n",
    "        parameters[\"WL\"]:train_close_array_integer_index[-1] + 2]\n",
    "    close_index_array_train = close_index_array[\n",
    "        train_close_array_integer_index[0] -\n",
    "        parameters[\"WL\"]:train_close_array_integer_index[-1] + 2]\n",
    "    end_time = time.time()\n",
    "    print(\"splitting into train/va/test sets finished\" +\n",
    "          str(end_time - start_time))\n",
    "    print(\"Make input features from orderbook data started\")\n",
    "    start_time = time.time()\n",
    "    # MAKE WINDOW FROM INPUTS\n",
    "    input_features_train = make_input_features_from_orderbook_data(\n",
    "        volumes_for_all_labels_train)\n",
    "    end_time = time.time()\n",
    "    print(\"Make input features from orderbook data started \" +\n",
    "          str(end_time - start_time))\n",
    "    print(\"Get train scalers started\")\n",
    "    start_time = time.time()\n",
    "    ### 2: Apply the normalisation based on the training fit scalars (-1,1 vs 0,1 min max test both)\n",
    "    maxes_or_means_np_array_train, mins_or_stds_np_array_train = get_fit_scalars(\n",
    "        scaling_type, input_features_train)\n",
    "    end_time = time.time()\n",
    "    print(\"Get train scalers finished\" + str(end_time - start_time))\n",
    "    print(\"Norm train data started\")\n",
    "    start_time = time.time()\n",
    "    # Norm train\n",
    "    input_features_normalized_train = scale_input_features(\n",
    "        scaling_type,\n",
    "        maxes_or_means_np_array_train,\n",
    "        mins_or_stds_np_array_train,\n",
    "        input_features_train,\n",
    "        minimum,\n",
    "        maximum,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(\"Get train scalers finished\" + str(end_time - start_time))\n",
    "    # print(\"Make window started\")\n",
    "    # start_time = time.time()\n",
    "    # padding = parameters[\"WL\"] * 2\n",
    "    # split_by = 100000\n",
    "    # number_of_splits = len(prices_for_window_index_array_train) // split_by\n",
    "    # for i in range(number_of_splits):\n",
    "    #     print(i)\n",
    "    #     start_time = time.time()\n",
    "    #     if i == 0:\n",
    "    #         start_index = None  # 0\n",
    "    #         end_index = (i + 1) * split_by\n",
    "    #         close_and_input_start_index = start_index\n",
    "    #         close_and_input_end_index = end_index + (padding)\n",
    "    #     elif i < number_of_splits - 1:\n",
    "    #         start_index = i * split_by\n",
    "    #         end_index = (i + 1) * split_by\n",
    "    #         close_and_input_start_index = start_index - (padding)\n",
    "    #         close_and_input_end_index = end_index + (padding)\n",
    "    #     elif i == number_of_splits - 1:\n",
    "    #         start_index = i * split_by\n",
    "    #         end_index = None  # -1\n",
    "    #         close_and_input_start_index = start_index - (padding)\n",
    "    #         close_and_input_end_index = end_index\n",
    "    #     # Window train\n",
    "    #     X_train_section = make_window_multivariate_numba(\n",
    "    #         len(prices_for_window_index_array_train[start_index:end_index]),\n",
    "    #         input_features_normalized_train[:, close_and_input_start_index:\n",
    "    #                                         close_and_input_end_index],\n",
    "    #         parameters[\"WL\"],\n",
    "    #         model_arch,\n",
    "    #     )\n",
    "    #     print(X_train_section.shape)\n",
    "    #     hdf5_epath = path_adjust + \"data/preprocessed/X_and_y.h5\"\n",
    "    #     if os.path.exists(hdf5_epath) == False or i == 0:\n",
    "    #         h5f = tb.open_file(hdf5_epath, mode=\"a\")\n",
    "    #         dataGroup = h5f.create_group(h5f.root, \"MyData\")\n",
    "    #         h5f.create_earray(dataGroup,\n",
    "    #                           \"X_train_section\",\n",
    "    #                           obj=X_train_section)\n",
    "    #         h5f.close()\n",
    "    #     else:\n",
    "    #         h5f = tb.open_file(hdf5_epath, mode=\"r+\")\n",
    "    #         h5f.root.MyData.X_train_section.append(X_train_section)\n",
    "    #         h5f.close()\n",
    "    #     # h5f = h5py.File(path_adjust + \"data/preprocessed/X_and_y.h5\", \"w\")\n",
    "    #     # h5f.create_dataset(\"X_train_section\", data=X_train_section)\n",
    "    #     # h5f.close()\n",
    "    #     end_time = time.time()\n",
    "    #     print(str(i) + \" finished taking \" + str(end_time - start_time))\n",
    "    # end_time = time.time()\n",
    "    # print(\"Make window finished\" + str(end_time - start_time))\n",
    "    #     # import pdb\n",
    "    #     # pdb.set_trace()\n",
    "    # Val\n",
    "    prices_for_window_index_array_val = prices_for_window_index_array_train_and_val[\n",
    "        round(len(prices_for_window_index_array_train_and_val) * 0.8):]\n",
    "    y_val = y_train_and_val[round(len(y_train_and_val) * 0.8):]\n",
    "    val_close_array_integer_index = np.nonzero(\n",
    "        np.in1d(close_index_array, prices_for_window_index_array_val))[0]\n",
    "    volumes_for_all_labels_val = volumes_for_all_labels.iloc[\n",
    "        val_close_array_integer_index[0] -\n",
    "        parameters[\"WL\"]:val_close_array_integer_index[-1] + 2]\n",
    "    close_index_array_val = close_index_array[\n",
    "        val_close_array_integer_index[0] -\n",
    "        parameters[\"WL\"]:val_close_array_integer_index[-1] + 2]\n",
    "    input_features_val = make_input_features_from_orderbook_data(\n",
    "        volumes_for_all_labels_val)\n",
    "    # Norm val\n",
    "    input_features_normalized_val = scale_input_features(\n",
    "        scaling_type,\n",
    "        maxes_or_means_np_array_train,\n",
    "        mins_or_stds_np_array_train,\n",
    "        input_features_val,\n",
    "        minimum,\n",
    "        maximum,\n",
    "    )\n",
    "    #     # # Make window val\n",
    "    #     # X_val = make_window_multivariate_numba(\n",
    "    #     #     prices_for_window_index_array_val,\n",
    "    #     #     close_index_array_val,\n",
    "    #     #     input_features_normalized_val,\n",
    "    #     #     parameters[\"WL\"],\n",
    "    #     #     model_arch,\n",
    "    #     # )\n",
    "    # Test\n",
    "    prices_for_window_index_array_test = prices_for_window_index_array[\n",
    "        round(len(prices_for_window_index_array) * 0.8):]\n",
    "    y_test = y[round(len(y) * 0.8):]\n",
    "    test_close_array_integer_index = np.nonzero(\n",
    "        np.in1d(close_index_array, prices_for_window_index_array_test))[0]\n",
    "    volumes_for_all_labels_test = volumes_for_all_labels.iloc[\n",
    "        test_close_array_integer_index[0] -\n",
    "        parameters[\"WL\"]:test_close_array_integer_index[-1] + 2]\n",
    "    close_index_array_test = close_index_array[\n",
    "        test_close_array_integer_index[0] -\n",
    "        parameters[\"WL\"]:test_close_array_integer_index[-1] + 2]\n",
    "    input_features_test = make_input_features_from_orderbook_data(\n",
    "        volumes_for_all_labels_test)\n",
    "    # Norm test\n",
    "    input_features_normalized_test = scale_input_features(\n",
    "        scaling_type,\n",
    "        maxes_or_means_np_array_train,\n",
    "        mins_or_stds_np_array_train,\n",
    "        input_features_test,\n",
    "        minimum,\n",
    "        maximum,\n",
    "    )\n",
    "#     # # Window test\n",
    "#     # # TABL\n",
    "#     # X_test = make_window_multivariate_numba(\n",
    "#     #     prices_for_window_index_array_test,\n",
    "#     #     close_index_array_test,\n",
    "#     #     input_features_normalized_test,\n",
    "#     #     parameters[\"WL\"],\n",
    "#     #     model_arch,\n",
    "#     # )\n",
    "#     start = time.time()\n",
    "#     end = time.time()\n",
    "#     print(\"numba make window time\" + str(end - start))\n",
    "#     start = time.time()\n",
    "\n",
    "print(\"plotting started\")\n",
    "# plot the whole sequence\n",
    "\n",
    "ax = plt.gca()\n",
    "data.plot(y=\"close\", use_index=True)\n",
    "window_index = 500\n",
    "ax = plt.gca()\n",
    "\n",
    "data.iloc[window_index - 200:window_index + 200].plot(y=\"close\",\n",
    "                                                      use_index=True)\n",
    "\n",
    "plot_window_and_touch_and_label(window_index, parameters[\"WL\"], data,\n",
    "                                triple_barrier_events, labels)\n",
    "\n",
    "data.iloc[window_index - 10:window_index + 30]\n",
    "\n",
    "print(\"plotting finished\")\n",
    "# Sample Weights\n",
    "# if stage == 1:\n",
    "#     weights = attribution.get_weights_by_return(triple_barrier_events.dropna(),\n",
    "#                                                 data.close,\n",
    "#                                                 num_threads=5)\n",
    "#     sample_weights = np.asarray(weights)\n",
    "#     sample_weights = sample_weights.reshape(len(sample_weights))\n",
    "#     sampled_idx_epoch = sampled_idx.astype(np.int64)  #\n",
    "\n",
    "if stage == 2:\n",
    "    # size\n",
    "    parameter_string = parameter_string + \"second_stage\"\n",
    "\n",
    "print(\"writing train/val/test to .h5 files starting\")\n",
    "\n",
    "start = time.time()\n",
    "# Writing preprocessed X,y\n",
    "\n",
    "h5f = h5py.File(path_adjust + \"data/preprocessed/\" + parameter_string + \".h5\",\n",
    "                \"w\")\n",
    "\n",
    "h5f.create_dataset(\"prices_for_window_index_array_train\",\n",
    "                   data=prices_for_window_index_array_train)\n",
    "\n",
    "h5f.create_dataset(\"prices_for_window_index_array_val\",\n",
    "                   data=prices_for_window_index_array_val)\n",
    "\n",
    "h5f.create_dataset(\"prices_for_window_index_array_test\",\n",
    "                   data=prices_for_window_index_array_test)\n",
    "\n",
    "h5f.create_dataset(\"close_index_array_train\", data=close_index_array_train)\n",
    "h5f.create_dataset(\"close_index_array_val\", data=close_index_array_val)\n",
    "h5f.create_dataset(\"close_index_array_test\", data=close_index_array_test)\n",
    "\n",
    "h5f.create_dataset(\"input_features_normalized_train\",\n",
    "                   data=input_features_normalized_train)\n",
    "\n",
    "h5f.create_dataset(\"input_features_normalized_val\",\n",
    "                   data=input_features_normalized_val)\n",
    "\n",
    "h5f.create_dataset(\"input_features_normalized_test\",\n",
    "                   data=input_features_normalized_test)\n",
    "\n",
    "h5f.create_dataset(\"y_train\", data=y_train)\n",
    "h5f.create_dataset(\"y_val\", data=y_val)\n",
    "h5f.create_dataset(\"y_test\", data=y_test)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"writing train/val/test to .h5 files finished taking \" +\n",
    "      str(end_time - start_time))\n",
    "# if stage == 2:\n",
    "#     # size\n",
    "#     h5f.create_dataset(\"P\", data=P)\n",
    "# h5f.create_dataset(\"y\", data=y)\n",
    "# h5f.create_dataset(\"sampled_idx_epoch\", data=sampled_idx_epoch)\n",
    "# if parameters[\"sw\"] == \"on\":\n",
    "#     h5f.create_dataset(\"sample_weights\", data=sample_weights)\n",
    "# elif parameters[\"sw\"] == \"off\":\n",
    "#     h5f.create_dataset(\"sample_weights\", data=np.zeros(1))\n",
    "# h5f.close()\n",
    "# # save data dataframe\n",
    "# table = pa.Table.from_pandas(data)\n",
    "# pq.write_table(\n",
    "#     table,\n",
    "#     path_adjust + \"data/preprocessed/\" + parameter_string + \"_data.parquet\",\n",
    "#     use_dictionary=True,\n",
    "#     compression=\"snappy\",\n",
    "#     use_deprecated_int96_timestamps=True,\n",
    "# )\n",
    "\n",
    "with open(path_adjust + \"temp/data_name.txt\", \"w+\") as text_file:\n",
    "    text_file.write(parameter_string)\n",
    "\n",
    "very_end = time.time()\n",
    "print(\"full_script_time\" + str(very_end - very_start))\n",
    "\n",
    "print(parameter_string + \".h5\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"script initiated\")\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from numba import njit, prange\n",
    "import tensorflow as tf\n",
    "\n",
    "import math\n",
    "# Init wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"prodigyai\")\n",
    "wandb.run._dir = \"~/ProdigyAI/wandb/\" + wandb.run.dir.split('/')[-1]\n",
    "import keras\n",
    "import numpy as np\n",
    "from third_party_libraries.TABL import Models\n",
    "from third_party_libraries.keras_lr_finder.lr_finder import LRFinder\n",
    "from third_party_libraries.CLR.clr_callbacks import CyclicLR\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# import tensorflow.keras as keras\n",
    "# import tensorflow.keras.layers as layers\n",
    "# import tensorflow.keras.backend as K\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    args = parser.parse_args()\n",
    "    if args.resuming != None:\n",
    "        resuming = args.resuming\n",
    "    else:\n",
    "        resuming = \"NA\"\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "wandb.run.dir = path_adjust + \"logs/wandb\"\n",
    "# try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes\n",
    "# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1\n",
    "\n",
    "template = [[40, 200], [60, 10], [120, 5], [3, 1]]\n",
    "# example data\n",
    "\n",
    "example_x = np.random.rand(1000, 40, 10)\n",
    "np.min(example_x)\n",
    "np.max(example_x)\n",
    "np.mean(example_x)\n",
    "example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)\n",
    "\n",
    "template = [[40, 200], [60, 10], [120, 5], [3, 1]]\n",
    "# 200(WINDOW LENGTH)\n",
    "## PRODIGY AI HOCKUS POCKUS START\n",
    "\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "home = str(Path.home())\n",
    "file_name = \"arch=TABL&name=two_model&WL=200&pt=1&sl=1&min_ret=4.347826086956522e-05&vbs=0.5&head=1000&skip=0&vol_max=4.3498260869565224e-05&vol_min=4.348826086956522e-05&filter=none&cm_vol_mod=0&sw=on&fd=off&input=obook&ntb=True&tslbc=True.h5\"\n",
    "path = home + \"/ProdigyAI/data/preprocessed/\" + file_name\n",
    "h5f = h5py.File(path, \"r\")\n",
    "\n",
    "prices_for_window_index_array_train = h5f[\n",
    "    \"prices_for_window_index_array_train\"][:]\n",
    "\n",
    "prices_for_window_index_array_val = h5f[\"prices_for_window_index_array_val\"][:]\n",
    "\n",
    "prices_for_window_index_array_test = h5f[\n",
    "    \"prices_for_window_index_array_test\"][:]\n",
    "\n",
    "close_index_array_train = h5f[\"close_index_array_train\"][:]\n",
    "close_index_array_val = h5f[\"close_index_array_val\"][:]\n",
    "close_index_array_test = h5f[\"close_index_array_test\"][:]\n",
    "input_features_normalized_train = h5f[\"input_features_normalized_train\"][:]\n",
    "input_features_normalized_val = h5f[\"input_features_normalized_val\"][:]\n",
    "input_features_normalized_test = h5f[\"input_features_normalized_test\"][:]\n",
    "y_train = h5f[\"y_train\"][:]\n",
    "y_val = h5f[\"y_val\"][:]\n",
    "y_test = h5f[\"y_test\"][:]\n",
    "\n",
    "h5f.close()\n",
    "## PRODIGY AI HOCKUS POCKUS END\n",
    "\n",
    "for i in range(len(close_index_array_train)):\n",
    "    if close_index_array_train[i] == prices_for_window_index_array_train[0]:\n",
    "        offset = i\n",
    "        break\n",
    "\n",
    "@njit\n",
    "def generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                     input_features_normalized):\n",
    "    X = np.empty((batch_size, dim[1], dim[0]))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        for k in prange(len(input_features_normalized)):\n",
    "            for l in prange(dim[0]):\n",
    "                X[i][k][l] = input_features_normalized[k][ID + l]\n",
    "    # X[i, ] = self.input_features_normalized[ID+self.window_length, ]\n",
    "    return X\n",
    "\n",
    "@njit\n",
    "def generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data):\n",
    "    y = np.empty((batch_size, n_classes))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        y[i, :] = y_data[ID]\n",
    "    return y\n",
    "\n",
    "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
    "    def __init__(self,\n",
    "                 prices_for_window_index_array,\n",
    "                 input_features_normalized,\n",
    "                 y_data,\n",
    "                 batch_size,\n",
    "                 dim,\n",
    "                 n_classes,\n",
    "                 to_fit,\n",
    "                 shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.prices_for_window_index_array = prices_for_window_index_array\n",
    "        self.input_features_normalized = input_features_normalized\n",
    "        self.labels = y_data\n",
    "        self.y_data = y_data\n",
    "        self.to_fit = to_fit\n",
    "        self.n_classes = n_classes\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.n = 0\n",
    "        self.list_IDs = np.arange(len(self.prices_for_window_index_array))\n",
    "        self.on_epoch_end()\n",
    "    def __next__(self):\n",
    "        # Get one batch of data\n",
    "        data = self.__getitem__(self.n)\n",
    "        # Batch index\n",
    "        self.n += 1\n",
    "        # If we have processed the entire dataset then\n",
    "        if self.n >= self.__len__():\n",
    "            self.on_epoch_end\n",
    "            self.n = 0\n",
    "        return data\n",
    "    def __len__(self):\n",
    "        # Return the number of batches of the dataset\n",
    "        return math.ceil(len(self.indexes) / self.batch_size)\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                               self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X = self._generate_x(list_IDs_temp)\n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(list_IDs_temp)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.prices_for_window_index_array))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def _generate_x(self, list_IDs_temp):\n",
    "        dim = self.dim\n",
    "        batch_size = self.batch_size\n",
    "        input_features_normalized = self.input_features_normalized\n",
    "        X = generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                             input_features_normalized)\n",
    "        # X[i, ] = self.input_features_normalized[ID+self.window_length, ]\n",
    "        return X\n",
    "    def _generate_y(self, list_IDs_temp):\n",
    "        batch_size = self.batch_size\n",
    "        n_classes = self.n_classes\n",
    "        y_data = self.y_data\n",
    "        y = generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data)\n",
    "        return y\n",
    "\n",
    "n_classes = 3\n",
    "dim = (200, 40)\n",
    "batch_size = 64\n",
    "\n",
    "to_fit = True\n",
    "# for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,\n",
    "#                                       input_features_normalized_train,\n",
    "#                                       y_train,\n",
    "#                                       batch_size,\n",
    "#                                       dim,\n",
    "#                                       n_classes,\n",
    "#                                       to_fit,\n",
    "#                                       shuffle=True):\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "# get Bilinear model\n",
    "\n",
    "projection_regularizer = None\n",
    "projection_constraint = keras.constraints.max_norm(3.0, axis=0)\n",
    "attention_regularizer = None\n",
    "attention_constraint = keras.constraints.max_norm(5.0, axis=1)\n",
    "dropout = 0.1\n",
    "\n",
    "model = Models.TABL(\n",
    "    template,\n",
    "    dropout,\n",
    "    projection_regularizer,\n",
    "    projection_constraint,\n",
    "    attention_regularizer,\n",
    "    attention_constraint,\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "# path = path_adjust + \"data/lob_2010/train_and_test_tabl.h5\"\n",
    "# h5f = h5py.File(path, \"r\")\n",
    "# trainX_CNN = h5f[\"trainX_CNN\"][:]\n",
    "# trainY_CNN = h5f[\"trainY_CNN\"][:]\n",
    "# testX_CNN = h5f[\"testX_CNN\"][:]\n",
    "# testY_CNN = h5f[\"testY_CNN\"][:]\n",
    "# h5f.close()\n",
    "# # train one epoch to find the learning rate\n",
    "# model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     batch_size=256,\n",
    "#     epochs=1,\n",
    "#     shuffle=False,\n",
    "# )  # no class weight\n",
    "# Model configuration\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "loss_function = categorical_crossentropy\n",
    "# no_epochs = 5\n",
    "# start_lr = 0.0001\n",
    "# end_lr = 1\n",
    "# moving_average = 20\n",
    "# # Determine tests you want to perform\n",
    "# tests = [\n",
    "#     (\"sgd\", \"SGD optimizer\"),\n",
    "#     (\"adam\", \"Adam optimizer\"),\n",
    "#     (\"rmsprop\", \"RMS Prop optimizer\"),\n",
    "# ]\n",
    "# # Set containers for tests\n",
    "# test_learning_rates = []\n",
    "# test_losses = []\n",
    "# test_loss_changes = []\n",
    "# labels = []\n",
    "# # Perform each test\n",
    "# for test_optimizer, label in tests:\n",
    "#     # Compile the model\n",
    "#     model.compile(loss=loss_function,\n",
    "#                   optimizer=test_optimizer,\n",
    "#                   metrics=[\"accuracy\"])\n",
    "#     # Instantiate the Learning Rate Range Test / LR Finder\n",
    "#     lr_finder = LRFinder(model)\n",
    "#     # Perform the Learning Rate Range Test\n",
    "#     outputs = lr_finder.find(\n",
    "#         trainX_CNN,\n",
    "#         trainY_CNN,\n",
    "#         start_lr=start_lr,\n",
    "#         end_lr=end_lr,\n",
    "#         batch_size=batch_size,\n",
    "#         epochs=no_epochs,\n",
    "#     )\n",
    "#     # Get values\n",
    "#     learning_rates = lr_finder.lrs\n",
    "#     losses = lr_finder.losses\n",
    "#     loss_changes = []\n",
    "#     # Compute smoothed loss changes\n",
    "#     # Inspired by Keras LR Finder: https://github.com/surmenok/keras_lr_finder/blob/master/keras_lr_finder/lr_finder.py\n",
    "#     for i in range(moving_average, len(learning_rates)):\n",
    "#         loss_changes.append(\n",
    "#             (losses[i] - losses[i - moving_average]) / moving_average)\n",
    "#     # Append values to container\n",
    "#     test_learning_rates.append(learning_rates)\n",
    "#     test_losses.append(losses)\n",
    "#     test_loss_changes.append(loss_changes)\n",
    "#     labels.append(label)\n",
    "# # Generate plot for Loss Deltas\n",
    "# for i in range(0, len(test_learning_rates)):\n",
    "#     plt.plot(test_learning_rates[i][moving_average:],\n",
    "#              test_loss_changes[i],\n",
    "#              label=labels[i])\n",
    "# plt.xscale(\"log\")\n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.ylabel(\"loss delta\")\n",
    "# plt.xlabel(\"learning rate (log scale)\")\n",
    "# plt.title(\n",
    "#     \"Results for Learning Rate Range Test / Loss Deltas for Learning Rate\")\n",
    "# plt.show()\n",
    "# # Generate plot for Loss Values\n",
    "# for i in range(0, len(test_learning_rates)):\n",
    "#     plt.plot(test_learning_rates[i], test_losses[i], label=labels[i])\n",
    "# plt.xscale(\"log\")\n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.ylabel(\"loss\")\n",
    "# plt.xlabel(\"learning rate (log scale)\")\n",
    "# plt.title(\n",
    "#     \"Results for Learning Rate Range Test / Loss Values for Learning Rate\")\n",
    "# plt.show()\n",
    "# # Configuration settings for LR finder\n",
    "# start_lr = 1e-4\n",
    "# end_lr = 1e0\n",
    "# no_epochs = 5\n",
    "# ##\n",
    "# ## LR Finder specific code\n",
    "# ##\n",
    "# optimizer = keras.optimizers.RMSprop()\n",
    "# # Compile the model\n",
    "# model.compile(loss=loss_function, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# # Define LR finder callback\n",
    "# from third_party_libraries.LRFT.keras_callbacks import LRFinder\n",
    "# lr_finder = LRFinder(min_lr=start_lr, max_lr=end_lr)\n",
    "# # Perform LR finder\n",
    "# model.fit(\n",
    "#     trainX_CNN,\n",
    "#     trainY_CNN,\n",
    "#     batch_size=batch_size,\n",
    "#     callbacks=[lr_finder],\n",
    "#     epochs=no_epochs,\n",
    "# )\n",
    "# print(\"learning rate found running cyclical learning\")\n",
    "# ### cyclical learning rate\n",
    "# Set CLR options\n",
    "\n",
    "clr_step_size = int(4 *\n",
    "                    (len(prices_for_window_index_array_train) / batch_size))\n",
    "\n",
    "base_lr = 1e-4\n",
    "max_lr = 1e-2\n",
    "\n",
    "mode = \"triangular\"\n",
    "# Define the callback\n",
    "\n",
    "clr = CyclicLR(base_lr=base_lr,\n",
    "               max_lr=max_lr,\n",
    "               step_size=clr_step_size,\n",
    "               mode=mode)\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "# Compile the model\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "checkpoint_path = path_adjust + \"temp/cp.ckpt\"\n",
    "# Create a callback that saves the model's weights\n",
    "\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                              save_weights_only=True,\n",
    "                              verbose=1)\n",
    "\n",
    "check_point_file = Path(checkpoint_path)\n",
    "\n",
    "if check_point_file.exists() and resuming == \"resuming\":\n",
    "    print(\"weights loaded\")\n",
    "    model.load_weights(checkpoint_path)\n",
    "# Fit data to model\n",
    "# history = model.fit(trainX_CNN,\n",
    "#                     trainY_CNN,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=20,\n",
    "#                     callbacks=[clr, cp_callback])\n",
    "\n",
    "finished_weights_path = path_adjust + \"temp/cp_end.ckpt\"\n",
    "\n",
    "model.save_weights(finished_weights_path)\n",
    "# create class weight\n",
    "# class_weight = {0: 1e6 / 300.0, 1: 1e6 / 400.0, 2: 1e6 / 300.0}\n",
    "\n",
    "train_generator = DataGenerator(prices_for_window_index_array_train,\n",
    "                                input_features_normalized_train,\n",
    "                                y_train,\n",
    "                                batch_size,\n",
    "                                dim,\n",
    "                                n_classes,\n",
    "                                to_fit,\n",
    "                                shuffle=True)\n",
    "\n",
    "val_generator = DataGenerator(prices_for_window_index_array_val,\n",
    "                              input_features_normalized_val,\n",
    "                              y_val,\n",
    "                              batch_size,\n",
    "                              dim,\n",
    "                              n_classes,\n",
    "                              to_fit,\n",
    "                              shuffle=True)\n",
    "\n",
    "steps_per_epoch = len(train_generator)\n",
    "\n",
    "validation_steps = len(val_generator)\n",
    "# example sata training\n",
    "\n",
    "model.fit(train_generator,\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          validation_steps=validation_steps,\n",
    "          epochs=100,\n",
    "          validation_data=val_generator,\n",
    "          callbacks=[cp_callback, WandbCallback()])\n",
    "\n",
    "model.save(os.path.join(wandb.run.dir, \"model.h5\"))\n",
    "# no class weight\n",
    "## lob FI-2010 DATA PREPERATION\n",
    "# The first 40 columns of the FI-2010 dataset are 10 levels ask and bid information\n",
    "# for a limit order book and we only use these 40 features in our network.\n",
    "# The last 5 columns of the FI-2010 dataset are the labels with different prediction horizons.\n",
    "# def prepare_x(data):\n",
    "#     df1 = data[:40, :].T\n",
    "#     return np.array(df1)\n",
    "# def get_label(data):\n",
    "#     lob = data[-5:, :].T\n",
    "#     return lob\n",
    "# def data_classification(X, Y, T):\n",
    "#     [N, D] = X.shape\n",
    "#     df = np.array(X)\n",
    "#     dY = np.array(Y)\n",
    "#     dataY = dY[T - 1 : N]\n",
    "#     dataX = np.zeros((N - T + 1, T, D))\n",
    "#     for i in range(T, N + 1):\n",
    "#         dataX[i - T] = df[i - T : i, :]\n",
    "#     return dataX.reshape(dataX.shape + (1,)), dataY\n",
    "# # please change the data_path to your local path\n",
    "# data_path = (\n",
    "#     \"../third_party_libraries/gam_rhn/95-FI2010/data/BenchmarkDatasets/NoAuction\"\n",
    "# )\n",
    "# dec_train = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Training/Train_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "# )\n",
    "# dec_test1 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "# )\n",
    "# dec_test2 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_8.txt\"\n",
    "# )\n",
    "# dec_test3 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_9.txt\"\n",
    "# )\n",
    "# dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "# # extract limit order book data from the FI-2010 dataset\n",
    "# train_lob = prepare_x(dec_train)\n",
    "# test_lob = prepare_x(dec_test)\n",
    "# # extract label from the FI-2010 dataset\n",
    "# train_label = get_label(dec_train)\n",
    "# test_label = get_label(dec_test)\n",
    "# # prepare training data. We feed past 100 observations into our algorithms and choose the prediction horizon.\n",
    "# trainX_CNN, trainY_CNN = data_classification(train_lob, train_label, T=10)\n",
    "# trainY_CNN = trainY_CNN[:, 3] - 1\n",
    "# trainY_CNN = np_utils.to_categorical(trainY_CNN, 3)\n",
    "# # trainX_CNN.shape (254651, 100, 40, 1)\n",
    "# # prepare test data.\n",
    "# testX_CNN, testY_CNN = data_classification(test_lob, test_label, T=10)\n",
    "# testY_CNN = testY_CNN[:, 3] - 1\n",
    "# testY_CNN = np_utils.to_categorical(testY_CNN, 3)\n",
    "# trainX_CNN = np.swapaxes(trainX_CNN, 1, 2)\n",
    "# trainX_CNN = trainX_CNN.reshape(\n",
    "#     trainX_CNN.shape[0], trainX_CNN.shape[1], trainX_CNN.shape[2]\n",
    "# )\n",
    "# testX_CNN = testX_CNN = np.swapaxes(testX_CNN, 1, 2)\n",
    "# testX_CNN = testX_CNN.reshape(\n",
    "#     testX_CNN.shape[0], testX_CNN.shape[1], testX_CNN.shape[2]\n",
    "# )\n",
    "# path = path_adjust + \"data/lob_2010/train_and_test_tabl.h5\"\n",
    "# h5f = h5py.File(path, \"w\")\n",
    "# h5f.create_dataset(\"trainX_CNN\", data=trainX_CNN)\n",
    "# h5f.create_dataset(\"trainY_CNN\", data=trainY_CNN)\n",
    "# h5f.create_dataset(\"testX_CNN\", data=testX_CNN)\n",
    "# h5f.create_dataset(\"testY_CNN\", data=testY_CNN)\n",
    "# h5f.close()\n",
    "# fi-02010 data\n",
    "# balancing seems to help abit (0.64 accuracy after 950 epochs), next try norm 0,1 rather than -1,1\n",
    "# after applying normalization not sure if ill need this\n",
    "# two_d_X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "# two_d_X_val = X_val.reshape(X_val.shape[0], X_val.shape[1] * X_val.shape[2])\n",
    "# oversample = SMOTE()\n",
    "# X_train, y_train = oversample.fit_resample(two_d_X_train, y_train)\n",
    "# X_val, y_val = oversample.fit_resample(two_d_X_val, y_val)\n",
    "# X_train = X_train.reshape((X_train.shape[0], 40, 10))\n",
    "# X_val = X_val.reshape((X_val.shape[0], 40, 10))\n",
    "# train one epoch to find the learning rate\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=256,\n",
    "    epochs=1,\n",
    "    shuffle=False,\n",
    ")  # no class weight\n",
    "\n",
    "score = model.evaluate(x=X_test, y=y_test, batch_size=256)\n",
    "# Save model to wandb\n",
    "# print(score)\n",
    "# score on deeplob fi-2010 loss: 0.7469 - acc: 0.6760 - val_loss: 0.6772 - val_acc: 0.7248"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

diff --git a/pipeline/preprocessing.py b/pipeline/preprocessing.py
index 714d325..adcd242 100644
--- a/pipeline/preprocessing.py
+++ b/pipeline/preprocessing.py
@@ -81,6 +81,10 @@ except Exception as e:
                         "--is_finished",
                         type=str,
                         help="Is this a continuation of preempted instance?")
+    parser.add_argument("-r",
+                        "--resuming",
+                        type=str,
+                        help="Is this a continuation of preempted instance?")
     args = parser.parse_args()
     if args.stage != None:
         arg_parse_stage = 1
@@ -146,7 +150,7 @@ with open(yaml_path) as file:
 config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
 
 wandb.init(
-    dir="~/ProdigyAI/",
+    dir="",
     project="prodigyai",
     config=config_dictionary,
 )
@@ -155,10 +159,6 @@ minimum_return = eval(wandb.config['params']['minimum_return']['value'])
 vertical_barrier_seconds = eval(
     wandb.config['params']['vertical_barrier_seconds']['value'])
 
-# Parameters
-parameters = dict()
-wandb.config['params']['head'][
-    'value'] = 1000  # take only first x number of rows 0 means of
 volume_max = (
     minimum_return + wandb.config['params']['vol_max_modifier']['value']
 )  # The higher this is the more an increase in volatility requries an increase
@@ -199,8 +199,9 @@ pt_sl = [
 ]
 cpus = cpu_count() - 1
 
-regenerate_features_and_labels = True
-if regenerate_features_and_labels == True:
+generate_features_and_labels = wandb.config['params'][
+    'generate_features_and_labels']['value']
+if generate_features_and_labels == True:
     # READ THE DATA
     if stage == 1:
         # Side
@@ -390,8 +391,8 @@ if regenerate_features_and_labels == True:
         num_threads=cpus * 2,
         vertical_barrier_times=vertical_barrier_timestamps,
         side_prediction=side,
-        split_by=
-        100  # maybe we want this as large as we can while still fitting in ram
+        split_by=wandb.config['params']['split_by']
+        ['value']  # maybe we want this as large as we can while still fitting in ram
     )
 
     end = time.time()
@@ -411,7 +412,7 @@ if regenerate_features_and_labels == True:
     print("returning bins finished taking" + str(end_time - start_time))
     # unique, counts = np.unique(y, return_counts=True)
 
-    sampled_idx_epoch = sampled_idx.astype(np.int64)
+    sampled_idx_epoch = sampled_idx.astype(np.int64) // 1000000
     h5f = h5py.File(
         path_adjust + "data/inputs_and_barrier_labels/sampled_idx_epoch.h5",
         "w")
@@ -481,374 +482,449 @@ else:
     h5f.close()
     sampled_idx = pd.DatetimeIndex(sampled_idx_epoch)
 
-y_dataframe = labels["bin"]
-data["bins"] = labels["bin"]
-y = np.asarray(y_dataframe)
-end = time.time()
+apply_train_test_split_and_normalize = wandb.config['params'][
+    'apply_train_test_split_and_normalize']['value']
+if apply_train_test_split_and_normalize == True:
+    if stage == 1:
+        # Get why from labels
+        y_dataframe = labels["bin"]
+        data["bins"] = labels["bin"]
+        y = np.asarray(y_dataframe)
 
-y = keras.utils.to_categorical(y, num_classes=3)
+        start_time = time.time()
+        # side
+        X_for_all_labels = data.loc[labels.index, :]
 
-if stage == 1:
+        end_time = time.time()
+        print(end_time - start_time)
 
-    start_time = time.time()
-    # side
-    X_for_all_labels = data.loc[labels.index, :]
+        ### FOR HIGHWAY RNN
+        X = np.asarray(volumes.loc[labels.index, :])
 
-    end_time = time.time()
-    print(end_time - start_time)
+        h5f = h5py.File(
+            path_adjust + "data/preprocessed/" + parameter_string +
+            "_gam_rhn.h5", "w")
+        h5f.create_dataset("X", data=X)
+        h5f.create_dataset("y", data=y)
+        h5f.close()
 
-    # ### FOR HIGHWAY RNN
-    # X = np.asarray(volumes.loc[labels.index, :])
+        X = []
 
-    # h5f = h5py.File(path_adjust + "data/preprocessed/" + parameter_string + ".h5", "w")
-    # h5f.create_dataset("X", data=X)
-    # h5f.create_dataset("y", data=y)
-    # h5f.close()
+        ### One hot encode y
+        y = keras.utils.to_categorical(y, num_classes=3)
 
-    # X = []
+        start_time = time.time()
 
-    start_time = time.time()
+        prices_for_window = data.loc[X_for_all_labels.index]
+        prices_for_window_index = prices_for_window.index.astype(
+            np.int64) // 1000000
+        prices_for_window_index_array = np.asarray(prices_for_window_index)
 
-    prices_for_window = data.loc[X_for_all_labels.index]
-    prices_for_window_index = prices_for_window.index.astype(np.int64)
-    prices_for_window_index_array = np.asarray(prices_for_window_index)
+        end_time = time.time()
+        print(end_time - start_time)
 
-    end_time = time.time()
-    print(end_time - start_time)
+        start_time = time.time()
 
-    start_time = time.time()
+        close_index = data.close.index.astype(np.int64) // 1000000
+        close_index_array = np.asarray(close_index)
 
-    close_index = data.close.index.astype(np.int64)
-    close_index_array = np.asarray(close_index)
+        end_time = time.time()
+        print(end_time - start_time)
 
-    end_time = time.time()
-    print(end_time - start_time)
+        start_time = time.time()
 
-    start_time = time.time()
+        # Make a new column time since last bar
+        unindexed_data = data.reset_index()
+        unindexed_data["shifted_date_time"] = unindexed_data[
+            "date_time"].shift(1)
+        unindexed_data["time_since_last_bar"] = (
+            unindexed_data["date_time"].sub(
+                unindexed_data["shifted_date_time"], axis=0).dt.seconds)
+        unindexed_data = unindexed_data.set_index("date_time")
+        data["time_since_last_bar"] = unindexed_data["time_since_last_bar"]
+        data["time_since_last_bar"].iloc[0] = 0
 
-    # Make a new column time since last bar
-    unindexed_data = data.reset_index()
-    unindexed_data["shifted_date_time"] = unindexed_data["date_time"].shift(1)
-    unindexed_data["time_since_last_bar"] = (unindexed_data["date_time"].sub(
-        unindexed_data["shifted_date_time"], axis=0).dt.seconds)
-    unindexed_data = unindexed_data.set_index("date_time")
-    data["time_since_last_bar"] = unindexed_data["time_since_last_bar"]
-    data["time_since_last_bar"].iloc[0] = 0
+        end_time = time.time()
+        print(end_time - start_time)
 
-    end_time = time.time()
-    print(end_time - start_time)
+        start_time = time.time()
 
-    start_time = time.time()
+        ### ORDERBOOK VOLUME DATA
+        volumes_for_all_labels = volumes.loc[data.close.index]
 
-    ### ORDERBOOK VOLUME DATA
-    volumes_for_all_labels = volumes.loc[data.close.index]
+        # ## TRADE DATA
+        # input_features_trade = []
+        # close_array = data.close.values
+        # input_features_trade.append(close_array)
 
-    # ## TRADE DATA
-    # input_features_trade = []
-    # close_array = data.close.values
-    # input_features_trade.append(close_array)
+        # if parameters["ntb"] == False and parameters["vbc"] == True:
+        #     volume_array = data.volume.values
+        #     input_features_trade.append(volume_array)
+        # if parameters["ntb"] == True and parameters["tslbc"] == True:
+        #     time_since_last_bar_array = data.time_since_last_bar.values
+        #     input_features_trade.append(time_since_last_bar_array)
 
-    # if parameters["ntb"] == False and parameters["vbc"] == True:
-    #     volume_array = data.volume.values
-    #     input_features_trade.append(volume_array)
-    # if parameters["ntb"] == True and parameters["tslbc"] == True:
-    #     time_since_last_bar_array = data.time_since_last_bar.values
-    #     input_features_trade.append(time_since_last_bar_array)
+        end_time = time.time()
+        print(end_time - start_time)
 
-    end_time = time.time()
-    print(end_time - start_time)
+        # Type of scaling to apply
+        scaling_type = wandb.config['params']['scaling_type']['value']
 
-    # Type of scaling to apply
-    scaling_type = wandb.config['params']['scaling_type']['value']
+        # min max limits
+        minimum = wandb.config['params']['scaling_maximum']['value']
+        maximum = wandb.config['params']['scaling_minimum']['value']
 
-    # min max limits
-    minimum = wandb.config['params']['scaling_maximum']['value']
-    maximum = wandb.config['params']['scaling_minimum']['value']
+        ### Split intothe training/validation/test sets
+        print("splitting into train/va/test sets start")
+        start_time = time.time()
 
-    ### Split intothe training/validation/test sets
-    print("splitting into train/va/test sets start")
-    start_time = time.time()
+        prices_for_window_index_array_train_and_val = prices_for_window_index_array[:round(
+            len(prices_for_window_index_array) * 0.8)]
 
-    prices_for_window_index_array_train_and_val = prices_for_window_index_array[:round(
-        len(prices_for_window_index_array) * 0.8)]
+        y_train_and_val = y[:round(len(y) * 0.8)]
 
-    y_train_and_val = y[:round(len(y) * 0.8)]
+        prices_for_window_index_array_train = prices_for_window_index_array_train_and_val[:round(
+            len(prices_for_window_index_array_train_and_val) * 0.8)]
 
-    prices_for_window_index_array_train = prices_for_window_index_array_train_and_val[:round(
-        len(prices_for_window_index_array_train_and_val) * 0.8)]
+        y_train = y_train_and_val[:round(len(y_train_and_val) * 0.8)]
 
-    y_train = y_train_and_val[:round(len(y_train_and_val) * 0.8)]
+        train_close_array_integer_index = np.nonzero(
+            np.in1d(close_index_array, prices_for_window_index_array_train))[0]
 
-    train_close_array_integer_index = np.nonzero(
-        np.in1d(close_index_array, prices_for_window_index_array_train))[0]
+        volumes_for_all_labels_train = volumes_for_all_labels.iloc[
+            train_close_array_integer_index[0] -
+            wandb.config['params']['window_length']['value']:
+            train_close_array_integer_index[-1] + 2]
 
-    volumes_for_all_labels_train = volumes_for_all_labels.iloc[
-        train_close_array_integer_index[0] -
-        wandb.config['params']['window_length']['value']:
-        train_close_array_integer_index[-1] + 2]
+        close_index_array_train = close_index_array[
+            train_close_array_integer_index[0] -
+            wandb.config['params']['window_length']['value']:
+            train_close_array_integer_index[-1] + 2]
 
-    close_index_array_train = close_index_array[
-        train_close_array_integer_index[0] -
-        wandb.config['params']['window_length']['value']:
-        train_close_array_integer_index[-1] + 2]
+        end_time = time.time()
 
-    end_time = time.time()
+        print("splitting into train/va/test sets finished" +
+              str(end_time - start_time))
 
-    print("splitting into train/va/test sets finished" +
-          str(end_time - start_time))
+        print("Make input features from orderbook data started")
+        start_time = time.time()
 
-    print("Make input features from orderbook data started")
-    start_time = time.time()
+        # Make input features from orderbook data
+        input_features_train = make_input_features_from_orderbook_data(
+            volumes_for_all_labels_train)
+        feature_span_in_ms = close_index_array_train[
+            -1] - close_index_array_train[0]
 
-    # MAKE WINDOW FROM INPUTS
-    input_features_train = make_input_features_from_orderbook_data(
-        volumes_for_all_labels_train)
+        psuedo_day_length_in_seconds = 5
 
-    end_time = time.time()
-    print("Make input features from orderbook data started " +
-          str(end_time - start_time))
+        number_of_splits = (
+            (feature_span_in_ms / 1000) // psuedo_day_length_in_seconds) + 1
 
-    print("Get train scalers started")
-    start_time = time.time()
-    ### 2: Apply the normalisation based on the training fit scalars (-1,1 vs 0,1 min max test both)
-    maxes_or_means_np_array_train, mins_or_stds_np_array_train = get_fit_scalars(
-        scaling_type, input_features_train)
+        start_and_finish_indexes = np.zeros((int(number_of_splits), 2))
+        start_and_finish_indexes[:] = np.nan
 
-    end_time = time.time()
-    print("Get train scalers finished" + str(end_time - start_time))
+        # first split normalize by itself
+        # second split by one before
+        # third split by the ones before until it hits 5 then keep it as the 5 before
 
-    print("Norm train data started")
-    start_time = time.time()
+        start_index = None
+        split_number = 0
+        for i in range(len(close_index_array_train)):
+            if start_index is None:
+                start_index = close_index_array_train[i]
+                try:
+                    start_and_finish_indexes[split_number, 0] = i
+                except:
+                    import pdb
+                    pdb.set_trace()
 
-    # Norm train
-    input_features_normalized_train = scale_input_features(
-        scaling_type,
-        maxes_or_means_np_array_train,
-        mins_or_stds_np_array_train,
-        input_features_train,
-        minimum,
-        maximum,
-    )
+            else:
+                if close_index_array_train[
+                        i] - start_index > 1000 * psuedo_day_length_in_seconds or close_index_array_train[
+                            i] == close_index_array_train[-1]:
+                    end_index = close_index_array_train[i - 1]
+                    start_and_finish_indexes[split_number, 1] = i
+                    split_number = split_number + 1
+                    start_index = None
+                    end_index = None
+
+        num_days_to_split_by = 5
+
+        input_features_train_normalized = np.zeros(input_features_train.shape)
+        input_features_train_normalized[:] = np.nan
+
+        for i in range(len(start_and_finish_indexes)):
+            if i == 0:
+                start_fitting_index = i
+                end_fitting_index = i
+
+            elif i > 0:
+                start_fitting_index = np.max([0, i - num_days_to_split_by])
+                end_fitting_index = i - 1
+                print(i)
+                print(start_fitting_index)
+                print(end_fitting_index)
+
+            maxes_or_means_np_array_train, mins_or_stds_np_array_train = get_fit_scalars(
+                scaling_type,
+                input_features_train[int(start_and_finish_indexes[
+                    start_fitting_index,
+                    0]):int(start_and_finish_indexes[end_fitting_index, 1])])
+
+            input_features_train_normalized[int(start_and_finish_indexes[
+                i, 0]):int(start_and_finish_indexes[
+                    i, 1])] = scale_input_features(
+                        scaling_type, maxes_or_means_np_array_train,
+                        mins_or_stds_np_array_train,
+                        input_features_train[int(start_and_finish_indexes[
+                            i, 0]):int(start_and_finish_indexes[i, 1])],
+                        minimum, maximum)
+
+        end_time = time.time()
+        print("Make input features from orderbook data started " +
+              str(end_time - start_time))
+
+        print("Get train scalers started")
+        start_time = time.time()
+        ### 2: Apply the normalisation based on the training fit scalars (-1,1 vs 0,1 min max test both)
+        maxes_or_means_np_array_train, mins_or_stds_np_array_train = get_fit_scalars(
+            scaling_type, input_features_train)
+
+        end_time = time.time()
+        print("Get train scalers finished" + str(end_time - start_time))
+
+        print("Norm train data started")
+        start_time = time.time()
+
+        # Norm train
+        input_features_normalized_train = scale_input_features(
+            scaling_type,
+            maxes_or_means_np_array_train,
+            mins_or_stds_np_array_train,
+            input_features_train,
+            minimum,
+            maximum,
+        )
 
-    end_time = time.time()
-    print("Get train scalers finished" + str(end_time - start_time))
-
-    # print("Make window started")
-    # start_time = time.time()
-
-    # padding = wandb.config['params']['window_length']['value'] * 2
-
-    # split_by = 100000
-
-    # number_of_splits = len(prices_for_window_index_array_train) // split_by
-    # for i in range(number_of_splits):
-    #     print(i)
-    #     start_time = time.time()
-    #     if i == 0:
-    #         start_index = None  # 0
-    #         end_index = (i + 1) * split_by
-    #         close_and_input_start_index = start_index
-    #         close_and_input_end_index = end_index + (padding)
-    #     elif i < number_of_splits - 1:
-    #         start_index = i * split_by
-    #         end_index = (i + 1) * split_by
-    #         close_and_input_start_index = start_index - (padding)
-    #         close_and_input_end_index = end_index + (padding)
-    #     elif i == number_of_splits - 1:
-    #         start_index = i * split_by
-    #         end_index = None  # -1
-    #         close_and_input_start_index = start_index - (padding)
-    #         close_and_input_end_index = end_index
-
-    #     # Window train
-    #     X_train_section = make_window_multivariate_numba(
-    #         len(prices_for_window_index_array_train[start_index:end_index]),
-    #         input_features_normalized_train[:, close_and_input_start_index:
-    #                                         close_and_input_end_index],
-    #         wandb.config['params']['window_length']['value'],
-    #         model_arch,
-    #     )
-
-    #     print(X_train_section.shape)
-
-    #     hdf5_epath = path_adjust + "data/preprocessed/X_and_y.h5"
-    #     if os.path.exists(hdf5_epath) == False or i == 0:
-    #         h5f = tb.open_file(hdf5_epath, mode="a")
-    #         dataGroup = h5f.create_group(h5f.root, "MyData")
-    #         h5f.create_earray(dataGroup,
-    #                           "X_train_section",
-    #                           obj=X_train_section)
-    #         h5f.close()
-
-    #     else:
-    #         h5f = tb.open_file(hdf5_epath, mode="r+")
-    #         h5f.root.MyData.X_train_section.append(X_train_section)
-    #         h5f.close()
-
-    #     # h5f = h5py.File(path_adjust + "data/preprocessed/X_and_y.h5", "w")
-    #     # h5f.create_dataset("X_train_section", data=X_train_section)
-    #     # h5f.close()
-
-    #     end_time = time.time()
-
-    #     print(str(i) + " finished taking " + str(end_time - start_time))
-
-    # end_time = time.time()
-    # print("Make window finished" + str(end_time - start_time))
-
-    #     # import pdb
-    #     # pdb.set_trace()
-
-    # Val
-    prices_for_window_index_array_val = prices_for_window_index_array_train_and_val[
-        round(len(prices_for_window_index_array_train_and_val) * 0.8):]
-
-    y_val = y_train_and_val[round(len(y_train_and_val) * 0.8):]
-
-    val_close_array_integer_index = np.nonzero(
-        np.in1d(close_index_array, prices_for_window_index_array_val))[0]
-
-    volumes_for_all_labels_val = volumes_for_all_labels.iloc[
-        val_close_array_integer_index[0] -
-        wandb.config['params']['window_length']['value']:
-        val_close_array_integer_index[-1] + 2]
-
-    close_index_array_val = close_index_array[
-        val_close_array_integer_index[0] -
-        wandb.config['params']['window_length']['value']:
-        val_close_array_integer_index[-1] + 2]
-
-    input_features_val = make_input_features_from_orderbook_data(
-        volumes_for_all_labels_val)
-
-    # Norm val
-    input_features_normalized_val = scale_input_features(
-        scaling_type,
-        maxes_or_means_np_array_train,
-        mins_or_stds_np_array_train,
-        input_features_val,
-        minimum,
-        maximum,
-    )
+        end_time = time.time()
+        print("Get train scalers finished" + str(end_time - start_time))
+
+        # print("Make window started")
+        # start_time = time.time()
+
+        # padding = wandb.config['params']['window_length']['value'] * 2
+
+        # split_by = 100000
+
+        # number_of_splits = len(prices_for_window_index_array_train) // split_by
+        # for i in range(number_of_splits):
+        #     print(i)
+        #     start_time = time.time()
+        #     if i == 0:
+        #         start_index = None  # 0
+        #         end_index = (i + 1) * split_by
+        #         close_and_input_start_index = start_index
+        #         close_and_input_end_index = end_index + (padding)
+        #     elif i < number_of_splits - 1:
+        #         start_index = i * split_by
+        #         end_index = (i + 1) * split_by
+        #         close_and_input_start_index = start_index - (padding)
+        #         close_and_input_end_index = end_index + (padding)
+        #     elif i == number_of_splits - 1:
+        #         start_index = i * split_by
+        #         end_index = None  # -1
+        #         close_and_input_start_index = start_index - (padding)
+        #         close_and_input_end_index = end_index
+
+        #     # Window train
+        #     X_train_section = make_window_multivariate_numba(
+        #         len(prices_for_window_index_array_train[start_index:end_index]),
+        #         input_features_normalized_train[:, close_and_input_start_index:
+        #                                         close_and_input_end_index],
+        #         wandb.config['params']['window_length']['value'],
+        #         model_arch,
+        #     )
+
+        #     print(X_train_section.shape)
+
+        #     hdf5_epath = path_adjust + "data/preprocessed/X_and_y.h5"
+        #     if os.path.exists(hdf5_epath) == False or i == 0:
+        #         h5f = tb.open_file(hdf5_epath, mode="a")
+        #         dataGroup = h5f.create_group(h5f.root, "MyData")
+        #         h5f.create_earray(dataGroup,
+        #                           "X_train_section",
+        #                           obj=X_train_section)
+        #         h5f.close()
+
+        #     else:
+        #         h5f = tb.open_file(hdf5_epath, mode="r+")
+        #         h5f.root.MyData.X_train_section.append(X_train_section)
+        #         h5f.close()
+
+        #     # h5f = h5py.File(path_adjust + "data/preprocessed/X_and_y.h5", "w")
+        #     # h5f.create_dataset("X_train_section", data=X_train_section)
+        #     # h5f.close()
+
+        #     end_time = time.time()
+
+        #     print(str(i) + " finished taking " + str(end_time - start_time))
+
+        # end_time = time.time()
+        # print("Make window finished" + str(end_time - start_time))
+
+        #     # import pdb
+        #     # pdb.set_trace()
+
+        # Val
+        prices_for_window_index_array_val = prices_for_window_index_array_train_and_val[
+            round(len(prices_for_window_index_array_train_and_val) * 0.8):]
+
+        y_val = y_train_and_val[round(len(y_train_and_val) * 0.8):]
+
+        val_close_array_integer_index = np.nonzero(
+            np.in1d(close_index_array, prices_for_window_index_array_val))[0]
+
+        volumes_for_all_labels_val = volumes_for_all_labels.iloc[
+            val_close_array_integer_index[0] -
+            wandb.config['params']['window_length']['value']:
+            val_close_array_integer_index[-1] + 2]
+
+        close_index_array_val = close_index_array[
+            val_close_array_integer_index[0] -
+            wandb.config['params']['window_length']['value']:
+            val_close_array_integer_index[-1] + 2]
+
+        input_features_val = make_input_features_from_orderbook_data(
+            volumes_for_all_labels_val)
+
+        # Norm val
+        input_features_normalized_val = scale_input_features(
+            scaling_type,
+            maxes_or_means_np_array_train,
+            mins_or_stds_np_array_train,
+            input_features_val,
+            minimum,
+            maximum,
+        )
 
-    #     # # Make window val
-    #     # X_val = make_window_multivariate_numba(
-    #     #     prices_for_window_index_array_val,
-    #     #     close_index_array_val,
-    #     #     input_features_normalized_val,
+        #     # # Make window val
+        #     # X_val = make_window_multivariate_numba(
+        #     #     prices_for_window_index_array_val,
+        #     #     close_index_array_val,
+        #     #     input_features_normalized_val,
+        #     #     wandb.config['params']['window_length']['value'],
+        #     #     model_arch,
+        #     # )
+
+        # Test
+        prices_for_window_index_array_test = prices_for_window_index_array[
+            round(len(prices_for_window_index_array) * 0.8):]
+
+        y_test = y[round(len(y) * 0.8):]
+
+        test_close_array_integer_index = np.nonzero(
+            np.in1d(close_index_array, prices_for_window_index_array_test))[0]
+
+        volumes_for_all_labels_test = volumes_for_all_labels.iloc[
+            test_close_array_integer_index[0] -
+            wandb.config['params']['window_length']['value']:
+            test_close_array_integer_index[-1] + 2]
+
+        close_index_array_test = close_index_array[
+            test_close_array_integer_index[0] -
+            wandb.config['params']['window_length']['value']:
+            test_close_array_integer_index[-1] + 2]
+
+        input_features_test = make_input_features_from_orderbook_data(
+            volumes_for_all_labels_test)
+
+        # Norm test
+        input_features_normalized_test = scale_input_features(
+            scaling_type,
+            maxes_or_means_np_array_train,
+            mins_or_stds_np_array_train,
+            input_features_test,
+            minimum,
+            maximum,
+        )
+
+    #     # # Window test
+    #     # # TABL
+    #     # X_test = make_window_multivariate_numba(
+    #     #     prices_for_window_index_array_test,
+    #     #     close_index_array_test,
+    #     #     input_features_normalized_test,
     #     #     wandb.config['params']['window_length']['value'],
     #     #     model_arch,
     #     # )
 
-    # Test
-    prices_for_window_index_array_test = prices_for_window_index_array[
-        round(len(prices_for_window_index_array) * 0.8):]
-
-    y_test = y[round(len(y) * 0.8):]
+    #     start = time.time()
 
-    test_close_array_integer_index = np.nonzero(
-        np.in1d(close_index_array, prices_for_window_index_array_test))[0]
+    #     end = time.time()
+    #     print("numba make window time" + str(end - start))
+    #     start = time.time()
 
-    volumes_for_all_labels_test = volumes_for_all_labels.iloc[
-        test_close_array_integer_index[0] -
-        wandb.config['params']['window_length']['value']:
-        test_close_array_integer_index[-1] + 2]
+    print("plotting started")
 
-    close_index_array_test = close_index_array[
-        test_close_array_integer_index[0] -
-        wandb.config['params']['window_length']['value']:
-        test_close_array_integer_index[-1] + 2]
+    # plot the whole sequence
+    ax = plt.gca()
+    data.plot(y="close", use_index=True)
 
-    input_features_test = make_input_features_from_orderbook_data(
-        volumes_for_all_labels_test)
-
-    # Norm test
-    input_features_normalized_test = scale_input_features(
-        scaling_type,
-        maxes_or_means_np_array_train,
-        mins_or_stds_np_array_train,
-        input_features_test,
-        minimum,
-        maximum,
-    )
+    window_index = 500
+    ax = plt.gca()
 
-#     # # Window test
-#     # # TABL
-#     # X_test = make_window_multivariate_numba(
-#     #     prices_for_window_index_array_test,
-#     #     close_index_array_test,
-#     #     input_features_normalized_test,
-#     #     wandb.config['params']['window_length']['value'],
-#     #     model_arch,
-#     # )
+    data.iloc[window_index - 200:window_index + 200].plot(y="close",
+                                                          use_index=True)
+    plot_window_and_touch_and_label(
+        window_index, wandb.config['params']['window_length']['value'], data,
+        triple_barrier_events, labels)
 
-#     start = time.time()
+    data.iloc[window_index - 10:window_index + 30]
 
-#     end = time.time()
-#     print("numba make window time" + str(end - start))
-#     start = time.time()
+    print("plotting finished")
 
-print("plotting started")
+    # Sample Weights
+    # if stage == 1:
+    #     weights = attribution.get_weights_by_return(triple_barrier_events.dropna(),
+    #                                                 data.close,
+    #                                                 num_threads=5)
+    #     sample_weights = np.asarray(weights)
+    #     sample_weights = sample_weights.reshape(len(sample_weights))
+    #     sampled_idx_epoch = sampled_idx.astype(np.int64) // 1000000 #
 
-# plot the whole sequence
-ax = plt.gca()
-data.plot(y="close", use_index=True)
-
-window_index = 500
-ax = plt.gca()
-
-data.iloc[window_index - 200:window_index + 200].plot(y="close",
-                                                      use_index=True)
-plot_window_and_touch_and_label(
-    window_index, wandb.config['params']['window_length']['value'], data,
-    triple_barrier_events, labels)
+    if stage == 2:
+        # size
+        parameter_string = parameter_string + "second_stage"
 
-data.iloc[window_index - 10:window_index + 30]
+    print("writing train/val/test to .h5 files starting")
+    start = time.time()
 
-print("plotting finished")
+    # Writing preprocessed X,y
+    h5f = h5py.File(
+        path_adjust + "data/preprocessed/" + parameter_string + ".h5", "w")
+    h5f.create_dataset("prices_for_window_index_array_train",
+                       data=prices_for_window_index_array_train)
+    h5f.create_dataset("prices_for_window_index_array_val",
+                       data=prices_for_window_index_array_val)
+    h5f.create_dataset("prices_for_window_index_array_test",
+                       data=prices_for_window_index_array_test)
+    h5f.create_dataset("close_index_array_train", data=close_index_array_train)
+    h5f.create_dataset("close_index_array_val", data=close_index_array_val)
+    h5f.create_dataset("close_index_array_test", data=close_index_array_test)
+    h5f.create_dataset("input_features_normalized_train",
+                       data=input_features_normalized_train)
+    h5f.create_dataset("input_features_normalized_val",
+                       data=input_features_normalized_val)
+    h5f.create_dataset("input_features_normalized_test",
+                       data=input_features_normalized_test)
+    h5f.create_dataset("y_train", data=y_train)
+    h5f.create_dataset("y_val", data=y_val)
+    h5f.create_dataset("y_test", data=y_test)
 
-# Sample Weights
-# if stage == 1:
-#     weights = attribution.get_weights_by_return(triple_barrier_events.dropna(),
-#                                                 data.close,
-#                                                 num_threads=5)
-#     sample_weights = np.asarray(weights)
-#     sample_weights = sample_weights.reshape(len(sample_weights))
-#     sampled_idx_epoch = sampled_idx.astype(np.int64)  #
+    end_time = time.time()
 
-if stage == 2:
-    # size
-    parameter_string = parameter_string + "second_stage"
-
-print("writing train/val/test to .h5 files starting")
-start = time.time()
-
-# Writing preprocessed X,y
-h5f = h5py.File(path_adjust + "data/preprocessed/" + parameter_string + ".h5",
-                "w")
-h5f.create_dataset("prices_for_window_index_array_train",
-                   data=prices_for_window_index_array_train)
-h5f.create_dataset("prices_for_window_index_array_val",
-                   data=prices_for_window_index_array_val)
-h5f.create_dataset("prices_for_window_index_array_test",
-                   data=prices_for_window_index_array_test)
-h5f.create_dataset("close_index_array_train", data=close_index_array_train)
-h5f.create_dataset("close_index_array_val", data=close_index_array_val)
-h5f.create_dataset("close_index_array_test", data=close_index_array_test)
-h5f.create_dataset("input_features_normalized_train",
-                   data=input_features_normalized_train)
-h5f.create_dataset("input_features_normalized_val",
-                   data=input_features_normalized_val)
-h5f.create_dataset("input_features_normalized_test",
-                   data=input_features_normalized_test)
-h5f.create_dataset("y_train", data=y_train)
-h5f.create_dataset("y_val", data=y_val)
-h5f.create_dataset("y_test", data=y_test)
-
-end_time = time.time()
-
-print("writing train/val/test to .h5 files finished taking " +
-      str(end_time - start_time))
+    print("writing train/val/test to .h5 files finished taking " +
+          str(end_time - start_time))
 
 # if stage == 2:
 #     # size
diff --git a/pipeline/tabl.py b/pipeline/tabl.py
index b06858f..98015e4 100644
--- a/pipeline/tabl.py
+++ b/pipeline/tabl.py
@@ -124,7 +124,7 @@ import h5py
 
 home = str(Path.home())
 
-file_name = "esugj36b.h5"
+file_name = "1fdpldp4.h5"
 wandb.config.update({'dataset': file_name})
 path = home + "/ProdigyAI/data/preprocessed/" + file_name
 h5f = h5py.File(path, "r")
Submodule third_party_libraries/TABL contains modified content
diff --git a/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc
deleted file mode 100644
index ff0a753..0000000
Binary files a/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc and /dev/null differ
diff --git a/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc
deleted file mode 100644
index a57f1ba..0000000
Binary files a/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc and /dev/null differ
Submodule third_party_libraries/gam_rhn contains untracked content
Submodule third_party_libraries/gam_rhn contains modified content
diff --git a/third_party_libraries/gam_rhn/05-TO/t5_fsrnn.py b/third_party_libraries/gam_rhn/05-TO/t5_fsrnn.py
index 1306cb9..18086c5 100644
--- a/third_party_libraries/gam_rhn/05-TO/t5_fsrnn.py
+++ b/third_party_libraries/gam_rhn/05-TO/t5_fsrnn.py
@@ -1,4 +1,4 @@
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 import to_core as core
 import to_mu as m
 from tframe import console
diff --git a/third_party_libraries/gam_rhn/05-TO/t5_gam_rhn.py b/third_party_libraries/gam_rhn/05-TO/t5_gam_rhn.py
index f4d602b..83ae482 100644
--- a/third_party_libraries/gam_rhn/05-TO/t5_gam_rhn.py
+++ b/third_party_libraries/gam_rhn/05-TO/t5_gam_rhn.py
@@ -1,4 +1,4 @@
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 import to_core as core
 import to_mu as m
 from tframe import console
diff --git a/third_party_libraries/gam_rhn/05-TO/t5_lstm.py b/third_party_libraries/gam_rhn/05-TO/t5_lstm.py
index 6d4f2ca..db9e3ff 100644
--- a/third_party_libraries/gam_rhn/05-TO/t5_lstm.py
+++ b/third_party_libraries/gam_rhn/05-TO/t5_lstm.py
@@ -1,4 +1,4 @@
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 import to_core as core
 import to_mu as m
 from tframe import console
diff --git a/third_party_libraries/gam_rhn/05-TO/t5_rhn.py b/third_party_libraries/gam_rhn/05-TO/t5_rhn.py
index ef05f04..eddc39c 100644
--- a/third_party_libraries/gam_rhn/05-TO/t5_rhn.py
+++ b/third_party_libraries/gam_rhn/05-TO/t5_rhn.py
@@ -1,4 +1,4 @@
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 import to_core as core
 import to_mu as m
 from tframe import console
diff --git a/third_party_libraries/gam_rhn/95-FI2010/fi_core.py b/third_party_libraries/gam_rhn/95-FI2010/fi_core.py
deleted file mode 100644
index 4a37d36..0000000
--- a/third_party_libraries/gam_rhn/95-FI2010/fi_core.py
+++ /dev/null
@@ -1,118 +0,0 @@
-import sys, os
-
-ROOT = os.path.abspath(__file__)
-# Specify the directory depth with respect to the root of your project here
-# (The project root usually holds your data folder and has a depth of 0)
-DIR_DEPTH = 1
-for _ in range(DIR_DEPTH + 1):
-    ROOT = os.path.dirname(ROOT)
-    sys.path.insert(0, ROOT)
-import numpy as np
-import tensorflow as tf
-from tframe import console, SaveMode
-from tframe import Classifier
-from tframe.trainers import SmartTrainerHub as Config
-
-import fi_du as du
-
-from_root = lambda path: os.path.join(ROOT, path)
-
-# -----------------------------------------------------------------------------
-# Initialize config and set data/job dir
-# -----------------------------------------------------------------------------
-th = Config(as_global=True)
-th.data_dir = from_root("95-FI2010/data/")
-th.job_dir = from_root("95-FI2010")
-# -----------------------------------------------------------------------------
-# Device configurations
-# -----------------------------------------------------------------------------
-th.allow_growth = False
-th.gpu_memory_fraction = 0.3
-
-# -----------------------------------------------------------------------------
-# Set information about the data set
-# -----------------------------------------------------------------------------
-th.max_level = 10
-th.volume_only = True
-th.developer_code = "use_log"
-
-th.input_shape = [20]
-th.num_classes = 3
-th.output_dim = 3
-th.target_dim = 1
-th.target_dtype = tf.int32
-
-th.val_size = 1
-
-th.loss_string = "cross_entropy"
-
-# -----------------------------------------------------------------------------
-# Set common trainer configs
-# -----------------------------------------------------------------------------
-th.epoch = 1  # STOCK 1000
-
-th.gather_note = True
-th.export_tensors_upon_validation = True
-th.train = True
-th.overwrite = True
-th.save_model = True
-th.save_mode = SaveMode.ON_RECORD
-
-th.validation_per_round = 2
-th.early_stop = True
-th.patience = 10
-th.lives = 1
-
-th.clip_threshold = 1.0
-th.clip_method = "value"
-
-th.print_cycle = 50
-th.val_progress_bar = True
-
-th.val_batch_size = -1
-th.val_num_steps = 2000
-
-th.clip_nan_protection = False
-th.state_nan_protection = False
-th.terminate_on_nan = True
-
-th.lr_decay = 0.4
-th.sub_seq_len = 500  # 5000 stock
-
-th.use_conveyor = True
-th.conveyor_length = 15
-
-
-def activate():
-    # Load datasets
-    train_set, val_set, test_set = du.load_data(th.data_dir)
-    # Calculate class weights
-    if th.class_weights is None and th.loss_string == "wce":
-        train_targets = train_set.stack.targets.flatten()
-        samples_per_class = [sum(train_targets == c) for c in range(th.num_classes)]
-        class_weights = min(samples_per_class) / np.array(samples_per_class)
-        th.class_weights = class_weights
-        console.show_status("Class weights set to {}".format(th.class_weights), "++")
-
-    # Set input shape according to th.max_level and th.volume_only
-    du.FI2010.set_input_shape()
-
-    # Build model
-    assert callable(th.model)
-    model = th.model(th)
-    assert isinstance(model, Classifier)
-
-    # Train or evaluate
-    if th.train:
-        model.train(
-            train_set,
-            validation_set=val_set,
-            test_set=test_set,
-            trainer_hub=th,
-            evaluate=lambda t: du.FI2010.evaluate(t, test_set),
-        )
-    else:
-        du.FI2010.evaluate(model, test_set)
-    # End
-    model.shutdown()
-    console.end()
diff --git a/third_party_libraries/gam_rhn/95-FI2010/fi_du.py b/third_party_libraries/gam_rhn/95-FI2010/fi_du.py
deleted file mode 100644
index 6a96f22..0000000
--- a/third_party_libraries/gam_rhn/95-FI2010/fi_du.py
+++ /dev/null
@@ -1,37 +0,0 @@
-import numpy as np
-
-from tframe.data.sequences.finance.fi2010 import FI2010
-from tframe.data.sequences.seq_set import SequenceSet
-
-
-def load_data(path):
-    # Load training set and test set
-    train_set, test_set = FI2010.load(path, horizon=50)
-    # For this task, train_set is used for validation
-    val_set = get_balanced_seq_set(
-        train_set, name="Train*Set", M=min(test_set.structure)
-    )
-    return train_set, val_set, test_set
-
-
-def get_balanced_seq_set(seq_set, sections=None, name="Balanced Set", M=None):
-    assert isinstance(seq_set, SequenceSet)
-    if sections is None:
-        if M is None:
-            M = min(seq_set.structure)
-        sections = [int(np.ceil(s / M)) for s in seq_set.structure]
-    assert isinstance(sections, list)
-    features, targets = [], []
-    for x, y, s in zip(seq_set.features, seq_set.targets, sections):
-        if s == 1:
-            features.append(x)
-            targets.append(y)
-            continue
-        L = int(len(x) / s)
-        indices = [(i + 1) * L for i in range(s - 1)]
-        features += np.split(x, indices)
-        targets += np.split(y, indices)
-    balanced_set = SequenceSet(features, targets, name=name)
-    assert len(balanced_set.structure) == sum(sections)
-    assert sum(balanced_set.structure) == sum(seq_set.structure)
-    return balanced_set
diff --git a/third_party_libraries/gam_rhn/95-FI2010/fi_mu.py b/third_party_libraries/gam_rhn/95-FI2010/fi_mu.py
deleted file mode 100644
index 3148d9e..0000000
--- a/third_party_libraries/gam_rhn/95-FI2010/fi_mu.py
+++ /dev/null
@@ -1,58 +0,0 @@
-import tensorflow as tf
-from tframe import Classifier
-
-from tframe.layers import Input, Activation
-from tframe.layers.common import Dropout
-from tframe.models import Recurrent
-
-from tframe.layers.advanced import Dense
-from tframe.trainers import SmartTrainerHub as Config
-
-from tframe.data.sequences.finance.fi2010 import Extract
-from tframe.layers.specific.lob_extraction import Significance
-
-from tframe.nets.rnn_cells.conveyor import Conveyor
-from tframe.layers.common import Flatten, Reshape
-
-
-def typical(th, cells):
-  assert isinstance(th, Config)
-
-  # Initiate a model
-  model = Classifier(mark=th.mark, net_type=Recurrent)
-  # Add layers
-  model.add(Input(sample_shape=th.input_shape))
-
-  # Add hidden layers
-  if not isinstance(cells, (list, tuple)): cells = [cells]
-  for cell in cells: model.add(cell)
-  # Build model and return
-  output_and_build(model, th)
-  return model
-
-
-def output_and_build(model, th):
-  assert isinstance(model, Classifier) and isinstance(th, Config)
-  # Add output dropout if necessary
-  if th.output_dropout > 0: model.add(Dropout(1 - th.output_dropout))
-  # Add output layer
-  model.add(Dense(num_neurons=th.output_dim))
-  model.add(Activation('softmax'))
-
-  # Build model
-  model.build(loss=th.loss_string, metric=['loss', 'f1'], batch_metric='f1')
-
-
-def bl_config():
-  """e.g. th.archi_string = '10x60+5x120+1x20'"""
-  from fi_core import th
-  layers = th.archi_string.split('+')
-  configs = []
-  for layer in layers:
-    sizes = [int(s) for s in layer.split('x')]
-    assert len(sizes) == 2 and sizes[0] > 0 and sizes[1] > 0
-    configs.append(sizes)
-  assert len(configs) > 0
-  return configs
-
-
diff --git a/third_party_libraries/gam_rhn/95-FI2010/t95_bl_gam_rhn.py b/third_party_libraries/gam_rhn/95-FI2010/t95_bl_gam_rhn.py
deleted file mode 100644
index 4e6678f..0000000
--- a/third_party_libraries/gam_rhn/95-FI2010/t95_bl_gam_rhn.py
+++ /dev/null
@@ -1,124 +0,0 @@
-import tensorflow as tf
-import fi_core as core
-import fi_mu as m
-from tframe import console
-from tframe.utils.misc import date_string
-
-from tframe.nets.hyper.gam_rhn import GamRHN
-from tframe.layers.hyper.bilinear import Bilinear
-from tframe.layers.common import Flatten
-from tframe.nets.rnn_cells.conveyor import Conveyor
-from tframe.layers.common import Dropout
-
-# -----------------------------------------------------------------------------
-# Define model here
-# -----------------------------------------------------------------------------
-model_name = "bl_gam_rhn"
-id = 1
-
-
-def model(th):
-    assert isinstance(th, m.Config)
-    layers = [Conveyor(length=th.conveyor_length)]
-    # Bilinear part
-    bl_config = m.bl_config()
-    for d1, d2 in bl_config[:-1]:
-        layers.append(Bilinear(d1, d2, "relu", max_norm=th.max_norm))
-        if th.dropout > 0:
-            layers.append(Dropout(1 - th.dropout))
-
-    d1, d2 = bl_config[-1]
-    layers.append(Bilinear(d1, d2, "tanh", max_norm=th.max_norm))
-    layers.append(Flatten())
-
-    # GAM-RHN part
-    layers.append(
-        GamRHN(
-            gam_config=th.gam_config,
-            head_size=th.head_size,
-            state_size=th.state_size,
-            num_layers=th.num_layers,
-            kernel=th.hyper_kernel,
-            gam_dropout=th.gam_dropout,
-            rhn_dropout=th.rhn_dropout,
-        )
-    )
-    return m.typical(th, layers)
-
-
-def main(_):
-    console.start("{} on FI-2010 task".format(model_name.upper()))
-
-    th = core.th
-    # ---------------------------------------------------------------------------
-    # 0. folder/file names and device
-    # ---------------------------------------------------------------------------
-    th.job_dir += "/{:02d}_{}".format(id, model_name)
-    th.prefix = "{}_".format(date_string())
-    summ_name = model_name
-    th.visible_gpu_id = 0
-
-    th.suffix = ""
-    # ---------------------------------------------------------------------------
-    # 1. dataset setup
-    # ---------------------------------------------------------------------------
-    th.volume_only = True
-
-    # ---------------------------------------------------------------------------
-    # 2. model setup
-    # ---------------------------------------------------------------------------
-    th.model = model
-    th.conveyor_length = 15
-    th.conveyor_input_shape = [20]
-
-    # - - - - - - - - - - - - - - - - - - - - - - ↑ common - - ↓ model specific
-    # Bilinear part
-    th.archi_string = "20x60+15x80+10x100+5x120+5x4"
-    th.max_norm = 2.5
-
-    # GAM-RHN part
-    th.gam_config = "2x50"
-    th.head_size = 15
-
-    th.hyper_kernel = "gru"
-    th.state_size = 200
-    th.num_layers = 8
-
-    th.dropout = 0.2
-    th.gam_dropout = 0.3
-    th.rhn_dropout = 0.5
-    th.output_dropout = 0.5
-    # ---------------------------------------------------------------------------
-    # 3. trainer setup
-    # ---------------------------------------------------------------------------
-    th.batch_size = 64
-    th.sub_seq_len = 500  # 5000 stock
-    th.num_steps = 10
-
-    th.optimizer = tf.train.AdamOptimizer
-    th.learning_rate = 0.001
-
-    th.clip_threshold = 1.0
-    th.clip_method = "value"
-
-    th.validation_per_round = 2
-
-    th.lives = 5
-    th.lr_decay = 0.4
-    # ---------------------------------------------------------------------------
-    # 4. summary and note setup
-    # ---------------------------------------------------------------------------
-    th.train = True
-    th.overwrite = True
-
-    # ---------------------------------------------------------------------------
-    # 5. other stuff and activate
-    # ---------------------------------------------------------------------------
-    th.mark = "{}_{}".format(th.archi_string, GamRHN.mark())
-    th.gather_summ_name = th.prefix + summ_name + th.suffix + ".sum"
-    core.activate()
-
-
-if __name__ == "__main__":
-    tf.app.run()
-
Submodule tframe contains modified content
Submodule tframe 4c4289d..37c9f0b:
diff --git a/third_party_libraries/gam_rhn/tframe/activations.py b/third_party_libraries/gam_rhn/tframe/activations.py
index e50c8b6..9bad4f0 100644
--- a/third_party_libraries/gam_rhn/tframe/activations.py
+++ b/third_party_libraries/gam_rhn/tframe/activations.py
@@ -4,7 +4,7 @@ from __future__ import print_function
 
 import six
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.utils import checker
 from tframe.utils.arg_parser import Parser
diff --git a/third_party_libraries/gam_rhn/tframe/advanced/krause_evaluator.py b/third_party_libraries/gam_rhn/tframe/advanced/krause_evaluator.py
index 972b7d5..cf3c423 100644
--- a/third_party_libraries/gam_rhn/tframe/advanced/krause_evaluator.py
+++ b/third_party_libraries/gam_rhn/tframe/advanced/krause_evaluator.py
@@ -5,7 +5,7 @@ from __future__ import print_function
 from collections import OrderedDict
 import numpy as np
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from tframe import console
 from tframe import checker
 from tframe import pedia
diff --git a/third_party_libraries/gam_rhn/tframe/configs/config_base.py b/third_party_libraries/gam_rhn/tframe/configs/config_base.py
index 367adcf..8dff896 100644
--- a/third_party_libraries/gam_rhn/tframe/configs/config_base.py
+++ b/third_party_libraries/gam_rhn/tframe/configs/config_base.py
@@ -4,7 +4,7 @@ from __future__ import print_function
 
 import numpy as np
 from collections import OrderedDict
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 import tframe as tfr
 from tframe import pedia
diff --git a/third_party_libraries/gam_rhn/tframe/configs/flag.py b/third_party_libraries/gam_rhn/tframe/configs/flag.py
index 32723d2..e4be7a6 100644
--- a/third_party_libraries/gam_rhn/tframe/configs/flag.py
+++ b/third_party_libraries/gam_rhn/tframe/configs/flag.py
@@ -3,18 +3,24 @@ from __future__ import division
 from __future__ import print_function
 
 import re
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+tf.disable_v2_behavior()
 from tframe.enums import EnumPro
 
 flags = tf.app.flags
 
-
 # TODO: Value set to Flag should be checked
 
+
 class Flag(object):
-  def __init__(self, default_value, description, register=None, name=None,
-               is_key=False, **kwargs):
-    """
+    def __init__(self,
+                 default_value,
+                 description,
+                 register=None,
+                 name=None,
+                 is_key=False,
+                 **kwargs):
+        """
     ... another way to design this class is to let name attribute be assigned
     when registered, but we need to allow FLAGS names such as 'job-dir' to be
     legal. In this perspective, this design is better.
@@ -26,143 +32,163 @@ class Flag(object):
                     (3) None: show if has been modified
     :param register: if is None, this flag can not be passed via tf FLAGS
     """
-    self.name = name
-    self._default_value = default_value
-    self._description = description
-    self._register = register
-    self._is_key = is_key
-    self._kwargs = kwargs
-
-    self._value = default_value
-    self._frozen = False
-
-  # region : Properties
-
-  @property
-  def ready_to_be_key(self):
-    return self._is_key is None
-
-  @property
-  def is_key(self):
-    if self._is_key is False: return False
-    if not self.should_register: return self._is_key is True
-    assert hasattr(flags.FLAGS, self.name)
-    return self._is_key is True or getattr(flags.FLAGS, self.name) is not None
-
-  @property
-  def frozen(self):
-    return self._frozen
-
-  @property
-  def value(self):
-    # If not registered to tf.app.flags or has been frozen
-    if self._register is None or self._frozen: return self._value
-
-    assert hasattr(flags.FLAGS, self.name)
-    f_value = getattr(flags.FLAGS, self.name)
-    # Configs defined via tensorflow FLAGS have priority over any other way
-    if f_value is None: return self._value
-    # If self is en enum Flag, then f_value must be a string in
-    # .. self.enum_class.value_list(), so we need to get its member
-    if self.is_enum: f_value = self.enum_class.get_member(f_value)
-    if self.frozen and self._value != f_value:
-      raise AssertionError(
-        "!! Invalid tensorflow FLAGS value {0}={1} 'cause {0} has been "
-        "frozen to {2}".format(self.name, f_value, self._value))
-    return f_value
-
-  @property
-  def should_register(self):
-    return self._register is not None
-
-  @property
-  def enum_class(self):
-    cls = self._kwargs.get('enum_class', None)
-    if cls is None or not issubclass(cls, EnumPro): return None
-    return cls
-
-  @property
-  def is_enum(self):
-    return self.enum_class is not None and self._register is flags.DEFINE_enum
-
-  # endregion : Properties
-
-  # region : Class Methods
-
-  @classmethod
-  def whatever(cls, default_value, description, is_key=False):
-    return Flag(default_value, description, is_key=is_key)
-
-  @classmethod
-  def string(cls, default_value, description, name=None, is_key=False):
-    return Flag(default_value, description, flags.DEFINE_string, name,
-                is_key=is_key)
-
-  @classmethod
-  def boolean(cls, default_value, description, name=None, is_key=False):
-    return Flag(default_value, description, flags.DEFINE_boolean, name,
-                is_key=is_key)
-
-  @classmethod
-  def integer(cls, default_value, description, name=None, is_key=False):
-    return Flag(default_value, description, flags.DEFINE_integer, name,
-                is_key=is_key)
-
-  @classmethod
-  def float(cls, default_value, description, name=None, is_key=False):
-    return Flag(default_value, description, flags.DEFINE_float, name,
-                is_key=is_key)
-
-  @classmethod
-  def list(cls, default_value, description, name=None):
-    return Flag(default_value, description, flags.DEFINE_list, name)
-
-  @classmethod
-  def enum(cls, default_value, enum_class, description, name=None,
-           is_key=False):
-    assert issubclass(enum_class, EnumPro)
-    return Flag(default_value, description, flags.DEFINE_enum, name,
-                enum_class=enum_class, is_key=is_key)
-
-  # endregion : Class Methods
-
-  # region : Public Methods
-
-  def register(self, name):
-    # If name is not specified during construction, use flag's attribute name
-    # .. in Config
-    if self.name is None: self.name = name
-    if self._register is None or self.name in list(flags.FLAGS): return
-    # Register enum flag
-    if self.is_enum:
-      flags.DEFINE_enum(
-        self.name, None, self.enum_class.value_list(), self._description)
-      return
-    # Register other flag
-    assert self._register is not flags.DEFINE_enum
-    self._register(self.name, None, self._description)
-
-  def new_value(self, value):
-    flg = Flag(self._default_value, self._description, self._register,
-               self.name, **self._kwargs)
-    flg._value = value
-    return flg
-
-  def freeze(self, value):
-    self._value = value
-    self._frozen = True
-
-  # endregion : Public Methods
-
-  # region : Private Methods
-
-  @staticmethod
-  def parse_comma(arg, dtype=str):
-    r = re.fullmatch(r'([\-\d.,]+)', arg)
-    if r is None: raise AssertionError(
-      'Can not parse argument `{}`'.format(arg))
-    val_list = re.split(r'[,]', r.group())
-    return [dtype(v) for v in val_list]
-
-  # endregion : Private Methods
-
+        self.name = name
+        self._default_value = default_value
+        self._description = description
+        self._register = register
+        self._is_key = is_key
+        self._kwargs = kwargs
+
+        self._value = default_value
+        self._frozen = False
+
+    # region : Properties
+
+    @property
+    def ready_to_be_key(self):
+        return self._is_key is None
+
+    @property
+    def is_key(self):
+        if self._is_key is False: return False
+        if not self.should_register: return self._is_key is True
+        assert hasattr(flags.FLAGS, self.name)
+        return self._is_key is True or getattr(flags.FLAGS,
+                                               self.name) is not None
+
+    @property
+    def frozen(self):
+        return self._frozen
+
+    @property
+    def value(self):
+        # If not registered to tf.app.flags or has been frozen
+        if self._register is None or self._frozen: return self._value
+
+        assert hasattr(flags.FLAGS, self.name)
+        f_value = getattr(flags.FLAGS, self.name)
+        # Configs defined via tensorflow FLAGS have priority over any other way
+        if f_value is None: return self._value
+        # If self is en enum Flag, then f_value must be a string in
+        # .. self.enum_class.value_list(), so we need to get its member
+        if self.is_enum: f_value = self.enum_class.get_member(f_value)
+        if self.frozen and self._value != f_value:
+            raise AssertionError(
+                "!! Invalid tensorflow FLAGS value {0}={1} 'cause {0} has been "
+                "frozen to {2}".format(self.name, f_value, self._value))
+        return f_value
+
+    @property
+    def should_register(self):
+        return self._register is not None
+
+    @property
+    def enum_class(self):
+        cls = self._kwargs.get('enum_class', None)
+        if cls is None or not issubclass(cls, EnumPro): return None
+        return cls
+
+    @property
+    def is_enum(self):
+        return self.enum_class is not None and self._register is flags.DEFINE_enum
+
+    # endregion : Properties
+
+    # region : Class Methods
+
+    @classmethod
+    def whatever(cls, default_value, description, is_key=False):
+        return Flag(default_value, description, is_key=is_key)
+
+    @classmethod
+    def string(cls, default_value, description, name=None, is_key=False):
+        return Flag(default_value,
+                    description,
+                    flags.DEFINE_string,
+                    name,
+                    is_key=is_key)
+
+    @classmethod
+    def boolean(cls, default_value, description, name=None, is_key=False):
+        return Flag(default_value,
+                    description,
+                    flags.DEFINE_boolean,
+                    name,
+                    is_key=is_key)
+
+    @classmethod
+    def integer(cls, default_value, description, name=None, is_key=False):
+        return Flag(default_value,
+                    description,
+                    flags.DEFINE_integer,
+                    name,
+                    is_key=is_key)
+
+    @classmethod
+    def float(cls, default_value, description, name=None, is_key=False):
+        return Flag(default_value,
+                    description,
+                    flags.DEFINE_float,
+                    name,
+                    is_key=is_key)
+
+    @classmethod
+    def list(cls, default_value, description, name=None):
+        return Flag(default_value, description, flags.DEFINE_list, name)
+
+    @classmethod
+    def enum(cls,
+             default_value,
+             enum_class,
+             description,
+             name=None,
+             is_key=False):
+        assert issubclass(enum_class, EnumPro)
+        return Flag(default_value,
+                    description,
+                    flags.DEFINE_enum,
+                    name,
+                    enum_class=enum_class,
+                    is_key=is_key)
+
+    # endregion : Class Methods
+
+    # region : Public Methods
+
+    def register(self, name):
+        # If name is not specified during construction, use flag's attribute name
+        # .. in Config
+        if self.name is None: self.name = name
+        if self._register is None or self.name in list(flags.FLAGS): return
+        # Register enum flag
+        if self.is_enum:
+            flags.DEFINE_enum(self.name, None, self.enum_class.value_list(),
+                              self._description)
+            return
+        # Register other flag
+        assert self._register is not flags.DEFINE_enum
+        self._register(self.name, None, self._description)
+
+    def new_value(self, value):
+        flg = Flag(self._default_value, self._description, self._register,
+                   self.name, **self._kwargs)
+        flg._value = value
+        return flg
+
+    def freeze(self, value):
+        self._value = value
+        self._frozen = True
+
+    # endregion : Public Methods
+
+    # region : Private Methods
+
+    @staticmethod
+    def parse_comma(arg, dtype=str):
+        r = re.fullmatch(r'([\-\d.,]+)', arg)
+        if r is None:
+            raise AssertionError('Can not parse argument `{}`'.format(arg))
+        val_list = re.split(r'[,]', r.group())
+        return [dtype(v) for v in val_list]
+
+    # endregion : Private Methods
diff --git a/third_party_libraries/gam_rhn/tframe/configs/trainer_configs.py b/third_party_libraries/gam_rhn/tframe/configs/trainer_configs.py
index ba65b5f..1d7e670 100644
--- a/third_party_libraries/gam_rhn/tframe/configs/trainer_configs.py
+++ b/third_party_libraries/gam_rhn/tframe/configs/trainer_configs.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from .flag import Flag
 from tframe.utils.arg_parser import Parser
 
diff --git a/third_party_libraries/gam_rhn/tframe/core/agent.py b/third_party_libraries/gam_rhn/tframe/core/agent.py
index 5ef39e3..30b2ce4 100644
--- a/third_party_libraries/gam_rhn/tframe/core/agent.py
+++ b/third_party_libraries/gam_rhn/tframe/core/agent.py
@@ -5,7 +5,8 @@ from __future__ import print_function
 import os
 import time
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+tf.disable_v2_behavior()
 import tframe as tfr
 
 from tframe import hub
@@ -23,336 +24,366 @@ from tframe.core.decorators import with_graph
 
 
 class Agent(object):
-  """An Agent works for TFrame Model, handling tensorflow stuffs"""
-  def __init__(self, model, graph=None):
-    # Each agent works on one tensorflow graph with a tensorflow session
-    # .. set to it
-    assert isinstance(model, tfr.models.Model)
-    self._model = model
-    self._session = None
-    self._graph = None
-    # Graph variables
-    self._is_training = None
-    self._init_graph(graph)
-    # An agent saves model and writes summary
-    self._saver = None
-    self._summary_writer = None
-    # An agent holds a default note
-    self._note = Note()
-    context.note = self._note
-
-  # region : Properties
-
-  # region : Accessors
-
-  @property
-  def graph(self):
-    assert isinstance(self._graph, tf.Graph)
-    return self._graph
-
-  @property
-  def session(self):
-    assert isinstance(self._session, tf.Session)
-    return self._session
-
-  @property
-  def saver(self):
-    assert isinstance(self._saver, tf.train.Saver)
-    return self._saver
-
-  @property
-  def summary_writer(self):
-    assert isinstance(self._summary_writer, tf.summary.FileWriter)
-    return self._summary_writer
-
-  # endregion : Accessors
-
-  # region : Paths
-
-  @property
-  def root_path(self):
-    if hub.job_dir == './': return hub.record_dir
-    else: return hub.job_dir
-
-  @property
-  def note_dir(self):
-    return check_path(self.root_path, hub.note_folder_name,
-                      self._model.mark, create_path=hub.export_note)
-  @property
-  def log_dir(self):
-    return check_path(self.root_path, hub.log_folder_name,
-                      self._model.mark, create_path=hub.summary)
-  @property
-  def ckpt_dir(self):
-    if hub.specified_ckpt_path is not None: return hub.specified_ckpt_path
-    return check_path(self.root_path, hub.ckpt_folder_name,
-                      self._model.mark, create_path=hub.save_model)
-  @property
-  def snapshot_dir(self):
-    return check_path(self.root_path, hub.snapshot_folder_name,
-                      self._model.mark, create_path=hub.snapshot)
-  @property
-  def model_path(self):
-    """This property will be used only when checkpoint is to be saved.
+    """An Agent works for TFrame Model, handling tensorflow stuffs"""
+    def __init__(self, model, graph=None):
+        # Each agent works on one tensorflow graph with a tensorflow session
+        # .. set to it
+        assert isinstance(model, tfr.models.Model)
+        self._model = model
+        self._session = None
+        self._graph = None
+        # Graph variables
+        self._is_training = None
+        self._init_graph(graph)
+        # An agent saves model and writes summary
+        self._saver = None
+        self._summary_writer = None
+        # An agent holds a default note
+        self._note = Note()
+        context.note = self._note
+
+    # region : Properties
+
+    # region : Accessors
+
+    @property
+    def graph(self):
+        assert isinstance(self._graph, tf.Graph)
+        return self._graph
+
+    @property
+    def session(self):
+        assert isinstance(self._session, tf.Session)
+        return self._session
+
+    @property
+    def saver(self):
+        assert isinstance(self._saver, tf.train.Saver)
+        return self._saver
+
+    @property
+    def summary_writer(self):
+        assert isinstance(self._summary_writer, tf.summary.FileWriter)
+        return self._summary_writer
+
+    # endregion : Accessors
+
+    # region : Paths
+
+    @property
+    def root_path(self):
+        if hub.job_dir == './': return hub.record_dir
+        else: return hub.job_dir
+
+    @property
+    def note_dir(self):
+        return check_path(self.root_path,
+                          hub.note_folder_name,
+                          self._model.mark,
+                          create_path=hub.export_note)
+
+    @property
+    def log_dir(self):
+        return check_path(self.root_path,
+                          hub.log_folder_name,
+                          self._model.mark,
+                          create_path=hub.summary)
+
+    @property
+    def ckpt_dir(self):
+        if hub.specified_ckpt_path is not None: return hub.specified_ckpt_path
+        return check_path(self.root_path,
+                          hub.ckpt_folder_name,
+                          self._model.mark,
+                          create_path=hub.save_model)
+
+    @property
+    def snapshot_dir(self):
+        return check_path(self.root_path,
+                          hub.snapshot_folder_name,
+                          self._model.mark,
+                          create_path=hub.snapshot)
+
+    @property
+    def model_path(self):
+        """This property will be used only when checkpoint is to be saved.
         Old name format: XXXX.model
         New name example: recurrent.predictor(26.799_epochs)-train-1800
         Where XXXX denotes self._model.model_name
     """
-    name = '{}.{}'.format(self._model.affix, self._model.model_name.lower())
-    return os.path.join(self.ckpt_dir, name)
+        name = '{}.{}'.format(self._model.affix,
+                              self._model.model_name.lower())
+        return os.path.join(self.ckpt_dir, name)
 
-  @property
-  def gather_path(self):
-    return os.path.join(check_path(self.root_path), hub.gather_file_name)
+    @property
+    def gather_path(self):
+        return os.path.join(check_path(self.root_path), hub.gather_file_name)
 
-  @property
-  def gather_summ_path(self):
-    return os.path.join(check_path(self.root_path), hub.gather_summ_name)
+    @property
+    def gather_summ_path(self):
+        return os.path.join(check_path(self.root_path), hub.gather_summ_name)
 
-  # endregion : Paths
+    # endregion : Paths
 
-  # endregion : Properties
+    # endregion : Properties
 
-  # region : Public Methods
+    # region : Public Methods
 
-  def get_status_feed_dict(self, is_training):
-    assert isinstance(is_training, bool)
-    feed_dict = {self._is_training: is_training}
-    return feed_dict
+    def get_status_feed_dict(self, is_training):
+        assert isinstance(is_training, bool)
+        feed_dict = {self._is_training: is_training}
+        return feed_dict
 
-  def load(self):
-    # TODO: when save_model option is turned off and the user want to
-    #   try loading the exist model, set overwrite to False
-    if not hub.save_model and hub.overwrite: return False, 0, None
-    return load_checkpoint(self.ckpt_dir, self.session, self._saver)
+    def load(self):
+        # TODO: when save_model option is turned off and the user want to
+        #   try loading the exist model, set overwrite to False
+        if not hub.save_model and hub.overwrite: return False, 0, None
+        return load_checkpoint(self.ckpt_dir, self.session, self._saver)
 
-  def save_model(self, rounds=None, suffix=None):
-    """rounds is used only by trainer"""
-    path = self.model_path
-    if rounds is not None: path += '({:.3f}_rounds)'.format(rounds)
-    if suffix is not None: path += '-{}'.format(suffix)
-    save_checkpoint(path, self.session, self._saver, self._model.counter)
+    def save_model(self, rounds=None, suffix=None):
+        """rounds is used only by trainer"""
+        path = self.model_path
+        if rounds is not None: path += '({:.3f}_rounds)'.format(rounds)
+        if suffix is not None: path += '-{}'.format(suffix)
+        save_checkpoint(path, self.session, self._saver, self._model.counter)
 
-  @with_graph
-  def reset_saver(self):
-    """This method will be used in some very special cased, e.g. for
+    @with_graph
+    def reset_saver(self):
+        """This method will be used in some very special cased, e.g. for
        saving train_stats used in dynamic evaluation (krause, 2018)
     """
-    self._saver = tf.train.Saver(
-      var_list=self._model.variable_to_save, max_to_keep=2)
-
-  @with_graph
-  def launch_model(self, overwrite=False):
-    if hub.suppress_logging: console.suppress_logging()
-    # Before launch session, do some cleaning work
-    if overwrite and hub.overwrite:
-      paths = []
-      if hub.summary: paths.append(self.log_dir)
-      if hub.save_model: paths.append(self.ckpt_dir)
-      if hub.snapshot: paths.append(self.snapshot_dir)
-      if hub.export_note: paths.append(self.note_dir)
-      clear_paths(paths)
-    if hub.summary: self._check_bash()
-
-    # Launch session on self.graph
-    console.show_status('Launching session ...')
-    config = tf.ConfigProto()
-    if hub.visible_gpu_id is not None:
-      gpu_id = hub.visible_gpu_id
-      if isinstance(gpu_id, int): gpu_id = '{}'.format(gpu_id)
-      elif not isinstance(gpu_id, str): raise TypeError(
-        '!! Visible GPU id provided must be an integer or a string')
-      os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id
-    if not hub.allow_growth:
-      value = hub.gpu_memory_fraction
-      config.gpu_options.per_process_gpu_memory_fraction = value
-    self._session = tf.Session(graph=self._graph, config=config)
-    console.show_status('Session launched')
-    # Prepare some tools
-    self.reset_saver()
-    if hub.summary or hub.hp_tuning:
-      self._summary_writer = tf.summary.FileWriter(self.log_dir)
-
-    # Initialize all variables
-    self._session.run(tf.global_variables_initializer())
-    # Set init_val for pruner if necessary
-    # .. if existed model is loaded, variables will be overwritten
-    if hub.prune_on: context.pruner.set_init_val_lottery18()
-
-    # Try to load exist model
-    load_flag, self._model.counter, self._model.rounds = self.load()
-    # Sanity check
-    if hub.prune_on and hub.pruning_iterations > 0:
-      if not load_flag: raise AssertionError(
-        '!! Model {} should be initialized'.format(self._model.mark))
-
-    if not load_flag:
-      assert self._model.counter == 0
-      # Add graph
-      if hub.summary: self._summary_writer.add_graph(self._session.graph)
-      # Write model description to file
-      if hub.snapshot:
-        description_path = os.path.join(self.snapshot_dir, 'description.txt')
-        write_file(description_path, self._model.description)
-      # Show status
-      console.show_status('New model initiated')
-    elif hub.branch_suffix not in [None, '']:
-      hub.mark += hub.branch_suffix
-      self._model.mark = hub.mark
-      console.show_status('Checkpoint switched to branch `{}`'.format(hub.mark))
-
-    self._model.launched = True
-    self.take_notes('Model launched')
-
-    # Handle structure detail here
-    self._model.handle_structure_detail()
-
-    return load_flag
-
-  def shutdown(self):
-    if hub.summary or hub.hp_tuning:
-      self._summary_writer.close()
-    self.session.close()
-
-  def write_summary(self, summary, step=None):
-    if not hub.summary: return
-    if step is None:
-      assert context.trainer is not None
-      if hub.epoch_as_step and context.trainer.total_rounds is not None:
-        step = int(context.trainer.total_rounds * 1000)
-      else: step = self._model.counter
-    assert isinstance(self._summary_writer, tf.summary.FileWriter)
-    self._summary_writer.add_summary(summary, step)
-
-  def save_plot(self, fig, filename):
-    imtool.save_plt(fig, '{}/{}'.format(self.snapshot_dir, filename))
-
-  # endregion : Public Methods
-
-  # region : Public Methods for Note
-
-  # region : For TensorViewer
-
-  def take_down_scalars_and_tensors(self, scalars, tensors):
-    assert isinstance(scalars, dict) and isinstance(tensors, dict)
-    if hub.epoch_as_step and context.trainer.total_rounds is not None:
-      step = int(context.trainer.total_rounds * 1000)
-    else: step = self._model.counter
-    self._note.take_down_scalars_and_tensors(step, scalars, tensors)
-
-  # endregion : For TensorViewer
-
-  # region : For SummaryViewer
-
-  def put_down_configs(self, th):
-    assert isinstance(th, Config)
-    self._note.put_down_configs(th.key_options)
-
-  def put_down_criterion(self, name, value):
-    self._note.put_down_criterion(name, value)
-
-  def gather_to_summary(self):
-    import pickle
-    # Try to load note list into summaries
-    file_path = self.gather_summ_path
-    if os.path.exists(file_path):
-      with open(file_path, 'rb') as f: summary = pickle.load(f)
-      assert len(summary) > 0
-    else: summary = []
-    # Add note to list and save
-    note = self._note.tensor_free if hub.gather_only_scalars else self._note
-    summary.append(note)
-    with open(file_path, 'wb') as f:
-      pickle.dump(summary, f, pickle.HIGHEST_PROTOCOL)
-    # Show status
-    console.show_status('Note added to summaries ({} => {}) at `{}`'.format(
-      len(summary) - 1, len(summary), file_path))
-
-  # endregion : For SummaryViewer
-
-  def take_notes(self, content, date_time=True, prompt=None):
-    if not isinstance(content, str):
-      raise TypeError('!! content must be a string')
-    if isinstance(prompt, str):
-      date_time = False
-      content = '{} {}'.format(prompt, content)
-    if date_time:
-      time_str = time.strftime('[{}-{}-%d %H:%M:%S]'.format(
-        time.strftime('%Y')[2:], time.strftime('%B')[:3]))
-      content = '{} {}'.format(time_str, content)
-
-    self._note.write_line(content)
-
-  def show_notes(self):
-    console.section('Notes')
-    console.write_line(self._note.content)
-
-  def export_notes(self, filename='notes'):
-    assert hub.export_note
-    # Export .txt file
-    file_path = '{}/{}.txt'.format(self.note_dir, filename)
-    writer = open(file_path, 'a')
-    writer.write('=' * 79 + '\n')
-    writer.write(self._note.content + '\n')
-    writer.close()
-    # Export .note file
-    file_path = '{}/{}.note'.format(self.note_dir, filename)
-    self._note.save(file_path)
-    console.show_status('Note exported to `{}`'.format(file_path))
-
-  def gather_notes(self, take_down_time=False):
-    assert hub.gather_note
-    # If gather file does not exist, create one
-    with open(self.gather_path, 'a'): pass
-    # Gather notes to .txt file
-    line = self._note.content
-    with open(self.gather_path, 'r+') as f:
-      content = f.readlines()
-      f.seek(0)
-      f.truncate()
-      if take_down_time:
-        time_str = time.strftime('[{}-{}-%d %H:%M:%S]'.format(
-          time.strftime('%Y')[2:], time.strftime('%B')[:3]))
-        line = '[{}] {}'.format(time_str, line)
-      f.write(line + '\n')
-      f.write('-' * 79 + '\n')
-      f.writelines(content)
-      # TODO: find a way to update immediately after training is over
-    # Gather notes to .summ file
-    self.gather_to_summary()
-
-  # endregion : Public Methods for Note
-
-  # region : Private Methods
-
-  def _init_graph(self, graph):
-    if graph is not None:
-      assert isinstance(graph, tf.Graph)
-      self._graph = graph
-    else: self._graph = tf.Graph()
-    # Initialize graph variables
-    with self.graph.as_default():
-      self._is_training = tf.placeholder(
-        dtype=tf.bool, name=pedia.is_training)
-      tf.add_to_collection(pedia.is_training, self._is_training)
-
-    # TODO
-    # When linking batch-norm layer (and dropout layer),
-    #   this placeholder will be got from default graph
-    # self._graph.is_training = self._is_training
-    # assert context.current_graph is not None
-    if not hub.suppress_current_graph: context.current_graph = self._graph
-    # tfr.current_graph = self._graph
-
-  def _check_bash(self):
-    command = 'tensorboard --logdir=./logs/ --port={}'.format(hub.tb_port)
-    file_path = check_path(self.root_path, create_path=True)
-    file_names = ['win_launch_tensorboard.bat', 'unix_launch_tensorboard.sh']
-    for file_name in file_names:
-      path = os.path.join(file_path, file_name)
-      if not os.path.exists(path):
-        f = open(path, 'w')
-        f.write(command)
-        f.close()
-
-  # endregion : Private Methods
+        self._saver = tf.train.Saver(var_list=self._model.variable_to_save,
+                                     max_to_keep=2)
+
+    @with_graph
+    def launch_model(self, overwrite=False):
+        if hub.suppress_logging: console.suppress_logging()
+        # Before launch session, do some cleaning work
+        if overwrite and hub.overwrite:
+            paths = []
+            if hub.summary: paths.append(self.log_dir)
+            if hub.save_model: paths.append(self.ckpt_dir)
+            if hub.snapshot: paths.append(self.snapshot_dir)
+            if hub.export_note: paths.append(self.note_dir)
+            clear_paths(paths)
+        if hub.summary: self._check_bash()
+
+        # Launch session on self.graph
+        console.show_status('Launching session ...')
+        config = tf.ConfigProto()
+        if hub.visible_gpu_id is not None:
+            gpu_id = hub.visible_gpu_id
+            if isinstance(gpu_id, int): gpu_id = '{}'.format(gpu_id)
+            elif not isinstance(gpu_id, str):
+                raise TypeError(
+                    '!! Visible GPU id provided must be an integer or a string'
+                )
+            os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id
+        if not hub.allow_growth:
+            value = hub.gpu_memory_fraction
+            config.gpu_options.per_process_gpu_memory_fraction = value
+        self._session = tf.Session(graph=self._graph, config=config)
+        console.show_status('Session launched')
+        # Prepare some tools
+        self.reset_saver()
+        if hub.summary or hub.hp_tuning:
+            self._summary_writer = tf.summary.FileWriter(self.log_dir)
+
+        # Initialize all variables
+        self._session.run(tf.global_variables_initializer())
+        # Set init_val for pruner if necessary
+        # .. if existed model is loaded, variables will be overwritten
+        if hub.prune_on: context.pruner.set_init_val_lottery18()
+
+        # Try to load exist model
+        load_flag, self._model.counter, self._model.rounds = self.load()
+        # Sanity check
+        if hub.prune_on and hub.pruning_iterations > 0:
+            if not load_flag:
+                raise AssertionError(
+                    '!! Model {} should be initialized'.format(
+                        self._model.mark))
+
+        if not load_flag:
+            assert self._model.counter == 0
+            # Add graph
+            if hub.summary: self._summary_writer.add_graph(self._session.graph)
+            # Write model description to file
+            if hub.snapshot:
+                description_path = os.path.join(self.snapshot_dir,
+                                                'description.txt')
+                write_file(description_path, self._model.description)
+            # Show status
+            console.show_status('New model initiated')
+        elif hub.branch_suffix not in [None, '']:
+            hub.mark += hub.branch_suffix
+            self._model.mark = hub.mark
+            console.show_status('Checkpoint switched to branch `{}`'.format(
+                hub.mark))
+
+        self._model.launched = True
+        self.take_notes('Model launched')
+
+        # Handle structure detail here
+        self._model.handle_structure_detail()
+
+        return load_flag
+
+    def shutdown(self):
+        if hub.summary or hub.hp_tuning:
+            self._summary_writer.close()
+        self.session.close()
+
+    def write_summary(self, summary, step=None):
+        if not hub.summary: return
+        if step is None:
+            assert context.trainer is not None
+            if hub.epoch_as_step and context.trainer.total_rounds is not None:
+                step = int(context.trainer.total_rounds * 1000)
+            else:
+                step = self._model.counter
+        assert isinstance(self._summary_writer, tf.summary.FileWriter)
+        self._summary_writer.add_summary(summary, step)
+
+    def save_plot(self, fig, filename):
+        imtool.save_plt(fig, '{}/{}'.format(self.snapshot_dir, filename))
+
+    # endregion : Public Methods
+
+    # region : Public Methods for Note
+
+    # region : For TensorViewer
+
+    def take_down_scalars_and_tensors(self, scalars, tensors):
+        assert isinstance(scalars, dict) and isinstance(tensors, dict)
+        if hub.epoch_as_step and context.trainer.total_rounds is not None:
+            step = int(context.trainer.total_rounds * 1000)
+        else:
+            step = self._model.counter
+        self._note.take_down_scalars_and_tensors(step, scalars, tensors)
+
+    # endregion : For TensorViewer
+
+    # region : For SummaryViewer
+
+    def put_down_configs(self, th):
+        assert isinstance(th, Config)
+        self._note.put_down_configs(th.key_options)
+
+    def put_down_criterion(self, name, value):
+        self._note.put_down_criterion(name, value)
+
+    def gather_to_summary(self):
+        import pickle
+        # Try to load note list into summaries
+        file_path = self.gather_summ_path
+        if os.path.exists(file_path):
+            with open(file_path, 'rb') as f:
+                summary = pickle.load(f)
+            assert len(summary) > 0
+        else:
+            summary = []
+        # Add note to list and save
+        note = self._note.tensor_free if hub.gather_only_scalars else self._note
+        summary.append(note)
+        with open(file_path, 'wb') as f:
+            pickle.dump(summary, f, pickle.HIGHEST_PROTOCOL)
+        # Show status
+        console.show_status(
+            'Note added to summaries ({} => {}) at `{}`'.format(
+                len(summary) - 1, len(summary), file_path))
+
+    # endregion : For SummaryViewer
+
+    def take_notes(self, content, date_time=True, prompt=None):
+        if not isinstance(content, str):
+            raise TypeError('!! content must be a string')
+        if isinstance(prompt, str):
+            date_time = False
+            content = '{} {}'.format(prompt, content)
+        if date_time:
+            time_str = time.strftime('[{}-{}-%d %H:%M:%S]'.format(
+                time.strftime('%Y')[2:],
+                time.strftime('%B')[:3]))
+            content = '{} {}'.format(time_str, content)
+
+        self._note.write_line(content)
+
+    def show_notes(self):
+        console.section('Notes')
+        console.write_line(self._note.content)
+
+    def export_notes(self, filename='notes'):
+        assert hub.export_note
+        # Export .txt file
+        file_path = '{}/{}.txt'.format(self.note_dir, filename)
+        writer = open(file_path, 'a')
+        writer.write('=' * 79 + '\n')
+        writer.write(self._note.content + '\n')
+        writer.close()
+        # Export .note file
+        file_path = '{}/{}.note'.format(self.note_dir, filename)
+        self._note.save(file_path)
+        console.show_status('Note exported to `{}`'.format(file_path))
+
+    def gather_notes(self, take_down_time=False):
+        assert hub.gather_note
+        # If gather file does not exist, create one
+        with open(self.gather_path, 'a'):
+            pass
+        # Gather notes to .txt file
+        line = self._note.content
+        with open(self.gather_path, 'r+') as f:
+            content = f.readlines()
+            f.seek(0)
+            f.truncate()
+            if take_down_time:
+                time_str = time.strftime('[{}-{}-%d %H:%M:%S]'.format(
+                    time.strftime('%Y')[2:],
+                    time.strftime('%B')[:3]))
+                line = '[{}] {}'.format(time_str, line)
+            f.write(line + '\n')
+            f.write('-' * 79 + '\n')
+            f.writelines(content)
+            # TODO: find a way to update immediately after training is over
+        # Gather notes to .summ file
+        self.gather_to_summary()
+
+    # endregion : Public Methods for Note
+
+    # region : Private Methods
+
+    def _init_graph(self, graph):
+        if graph is not None:
+            assert isinstance(graph, tf.Graph)
+            self._graph = graph
+        else:
+            self._graph = tf.Graph()
+        # Initialize graph variables
+        with self.graph.as_default():
+            self._is_training = tf.placeholder(dtype=tf.bool,
+                                               name=pedia.is_training)
+            tf.add_to_collection(pedia.is_training, self._is_training)
+
+        # TODO
+        # When linking batch-norm layer (and dropout layer),
+        #   this placeholder will be got from default graph
+        # self._graph.is_training = self._is_training
+        # assert context.current_graph is not None
+        if not hub.suppress_current_graph: context.current_graph = self._graph
+        # tfr.current_graph = self._graph
+
+    def _check_bash(self):
+        command = 'tensorboard --logdir=./logs/ --port={}'.format(hub.tb_port)
+        file_path = check_path(self.root_path, create_path=True)
+        file_names = [
+            'win_launch_tensorboard.bat', 'unix_launch_tensorboard.sh'
+        ]
+        for file_name in file_names:
+            path = os.path.join(file_path, file_name)
+            if not os.path.exists(path):
+                f = open(path, 'w')
+                f.write(command)
+                f.close()
+
+    # endregion : Private Methods
diff --git a/third_party_libraries/gam_rhn/tframe/core/context.py b/third_party_libraries/gam_rhn/tframe/core/context.py
index eaaf6c6..0f3e8c3 100644
--- a/third_party_libraries/gam_rhn/tframe/core/context.py
+++ b/third_party_libraries/gam_rhn/tframe/core/context.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from collections import OrderedDict
 
 from tframe.configs.config_base import Config
diff --git a/third_party_libraries/gam_rhn/tframe/core/decorators.py b/third_party_libraries/gam_rhn/tframe/core/decorators.py
index 70301c1..c1a3707 100644
--- a/third_party_libraries/gam_rhn/tframe/core/decorators.py
+++ b/third_party_libraries/gam_rhn/tframe/core/decorators.py
@@ -1,4 +1,4 @@
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 import tframe as tfr
 
diff --git a/third_party_libraries/gam_rhn/tframe/core/function.py b/third_party_libraries/gam_rhn/tframe/core/function.py
index 1586f8d..59e3cab 100644
--- a/third_party_libraries/gam_rhn/tframe/core/function.py
+++ b/third_party_libraries/gam_rhn/tframe/core/function.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 class Function(object):
diff --git a/third_party_libraries/gam_rhn/tframe/core/quantity.py b/third_party_libraries/gam_rhn/tframe/core/quantity.py
index 4189932..8cf3fb8 100644
--- a/third_party_libraries/gam_rhn/tframe/core/quantity.py
+++ b/third_party_libraries/gam_rhn/tframe/core/quantity.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 import tframe as tfr
 
 
diff --git a/third_party_libraries/gam_rhn/tframe/core/slots.py b/third_party_libraries/gam_rhn/tframe/core/slots.py
index f455ec5..8feacca 100644
--- a/third_party_libraries/gam_rhn/tframe/core/slots.py
+++ b/third_party_libraries/gam_rhn/tframe/core/slots.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 import tframe as tfr
 from .quantity import Quantity
 
diff --git a/third_party_libraries/gam_rhn/tframe/data/images/mnist.py b/third_party_libraries/gam_rhn/tframe/data/images/mnist.py
index 36c1be1..1dc74b7 100644
--- a/third_party_libraries/gam_rhn/tframe/data/images/mnist.py
+++ b/third_party_libraries/gam_rhn/tframe/data/images/mnist.py
@@ -7,7 +7,7 @@ import gzip
 import numpy as np
 from collections import OrderedDict
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import console, context, hub
 from tframe import pedia
diff --git a/third_party_libraries/gam_rhn/tframe/data/sequences/cifar10.py b/third_party_libraries/gam_rhn/tframe/data/sequences/cifar10.py
index b172600..2b1e65c 100644
--- a/third_party_libraries/gam_rhn/tframe/data/sequences/cifar10.py
+++ b/third_party_libraries/gam_rhn/tframe/data/sequences/cifar10.py
@@ -6,7 +6,7 @@ import os
 import numpy as np
 import random
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import console
diff --git a/third_party_libraries/gam_rhn/tframe/data/sequences/finance/fi2010.py b/third_party_libraries/gam_rhn/tframe/data/sequences/finance/fi2010.py
index 370b3f4..4009434 100644
--- a/third_party_libraries/gam_rhn/tframe/data/sequences/finance/fi2010.py
+++ b/third_party_libraries/gam_rhn/tframe/data/sequences/finance/fi2010.py
@@ -3,7 +3,8 @@ from __future__ import division
 from __future__ import print_function
 
 import os
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+tf.disable_v2_behavior()
 
 import math
 import numpy as np
@@ -51,26 +52,27 @@ class FI2010(DataAgent):
     DATA_NAME = "FI-2010"
     DATA_URL = "https://etsin.fairdata.fi/api/dl?cr_id=73eb48d7-4dbc-4a10-a52a-da745b47a649&file_id=5b32ac028ab4d130110888f19872320"
     DAY_LENGTH = {
-        True: [47342, 45114, 33720, 43252, 41171, 47253, 45099, 59973, 57951, 37250],
-        False: [39512, 38397, 28535, 37023, 34785, 39152, 37346, 55478, 52172, 31937],
+        True:
+        [47342, 45114, 33720, 43252, 41171, 47253, 45099, 59973, 57951, 37250],
+        False:
+        [39512, 38397, 28535, 37023, 34785, 39152, 37346, 55478, 52172, 31937],
     }
     STOCK_IDs = ["KESBV", "OUT1V", "SAMPO", "RTRKS", "WRT1V"]
     LEN_PER_DAY_PER_STOCK = "LEN_PER_DAY_PER_STOCK"
 
     @classmethod
-    def load(
-        cls,
-        data_dir,
-        auction=False,
-        norm_type="zscore",
-        setup=2,
-        val_size=None,
-        horizon=100,
-        **kwargs
-    ):
-        should_apply_norm = any(
-            ["use_log" not in th.developer_code, "force_norm" in th.developer_code]
-        )
+    def load(cls,
+             data_dir,
+             auction=False,
+             norm_type="zscore",
+             setup=2,
+             val_size=None,
+             horizon=100,
+             **kwargs):
+        should_apply_norm = any([
+            "use_log" not in th.developer_code,
+            "force_norm" in th.developer_code
+        ])
         # Sanity check
         assert setup in [1, 2]
         # Load raw LOB data
@@ -82,13 +84,16 @@ class FI2010(DataAgent):
 
         home = str(Path.home())
 
-        file_name = "model_name=two_model&WL=15&pt=1&sl=1&min_ret=0.0021&vbs=240&head=0&skip=0&fraction=1&vol_max=0.0022&vol_min=0.00210001&filter_type=none&cm_vol_mod=0&sample_weights=on&frac_diff=off&prices_type=orderbook&ntb=True&tslbc=True.h5"
+        file_name = "2ml27cbo_gam_rhn.h5"
         path = home + "/ProdigyAI/data/preprocessed/" + file_name
         h5f = h5py.File(path, "r")
         X = h5f["X"][:]
         y = h5f["y"][:]
         h5f.close()
 
+        import pdb
+        pdb.set_trace()
+
         X_zeros = np.zeros((X.shape))
         X = np.concatenate((X, X_zeros), axis=1)
         y = np.add(y, 1)  # go from -1,0,1 to 0,1,2
@@ -99,7 +104,7 @@ class FI2010(DataAgent):
         lob_set.data_dict["raw_data"] = [X]
 
         def round_down(n, decimals=0):
-            multiplier = 10 ** decimals
+            multiplier = 10**decimals
             return math.floor(n * multiplier) / multiplier
 
         number = round_down(len(y) / 10)
@@ -118,33 +123,34 @@ class FI2010(DataAgent):
 
         if should_apply_norm:
             train_set, test_set = cls._apply_normalization(
-                train_set, test_set, norm_type
-            )
-        if kwargs.get("validate_setup2") and setup == 2 and norm_type == "zscore":
+                train_set, test_set, norm_type)
+        if kwargs.get(
+                "validate_setup2") and setup == 2 and norm_type == "zscore":
             cls._validate_setup2(data_dir, auction, train_set)
 
         return train_set, test_set
 
     @classmethod
     def extract_seq_set(cls, raw_set, horizon):
-        assert isinstance(raw_set, SequenceSet) and horizon in [10, 20, 30, 50, 100]
+        assert isinstance(raw_set,
+                          SequenceSet) and horizon in [10, 20, 30, 50, 100]
         seq_set = SequenceSet(
-            features=[array[:, :40] for array in raw_set.data_dict["raw_data"]],
+            features=[
+                array[:, :40] for array in raw_set.data_dict["raw_data"]
+            ],
             targets=raw_set.data_dict[horizon],
             name=raw_set.name,
         )
         return seq_set
 
     @classmethod
-    def load_as_tframe_data(
-        cls,
-        data_dir,
-        auction=False,
-        norm_type="zscore",
-        setup=None,
-        file_slices=None,
-        **kwargs
-    ):
+    def load_as_tframe_data(cls,
+                            data_dir,
+                            auction=False,
+                            norm_type="zscore",
+                            setup=None,
+                            file_slices=None,
+                            **kwargs):
         # Confirm type of normalization
         nt_lower = norm_type.lower()
         # 'Zscore' for directory names and 'ZScore' for file names
@@ -155,19 +161,21 @@ class FI2010(DataAgent):
         elif nt_lower in ["3", "decpre"]:
             type_id, norm_type = 3, "DecPre"
         else:
-            raise KeyError("Unknown type of normalization `{}`".format(norm_type))
+            raise KeyError(
+                "Unknown type of normalization `{}`".format(norm_type))
         # Load directly if dataset exists
         data_path = cls._get_data_path(data_dir, auction, norm_type, setup)
         if os.path.exists(data_path):
             return SequenceSet.load(data_path)
         # If dataset does not exist, create from raw data
-        console.show_status(
-            "Creating `{}` from raw data ...".format(os.path.basename(data_path))
-        )
+        console.show_status("Creating `{}` from raw data ...".format(
+            os.path.basename(data_path)))
         # Load raw data
-        features, targets = cls._load_raw_data(
-            data_dir, auction, norm_type, type_id, file_slices=file_slices
-        )
+        features, targets = cls._load_raw_data(data_dir,
+                                               auction,
+                                               norm_type,
+                                               type_id,
+                                               file_slices=file_slices)
 
         # Wrap raw data into tframe Sequence set
         data_dict = {"raw_data": features}
@@ -190,8 +198,7 @@ class FI2010(DataAgent):
         len_per_day_per_stock = lob_set[cls.LEN_PER_DAY_PER_STOCK]
         assert len(len_per_day_per_stock) == lob_set.size
         for stock, (k, lob, move) in enumerate(
-            zip(k_list, lob_set.features, lob_set.targets)
-        ):
+                zip(k_list, lob_set.features, lob_set.targets)):
             lengths = len_per_day_per_stock[stock]
             L = sum(lengths[:k])
             if k != 0:
@@ -202,26 +209,27 @@ class FI2010(DataAgent):
                 second_targets.append(move[L:])
         # Wrap data sets and return
         first_properties = {
-            cls.LEN_PER_DAY_PER_STOCK: [
-                s[:k] for k, s in zip(k_list, len_per_day_per_stock) if k != 0
-            ]
+            cls.LEN_PER_DAY_PER_STOCK:
+            [s[:k] for k, s in zip(k_list, len_per_day_per_stock) if k != 0]
         }
-        first_set = SequenceSet(
-            first_features, first_targets, name=first_name, **first_properties
-        )
+        first_set = SequenceSet(first_features,
+                                first_targets,
+                                name=first_name,
+                                **first_properties)
         second_properties = {
             cls.LEN_PER_DAY_PER_STOCK: [
-                s[k:] for k, s in zip(k_list, len_per_day_per_stock) if k != len(s)
+                s[k:] for k, s in zip(k_list, len_per_day_per_stock)
+                if k != len(s)
             ]
         }
-        second_set = SequenceSet(
-            second_features, second_targets, name=second_name, **second_properties
-        )
+        second_set = SequenceSet(second_features,
+                                 second_targets,
+                                 name=second_name,
+                                 **second_properties)
 
         for seq_set in [first_set, second_set]:
             assert np.sum(seq_set.structure) == np.sum(
-                np.concatenate(seq_set[cls.LEN_PER_DAY_PER_STOCK])
-            )
+                np.concatenate(seq_set[cls.LEN_PER_DAY_PER_STOCK]))
 
         return first_set, second_set
 
@@ -267,7 +275,9 @@ class FI2010(DataAgent):
         for j, lobs in enumerate(lob_list):
             # Find cliff indices
             max_delta = 300 if auction else 200
-            indices = cls._get_cliff_indices(lobs, auction, max_delta=max_delta)
+            indices = cls._get_cliff_indices(lobs,
+                                             auction,
+                                             max_delta=max_delta)
             # Fill LOBs
             from_i = 0
             for stock in range(5):
@@ -285,15 +295,13 @@ class FI2010(DataAgent):
         }
         data_dict["raw_data"] = [np.concatenate(lb_list) for lb_list in LOBs]
         # Initiate a new seq_set
-        seq_set = SequenceSet(
-            data_dict=data_dict,
-            name="FI-2010-LOBs",
-            **{
-                cls.LEN_PER_DAY_PER_STOCK: cls._get_len_per_day_per_stock(
-                    data_dir, auction
-                )
-            }
-        )
+        seq_set = SequenceSet(data_dict=data_dict,
+                              name="FI-2010-LOBs",
+                              **{
+                                  cls.LEN_PER_DAY_PER_STOCK:
+                                  cls._get_len_per_day_per_stock(
+                                      data_dir, auction)
+                              })
         # Sanity check (394337)
         assert sum(seq_set.structure) == sum(cls.DAY_LENGTH[auction])
         # Save and return
@@ -330,7 +338,7 @@ class FI2010(DataAgent):
         # Initialize features
         features = lob_set.data_dict["raw_data"]
         # .. max_level
-        features = [array[:, : 4 * max_level] for array in features]
+        features = [array[:, :4 * max_level] for array in features]
         # .. check developer code
         if "use_log" in th.developer_code:
             for x in features:
@@ -339,7 +347,8 @@ class FI2010(DataAgent):
         # .. volume only
         if th.volume_only:
             # features = [array[:, 1::2] for array in features] # For original Data
-            features = [array[:, 0:20] for array in features]  # For crypto data
+            features = [array[:, 0:20]
+                        for array in features]  # For crypto data
         # Set features back
         lob_set.features = features
         # Initialize targets
@@ -373,9 +382,9 @@ class FI2010(DataAgent):
         indices = np.where(delta > max_delta)[0] + shift - 1
         if auction:
             indices = [
-                i
-                for i in indices
-                if np.abs(p[min(i + 100, len(p) - 1)] - p[i - 100]) > max_delta
+                i for i in indices
+                if np.abs(p[min(i + 100,
+                                len(p) - 1)] - p[i - 100]) > max_delta
             ]
         if len(indices) != 4:
             raise AssertionError
@@ -397,14 +406,20 @@ class FI2010(DataAgent):
             max_delta = 0.4 if auction else 0.1
             indices = cls._get_cliff_indices(lobs, auction, max_delta)
             indices = [-1] + indices + [len(lobs) - 1]
-            for i, L in enumerate([indices[j + 1] - indices[j] for j in range(5)]):
+            for i, L in enumerate(
+                [indices[j + 1] - indices[j] for j in range(5)]):
                 lengths[i].append(L)
         # Sanity check
         assert np.sum(lengths) == sum(cls.DAY_LENGTH[auction])
         return lengths
 
     @classmethod
-    def _load_raw_data(cls, data_dir, auction, norm_type, type_id, file_slices=None):
+    def _load_raw_data(cls,
+                       data_dir,
+                       auction,
+                       norm_type,
+                       type_id,
+                       file_slices=None):
         assert isinstance(auction, bool)
         if not isinstance(norm_type, str):
             norm_type = str(norm_type)
@@ -414,37 +429,34 @@ class FI2010(DataAgent):
             auction_dir_name = "No" + auction_dir_name
         # Get directory name for training and test set
         norm_dir_name = "{}.{}_{}".format(type_id, auction_dir_name, norm_type)
-        path = os.path.join(
-            data_dir, "BenchmarkDatasets", auction_dir_name, norm_dir_name
-        )
+        path = os.path.join(data_dir, "BenchmarkDatasets", auction_dir_name,
+                            norm_dir_name)
         training_set_path = os.path.join(
-            path, "{}_{}_Training".format(auction_dir_name, norm_type)
-        )
+            path, "{}_{}_Training".format(auction_dir_name, norm_type))
         test_set_path = os.path.join(
-            path, "{}_{}_Testing".format(auction_dir_name, norm_type)
-        )
+            path, "{}_{}_Testing".format(auction_dir_name, norm_type))
 
         # Check training and test path
-        if any(
-            [not os.path.exists(training_set_path), not os.path.exists(test_set_path)]
-        ):
+        if any([
+                not os.path.exists(training_set_path),
+                not os.path.exists(test_set_path)
+        ]):
             import zipfile
 
             zip_file_name = "BenchmarkDatasets.zip"
             zip_file_path = cls._check_raw_data(data_dir, zip_file_name)
             console.show_status(
                 "Extracting {} (this may need several minutes) ...".format(
-                    zip_file_name
-                )
-            )
+                    zip_file_name))
             zipfile.ZipFile(zip_file_path, "r").extractall(data_dir)
             console.show_status("{} extracted successfully.".format(data_dir))
-        assert all([os.path.exists(training_set_path), os.path.exists(test_set_path)])
+        assert all(
+            [os.path.exists(training_set_path),
+             os.path.exists(test_set_path)])
 
         # Read data and return
-        return cls._read_train_test(
-            training_set_path, test_set_path, auction, norm_type, file_slices
-        )
+        return cls._read_train_test(training_set_path, test_set_path, auction,
+                                    norm_type, file_slices)
 
     @classmethod
     def _get_data_file_path_list(cls, dir_name, training, auction, norm_type):
@@ -454,8 +466,8 @@ class FI2010(DataAgent):
         prefix = "Train" if training else "Test"
         file_path_list = [
             os.path.join(
-                dir_name, "{}_Dst_{}_{}_CF_{}.txt".format(prefix, auction, norm_type, i)
-            )
+                dir_name,
+                "{}_Dst_{}_{}_CF_{}.txt".format(prefix, auction, norm_type, i))
             for i in range(1, 10)
         ]
         # Make sure each file exists
@@ -465,13 +477,18 @@ class FI2010(DataAgent):
         return file_path_list
 
     @classmethod
-    def _read_train_test(
-        cls, train_dir, test_dir, auction, norm_type, file_slices=None
-    ):
+    def _read_train_test(cls,
+                         train_dir,
+                         test_dir,
+                         auction,
+                         norm_type,
+                         file_slices=None):
         """This method is better used for reading DecPre data for further restoring
     """
-        train_paths = cls._get_data_file_path_list(train_dir, True, auction, norm_type)
-        test_paths = cls._get_data_file_path_list(test_dir, False, auction, norm_type)
+        train_paths = cls._get_data_file_path_list(train_dir, True, auction,
+                                                   norm_type)
+        test_paths = cls._get_data_file_path_list(test_dir, False, auction,
+                                                  norm_type)
         # Read data from .txt files
         features, targets = [], {}
         horizons = [10, 20, 30, 50, 100]
@@ -490,9 +507,8 @@ class FI2010(DataAgent):
             features.append([])
             for h in horizons:
                 targets[h].append([])
-            console.show_status(
-                "Reading data from `{}` ...".format(os.path.basename(path))
-            )
+            console.show_status("Reading data from `{}` ...".format(
+                os.path.basename(path)))
             with open(path, "r") as f:
                 lines = f.readlines()
             # Sanity check
@@ -513,10 +529,10 @@ class FI2010(DataAgent):
             # Stack list
             features[-1] = np.stack(features[-1], axis=0)
             for k, h in enumerate(horizons):
-                targets[h][-1] = (
-                    np.array(np.stack(targets[h][-1], axis=0), dtype=np.int64) - 1
-                )
-            console.show_status("Successfully read {} event blocks".format(total))
+                targets[h][-1] = (np.array(np.stack(targets[h][-1], axis=0),
+                                           dtype=np.int64) - 1)
+            console.show_status(
+                "Successfully read {} event blocks".format(total))
         # Sanity check and return
         total = sum([len(x) for x in features])
         console.show_status("Totally {} event blocks read.".format(total))
@@ -545,7 +561,8 @@ class FI2010(DataAgent):
             lob_targets = np.concatenate(data_dict[h])
             zs_targets = np.concatenate(zscore_set.data_dict[h])
             if not np.equal(lob_targets, zs_targets).all():
-                raise AssertionError("Targets not equal when horizon = {}".format(h))
+                raise AssertionError(
+                    "Targets not equal when horizon = {}".format(h))
         console.show_info("Targets are all correct.")
 
     @classmethod
@@ -570,8 +587,8 @@ class FI2010(DataAgent):
         )
         assert isinstance(zscore_set, SequenceSet)
         zs_all = np.concatenate(
-            [array[:, :40] for array in zscore_set.data_dict["raw_data"]], axis=0
-        )
+            [array[:, :40] for array in zscore_set.data_dict["raw_data"]],
+            axis=0)
         # Load min-max data
         mm_set = cls.load_as_tframe_data(
             data_dir,
@@ -581,8 +598,7 @@ class FI2010(DataAgent):
             file_slices=(slice(8, 9), slice(8, 9)),
         )
         mm_all = np.concatenate(
-            [array[:, :40] for array in mm_set.data_dict["raw_data"]], axis=0
-        )
+            [array[:, :40] for array in mm_set.data_dict["raw_data"]], axis=0)
         # Generate lob -> zscore data for validation
         lob_all = np.concatenate(lob_list, axis=0)
         lob_zs_all = (lob_all - mu) / sigma
@@ -608,9 +624,7 @@ class FI2010(DataAgent):
             if zs_mm_err > 0.1:
                 raise AssertionError(
                     "In LOB[{}, {}] val_zs = {} while val_mm = {}".format(
-                        i, j, val_zs, val_mm
-                    )
-                )
+                        i, j, val_zs, val_mm))
             correct_val = val_mm
             if not P_errs:
                 correct_val = np.round(val_mm)
@@ -618,18 +632,14 @@ class FI2010(DataAgent):
                 if cor_mm_err > 1e-3:
                     raise AssertionError(
                         "In LOB[{}, {}] cor_val = {} while val_mm = {}".format(
-                            i, j, cor_mm_err, val_mm
-                        )
-                    )
+                            i, j, cor_mm_err, val_mm))
             # Correct value in lob_all
             lob_all[i, j] = correct_val
             bar.show(i)
         # Show status after correction
         console.show_status(
             "{} price errors and {} volume errors have been corrected".format(
-                P_errs, V_errs
-            )
-        )
+                P_errs, V_errs))
         new_lob_list = []
         for s in [len(array) for array in lob_list]:
             day_block, lob_all = np.split(lob_all, [s])
@@ -642,11 +652,11 @@ class FI2010(DataAgent):
     def _get_data_path(cls, data_dir, auction, norm_type=None, setup=None):
         assert isinstance(auction, bool)
         if all([norm_type is None, setup is None]):
-            file_name = "FI-2010-{}Auction-LOBs.tfds".format("" if auction else "No")
+            file_name = "FI-2010-{}Auction-LOBs.tfds".format(
+                "" if auction else "No")
         else:
             file_name = "FI-2010-{}Auction-{}-Setup{}.tfds".format(
-                "" if auction else "No", norm_type, setup
-            )
+                "" if auction else "No", norm_type, setup)
         return os.path.join(data_dir, file_name)
 
     # endregion : Private Methods
@@ -662,7 +672,8 @@ class FI2010(DataAgent):
     # region : RNN batch generator for Sequence Set
 
     @staticmethod
-    def rnn_batch_generator(data_set, batch_size, num_steps, is_training, round_len):
+    def rnn_batch_generator(data_set, batch_size, num_steps, is_training,
+                            round_len):
         """Generated epoch batches are guaranteed to cover all sequences"""
         assert isinstance(data_set, SequenceSet) and is_training
         L = int(sum(data_set.structure) / batch_size)
@@ -673,22 +684,25 @@ class FI2010(DataAgent):
         num_sequences = wise_man.apportion(data_set.structure, batch_size)
         # Generate feature list and target list
         features, targets = [], []
-        for num, x, y in zip(num_sequences, data_set.features, data_set.targets):
+        for num, x, y in zip(num_sequences, data_set.features,
+                             data_set.targets):
             # Find starts for each sequence to sample
             starts = wise_man.spread(len(x), num, L, rad)
             # Sanity check
             assert len(starts) == num
             # Put the sub-sequences into corresponding lists
             for s in starts:
-                features.append(x[s : s + L])
-                targets.append(y[s : s + L])
+                features.append(x[s:s + L])
+                targets.append(y[s:s + L])
         # Stack features and targets
         features, targets = np.stack(features), np.stack(targets)
         data_set = DataSet(features, targets, is_rnn_input=True)
         assert data_set.size == batch_size
         # Generate RNN batches using DataSet.gen_rnn_batches
         counter = 0
-        for batch in data_set.gen_rnn_batches(batch_size, num_steps, is_training=True):
+        for batch in data_set.gen_rnn_batches(batch_size,
+                                              num_steps,
+                                              is_training=True):
             yield batch
             counter += 1
 
@@ -696,8 +710,7 @@ class FI2010(DataAgent):
         if counter != round_len:
             raise AssertionError(
                 "!! counter = {} while round_len = {}. (batch_size = {}, num_steps={})"
-                "".format(counter, round_len, batch_size, num_steps)
-            )
+                "".format(counter, round_len, batch_size, num_steps))
 
     # endregion : RNN batch generator for Sequence Set
 
@@ -708,7 +721,8 @@ class FI2010(DataAgent):
         from tframe.trainers.trainer import Trainer
 
         # Sanity check
-        assert isinstance(trainer, Trainer) and isinstance(dataset, SequenceSet)
+        assert isinstance(trainer, Trainer) and isinstance(
+            dataset, SequenceSet)
         model = trainer.model
         assert isinstance(model, Classifier)
 
@@ -724,11 +738,9 @@ class FI2010(DataAgent):
         label_pred = np.concatenate(label_pred)
         table, F1 = FI2010._get_table_and_F1(label_pred, title="All Stocks, ")
 
-        content = "F1 Scores: {}".format(
-            ", ".join(
-                ["[{}] {:.2f}".format(i + 1, score) for i, score in enumerate(F1s)]
-            )
-        )
+        content = "F1 Scores: {}".format(", ".join([
+            "[{}] {:.2f}".format(i + 1, score) for i, score in enumerate(F1s)
+        ]))
         return content + table.content
 
     @staticmethod
@@ -739,22 +751,22 @@ class FI2010(DataAgent):
         else:
             model = entity
         # Sanity check
-        assert isinstance(model, Classifier) and isinstance(seq_set, SequenceSet)
+        assert isinstance(model, Classifier) and isinstance(
+            seq_set, SequenceSet)
         # Get table and F1 score for each stock
         label_pred = FI2010._get_label_pred(model, seq_set)
         ### PRODIGY AI HOKUS POKUS START
         label_pred[0] = label_pred[0].reshape((len(label_pred[0]), 1))
         label_pred[0] = np.round(label_pred[0])
-        seq_set.data_dict["targets"][0] = seq_set.data_dict["targets"][0].reshape(
-            (len(seq_set.data_dict["targets"][0]), 1)
-        )
+        seq_set.data_dict["targets"][0] = seq_set.data_dict["targets"][
+            0].reshape((len(seq_set.data_dict["targets"][0]), 1))
         label_pred[0] = np.concatenate(
-            (label_pred[0], seq_set.data_dict["targets"][0]), axis=1
-        )
+            (label_pred[0], seq_set.data_dict["targets"][0]), axis=1)
         ### PRODIGY AI HOKUS POKUS END
 
         for i, lp in enumerate(label_pred):
-            table, _ = FI2010._get_table_and_F1(lp, title="[{}] ".format(i + 1))
+            table, _ = FI2010._get_table_and_F1(lp,
+                                                title="[{}] ".format(i + 1))
             table.print_buffer()
             if is_training:
                 model.agent.take_notes(table.content)
@@ -787,9 +799,10 @@ class FI2010(DataAgent):
         assert isinstance(model, Classifier)
         # Get predictions and labels
         label_pred_tensor = model.key_metric.quantity_definition.quantities
-        label_pred = model.evaluate(
-            label_pred_tensor, dataset, batch_size=batch_size, verbose=True
-        )
+        label_pred = model.evaluate(label_pred_tensor,
+                                    dataset,
+                                    batch_size=batch_size,
+                                    verbose=True)
         console.show_status("Evaluation completed")
         table, F1 = cls._get_table_and_F1(label_pred)
         return table, F1
@@ -800,7 +813,9 @@ class FI2010(DataAgent):
         # assert isinstance(label_pred, np.ndarray) and label_pred.shape[-1] == 2
         # Initialize table
         movements = ["Upward", "Stationary", "Downward"]
-        header = ["Movement  ", "Accuracy %", "Precision %", "Recall %", "   F1 %"]
+        header = [
+            "Movement  ", "Accuracy %", "Precision %", "Recall %", "   F1 %"
+        ]
         widths = [len(h) for h in header]
         table = Table(*widths, tab=3, margin=1, buffered=True)
         table.specify_format(*["{:.2f}" for _ in header], align="lrrrr")
@@ -811,7 +826,8 @@ class FI2010(DataAgent):
         precisions, recalls, F1s = [], [], []
         x = label_pred
         for c_thingo, move in enumerate(movements):
-            col, row = x[x[:, 0] == c_thingo][:, 1], x[x[:, 1] == c_thingo][:, 0]
+            col, row = x[x[:, 0] == c_thingo][:, 1], x[x[:, 1] == c_thingo][:,
+                                                                            0]
             TP = len(col[col == c_thingo])
             FP, FN = len(row) - TP, len(col) - TP
             precision = TP / (TP + FP) * 100 if TP + FP > 0 else 0
@@ -842,8 +858,10 @@ class FI2010(DataAgent):
     def _read_10_days_dep(cls, train_dir, test_dir, auction, norm_type, setup):
         """Read train_1, test_1, ... test_9 in order."""
         assert setup == 2
-        train_paths = cls._get_data_file_path_list(train_dir, True, auction, norm_type)
-        test_paths = cls._get_data_file_path_list(test_dir, False, auction, norm_type)
+        train_paths = cls._get_data_file_path_list(train_dir, True, auction,
+                                                   norm_type)
+        test_paths = cls._get_data_file_path_list(test_dir, False, auction,
+                                                  norm_type)
         # Read data from .txt files
         features, targets = [], {}
         horizons = [10, 20, 30, 50, 100]
@@ -857,9 +875,8 @@ class FI2010(DataAgent):
             features.append([])
             for h in horizons:
                 targets[h].append([])
-            console.show_status(
-                "Reading data from `{}` ...".format(os.path.basename(path))
-            )
+            console.show_status("Reading data from `{}` ...".format(
+                os.path.basename(path)))
             with open(path, "r") as f:
                 lines = f.readlines()
             # Sanity check
@@ -880,10 +897,10 @@ class FI2010(DataAgent):
             # Stack list
             features[-1] = np.stack(features[-1], axis=0)
             for k, h in enumerate(horizons):
-                targets[h][-1] = (
-                    np.array(np.stack(targets[h][-1], axis=0), dtype=np.int64) - 1
-                )
-            console.show_status("Successfully read {} event blocks".format(total))
+                targets[h][-1] = (np.array(np.stack(targets[h][-1], axis=0),
+                                           dtype=np.int64) - 1)
+            console.show_status(
+                "Successfully read {} event blocks".format(total))
         # Sanity check and return
         total = sum([len(x) for x in features])
         console.show_status("Totally {} event blocks read.".format(total))
diff --git a/third_party_libraries/gam_rhn/tframe/data/sequences/nlp/text_data_agent.py b/third_party_libraries/gam_rhn/tframe/data/sequences/nlp/text_data_agent.py
index d7eb86e..3febc6f 100644
--- a/third_party_libraries/gam_rhn/tframe/data/sequences/nlp/text_data_agent.py
+++ b/third_party_libraries/gam_rhn/tframe/data/sequences/nlp/text_data_agent.py
@@ -5,7 +5,7 @@ from __future__ import print_function
 import collections
 import sys
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.data.base_classes import DataAgent
 
diff --git a/third_party_libraries/gam_rhn/tframe/data/sequences/pmnist.py b/third_party_libraries/gam_rhn/tframe/data/sequences/pmnist.py
index dfd06b2..9085ca5 100644
--- a/third_party_libraries/gam_rhn/tframe/data/sequences/pmnist.py
+++ b/third_party_libraries/gam_rhn/tframe/data/sequences/pmnist.py
@@ -6,7 +6,7 @@ import os
 import numpy as np
 import random
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import console
diff --git a/third_party_libraries/gam_rhn/tframe/data/sequences/reber.py b/third_party_libraries/gam_rhn/tframe/data/sequences/reber.py
index 37f283b..558ae49 100644
--- a/third_party_libraries/gam_rhn/tframe/data/sequences/reber.py
+++ b/third_party_libraries/gam_rhn/tframe/data/sequences/reber.py
@@ -8,7 +8,7 @@ import random
 from enum import Enum, unique
 from collections import OrderedDict
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import console
diff --git a/third_party_libraries/gam_rhn/tframe/deprecated/_monitor.py b/third_party_libraries/gam_rhn/tframe/deprecated/_monitor.py
index 072f44d..d3388cf 100644
--- a/third_party_libraries/gam_rhn/tframe/deprecated/_monitor.py
+++ b/third_party_libraries/gam_rhn/tframe/deprecated/_monitor.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 import tframe as tfr
 from tframe import pedia
diff --git a/third_party_libraries/gam_rhn/tframe/deprecated/ark.py b/third_party_libraries/gam_rhn/tframe/deprecated/ark.py
index f930f65..3b0b478 100644
--- a/third_party_libraries/gam_rhn/tframe/deprecated/ark.py
+++ b/third_party_libraries/gam_rhn/tframe/deprecated/ark.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import activations
 from tframe import checker
diff --git a/third_party_libraries/gam_rhn/tframe/deprecated/bamboo.py b/third_party_libraries/gam_rhn/tframe/deprecated/bamboo.py
index 9ee226f..df81981 100644
--- a/third_party_libraries/gam_rhn/tframe/deprecated/bamboo.py
+++ b/third_party_libraries/gam_rhn/tframe/deprecated/bamboo.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import pedia
 from tframe.core import with_graph
diff --git a/third_party_libraries/gam_rhn/tframe/deprecated/bresnet.py b/third_party_libraries/gam_rhn/tframe/deprecated/bresnet.py
index dac111e..148ea90 100644
--- a/third_party_libraries/gam_rhn/tframe/deprecated/bresnet.py
+++ b/third_party_libraries/gam_rhn/tframe/deprecated/bresnet.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import console
 from tframe import Predictor
diff --git a/third_party_libraries/gam_rhn/tframe/deprecated/vn.py b/third_party_libraries/gam_rhn/tframe/deprecated/vn.py
index 41fb0ab..b536bb3 100644
--- a/third_party_libraries/gam_rhn/tframe/deprecated/vn.py
+++ b/third_party_libraries/gam_rhn/tframe/deprecated/vn.py
@@ -4,7 +4,7 @@ from __future__ import print_function
 
 import collections
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.models.model import Model
 from tframe.nets.net import Net
diff --git a/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/mn_core.py b/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/mn_core.py
index d1fabef..a4adcd8 100644
--- a/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/mn_core.py
+++ b/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/mn_core.py
@@ -9,7 +9,9 @@ for _ in range(DIR_DEPTH + 1):
 from tframe import console, SaveMode
 from tframe.trainers import SmartTrainerHub
 from tframe import Classifier
+from tframe import monitor
 
+import mn_ad as ad
 import mn_du as du
 
 
@@ -61,6 +63,10 @@ th.evaluate_test_set = True
 
 
 def activate(export_false=False):
+  # Register activation filter
+  if th.export_activations:
+    monitor.register_activation_filter(ad.act_type_ii_filter)
+
   # Load data
   train_set, val_set, test_set = du.load_data(th.data_dir)
 
diff --git a/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/mn_mu.py b/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/mn_mu.py
index ab29462..788e663 100644
--- a/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/mn_mu.py
+++ b/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/mn_mu.py
@@ -19,7 +19,7 @@ def get_container(th, flatten=False):
     model.add(Flatten())
     # Register extractor and researcher
     model.register_extractor(mn_du.MNIST.connection_heat_map_extractor)
-    monitor.register_researcher(mn_du.MNIST.flatten_researcher)
+    monitor.register_grad_researcher(mn_du.MNIST.flatten_researcher)
   return model
 
 
diff --git a/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/t00_hw.py b/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/t00_hw.py
index b59f439..03dd8ca 100644
--- a/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/t00_hw.py
+++ b/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/t00_hw.py
@@ -1,6 +1,6 @@
 import mn_core as core
 import mn_mu as m
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from tframe import console
 from tframe.utils.misc import date_string
 from tframe.layers.advanced import Dense
diff --git a/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/t00_slhw.py b/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/t00_slhw.py
index 57ef0b6..6ec6baf 100644
--- a/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/t00_slhw.py
+++ b/third_party_libraries/gam_rhn/tframe/examples/00-MNIST/t00_slhw.py
@@ -1,6 +1,6 @@
 import mn_core as core
 import mn_mu as m
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from tframe import console
 from tframe.utils.misc import date_string
 from tframe.layers.advanced import Dense
diff --git a/third_party_libraries/gam_rhn/tframe/examples/01-CIFAR10/cf10_mu.py b/third_party_libraries/gam_rhn/tframe/examples/01-CIFAR10/cf10_mu.py
index 49868ae..aa04503 100644
--- a/third_party_libraries/gam_rhn/tframe/examples/01-CIFAR10/cf10_mu.py
+++ b/third_party_libraries/gam_rhn/tframe/examples/01-CIFAR10/cf10_mu.py
@@ -1,4 +1,4 @@
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import Classifier
 from tframe.layers import Input, Linear, Activation, Flatten
@@ -20,7 +20,7 @@ def get_container(th, flatten=False):
   assert isinstance(th, Config)
   model = Classifier(mark=th.mark)
   model.add(Input(sample_shape=th.input_shape))
-  if th.centralize_data: model.add(Normalize(mu=th.data_mean))
+  if th.centralize_data: model.add(Normalize(mu=th.data_mean, sigma=255.))
   if flatten: model.add(Flatten())
   return model
 
@@ -32,7 +32,7 @@ def finalize(th, model):
   # model.add(Dense(num_neurons=th.num_classes))
   model.add(Activation('softmax'))
   # Build model
-  model.build(metric=['loss', 'accuracy'], batch_metric='accuracy',
+  model.build(metric=['accuracy', 'loss'], batch_metric='accuracy',
               eval_metric='accuracy')
   return model
 
@@ -61,7 +61,7 @@ def multinput(th):
   model.add(Linear(output_dim=th.num_classes))
 
   # Build model
-  model.build(metric=['loss', 'accuracy'], batch_metric='accuracy',
+  model.build(metric=['accuracy', 'loss'], batch_metric='accuracy',
               eval_metric='accuracy')
 
   return model
diff --git a/third_party_libraries/gam_rhn/tframe/examples/01-CIFAR10/t01_hw.py b/third_party_libraries/gam_rhn/tframe/examples/01-CIFAR10/t01_hw.py
index 3c5fe1b..5f15b27 100644
--- a/third_party_libraries/gam_rhn/tframe/examples/01-CIFAR10/t01_hw.py
+++ b/third_party_libraries/gam_rhn/tframe/examples/01-CIFAR10/t01_hw.py
@@ -1,6 +1,6 @@
 import cf10_core as core
 import cf10_mu as m
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from tframe import console
 from tframe.utils.misc import date_string
 from tframe.layers.advanced import Dense
diff --git a/third_party_libraries/gam_rhn/tframe/examples/01-CIFAR10/t01_slhw.py b/third_party_libraries/gam_rhn/tframe/examples/01-CIFAR10/t01_slhw.py
index 7293235..215870c 100644
--- a/third_party_libraries/gam_rhn/tframe/examples/01-CIFAR10/t01_slhw.py
+++ b/third_party_libraries/gam_rhn/tframe/examples/01-CIFAR10/t01_slhw.py
@@ -1,6 +1,6 @@
 import cf10_core as core
 import cf10_mu as m
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from tframe import console
 from tframe.utils.misc import date_string
 from tframe.layers.advanced import Dense
diff --git a/third_party_libraries/gam_rhn/tframe/examples/view_notes.py b/third_party_libraries/gam_rhn/tframe/examples/view_notes.py
index 1e7ac4f..bdfa201 100644
--- a/third_party_libraries/gam_rhn/tframe/examples/view_notes.py
+++ b/third_party_libraries/gam_rhn/tframe/examples/view_notes.py
@@ -8,6 +8,7 @@ for _ in range(DIR_DEPTH + 1):
 from tframe.utils.summary_viewer.main_frame import SummaryViewer
 from tframe import local
 from tframe.utils.tensor_viewer.plugins import lottery
+from tframe.utils.tensor_viewer.plugins import activation_sparsity
 
 
 default_inactive_flags = (
@@ -58,7 +59,8 @@ while True:
       default_inactive_criteria=default_inactive_criteria,
       flags_to_ignore=flags_to_ignore,
     )
-    viewer.register_plugin(lottery.plugin)
+    # viewer.register_plugin(lottery.plugin)
+    viewer.register_plugin(activation_sparsity.plugin)
     viewer.show()
 
   except Exception as e:
diff --git a/third_party_libraries/gam_rhn/tframe/initializers.py b/third_party_libraries/gam_rhn/tframe/initializers.py
index 688d48c..1531989 100644
--- a/third_party_libraries/gam_rhn/tframe/initializers.py
+++ b/third_party_libraries/gam_rhn/tframe/initializers.py
@@ -5,7 +5,7 @@ from __future__ import print_function
 import six
 import numpy as np
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from tensorflow.python.ops import init_ops
 
 from tframe.utils import checker
diff --git a/third_party_libraries/gam_rhn/tframe/institute/tflab/template.py b/third_party_libraries/gam_rhn/tframe/institute/tflab/template.py
index e518a32..c57bf8f 100644
--- a/third_party_libraries/gam_rhn/tframe/institute/tflab/template.py
+++ b/third_party_libraries/gam_rhn/tframe/institute/tflab/template.py
@@ -1,5 +1,5 @@
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from tframe import console
 
 
diff --git a/third_party_libraries/gam_rhn/tframe/layers/advanced.py b/third_party_libraries/gam_rhn/tframe/layers/advanced.py
index e67115c..e182c16 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/advanced.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/advanced.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker, linker
 from tframe import hub, context, console
diff --git a/third_party_libraries/gam_rhn/tframe/layers/common.py b/third_party_libraries/gam_rhn/tframe/layers/common.py
index 1aaa0f6..69469a8 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/common.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/common.py
@@ -4,7 +4,8 @@ from __future__ import print_function
 
 import numpy as np
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+tf.disable_v2_behavior()
 
 import tframe as tfr
 from tframe.utils.arg_parser import Parser
@@ -21,271 +22,269 @@ from tframe import pedia
 
 
 class Activation(Layer):
-  """"""
-  def __init__(self, identifier, set_logits=False):
-    p = Parser.parse(identifier)
-    self._id = p.name
-    self.abbreviation = (p.name if isinstance(identifier, six.string_types)
-                         else identifier.__name__)
-    self.full_name = self.abbreviation
-    self._activation = activations.get(identifier)
-    self._set_logits = set_logits
-
-  @single_input
-  def _link(self, inputs, **kwargs):
-    """Group name of Activation layer is decided not in calling
+    """"""
+    def __init__(self, identifier, set_logits=False):
+        p = Parser.parse(identifier)
+        self._id = p.name
+        self.abbreviation = (p.name if isinstance(identifier, six.string_types)
+                             else identifier.__name__)
+        self.full_name = self.abbreviation
+        self._activation = activations.get(identifier)
+        self._set_logits = set_logits
+
+    @single_input
+    def _link(self, inputs, **kwargs):
+        """Group name of Activation layer is decided not in calling
        Function.__call__ but calling self._activation"""
-    if self._id == 'softmax' or self._set_logits:
-      tfr.context.set_logits_tensor(inputs)
-    outputs = self._activation(inputs)
-    return outputs
+        if self._id == 'softmax' or self._set_logits:
+            tfr.context.set_logits_tensor(inputs)
+        outputs = self._activation(inputs)
+        return outputs
 
-  @staticmethod
-  def ReLU():
-    return Activation('relu')
+    @staticmethod
+    def ReLU():
+        return Activation('relu')
 
-  @staticmethod
-  def LeakyReLU(leak):
-    assert isinstance(leak, float) and leak > 0
-    return Activation('lrelu:{}'.format(leak))
+    @staticmethod
+    def LeakyReLU(leak):
+        assert isinstance(leak, float) and leak > 0
+        return Activation('lrelu:{}'.format(leak))
 
 
 class Dropout(Layer):
-  """"""
-  abbreviation = 'dropout'
-  full_name = abbreviation
+    """"""
+    abbreviation = 'dropout'
+    full_name = abbreviation
 
-  def __init__(self, train_keep_prob=0.5):
-    # Initialize keep probability until while linking to put the
-    #   the placeholder in the right name scope
+    def __init__(self, train_keep_prob=0.5):
+        # Initialize keep probability until while linking to put the
+        #   the placeholder in the right name scope
 
-    # self._keep_prob = None
-    self.train_keep_prob = train_keep_prob
+        # self._keep_prob = None
+        self.train_keep_prob = train_keep_prob
 
-  @property
-  def structure_tail(self):
-    return '({:.2f})'.format(1 - self.train_keep_prob)
+    @property
+    def structure_tail(self):
+        return '({:.2f})'.format(1 - self.train_keep_prob)
 
-  @single_input
-  def _link(self, input_, **kwargs):
-    return tf.nn.dropout(
-      input_, tf.cond(tf.get_collection(pedia.is_training)[0],
-                      lambda: self.train_keep_prob, lambda: 1.0))
+    @single_input
+    def _link(self, input_, **kwargs):
+        return tf.nn.dropout(
+            input_,
+            tf.cond(
+                tf.get_collection(pedia.is_training)[0],
+                lambda: self.train_keep_prob, lambda: 1.0))
 
 
 class Linear(Layer):
-  """Linear transformation layer, also known as fully connected layer or
+    """Linear transformation layer, also known as fully connected layer or
      dense layer"""
-  is_nucleus = True
-
-  full_name = 'linear'
-  abbreviation = 'fc'
-
-  def __init__(self, output_dim,
-               force_real=False,
-               use_bias=True,
-               weight_initializer='xavier_normal',
-               bias_initializer='zeros',
-               weight_regularizer=None,
-               bias_regularizer=None,
-               **kwargs):
-    if not np.isscalar(output_dim):
-      raise TypeError('!! output_dim must be a scalar, not {}'.format(
-        type(output_dim)))
-
-    self._output_dim = output_dim
-    self._force_real = force_real
-    self._use_bias = use_bias
-
-    self._weight_initializer = initializers.get(weight_initializer)
-    self._bias_initializer = initializers.get(bias_initializer)
-    self._weight_regularizer = regularizers.get(weight_regularizer, **kwargs)
-    self._bias_regularizer = regularizers.get(bias_regularizer, **kwargs)
-
-    self.weights = None
-    self.biases = None
-
-    self.neuron_scale = [output_dim]
-
-  @single_input
-  def _link(self, input_, **kwargs):
-    assert isinstance(input_, tf.Tensor)
-
-    # If this layer has been linked once, variables should be reused
-    if self.weights is not None:
-      tf.get_variable_scope().reuse_variables()
-
-    # Get the shape and data type of input
-    input_shape = input_.get_shape().as_list()
-    dtype = input_.dtype
-
-    weight_shape = (input_shape[-1], self._output_dim)
-    bias_shape = (self._output_dim, )
-
-    # Use lambda to make getting variable easier
-    get_weight_variable = lambda name, fixed_zero=False: self._get_variable(
-      name, weight_shape, fixed_zero, self._weight_initializer,
-      self._weight_regularizer)
-    get_bias_variable = lambda name, fixed_zero=False: self._get_variable(
-      name, bias_shape, fixed_zero, self._bias_initializer,
-      self._bias_regularizer)
-
-    # Get variable
-    if dtype in [tf.complex64, tf.complex128]:
-      # Get complex weights and biases
-      self.weights = tf.complex(
-        get_weight_variable('weights_real'),
-        get_weight_variable('weights_imag', self._force_real),
-        name='weights')
-      if self._use_bias:
-        self.biases = tf.complex(
-          get_bias_variable('biases_real'),
-          get_bias_variable('biases_imag', self._force_real),
-          name='biases')
-    else:
-      # Get real weights and biases
-      self.weights = get_weight_variable('weights')
-      if self._use_bias:
-        self.biases = get_bias_variable('biases')
-
-    # Calculate output
-    output = tf.matmul(input_, self.weights)
-    if self._use_bias:
-      output += self.biases
-
-    # Monitor
-    # if hub.monitor_weight or hub.monitor_grad:
-    #   tfr.monitor.add_weight(self.weights)
-
-    self.output_tensor = output
-    return output
+    is_nucleus = True
+
+    full_name = 'linear'
+    abbreviation = 'fc'
+
+    def __init__(self,
+                 output_dim,
+                 force_real=False,
+                 use_bias=True,
+                 weight_initializer='xavier_normal',
+                 bias_initializer='zeros',
+                 weight_regularizer=None,
+                 bias_regularizer=None,
+                 **kwargs):
+        if not np.isscalar(output_dim):
+            raise TypeError('!! output_dim must be a scalar, not {}'.format(
+                type(output_dim)))
+
+        self._output_dim = output_dim
+        self._force_real = force_real
+        self._use_bias = use_bias
+
+        self._weight_initializer = initializers.get(weight_initializer)
+        self._bias_initializer = initializers.get(bias_initializer)
+        self._weight_regularizer = regularizers.get(weight_regularizer,
+                                                    **kwargs)
+        self._bias_regularizer = regularizers.get(bias_regularizer, **kwargs)
+
+        self.weights = None
+        self.biases = None
+
+        self.neuron_scale = [output_dim]
+
+    @single_input
+    def _link(self, input_, **kwargs):
+        assert isinstance(input_, tf.Tensor)
+
+        # If this layer has been linked once, variables should be reused
+        if self.weights is not None:
+            tf.get_variable_scope().reuse_variables()
+
+        # Get the shape and data type of input
+        input_shape = input_.get_shape().as_list()
+        dtype = input_.dtype
+
+        weight_shape = (input_shape[-1], self._output_dim)
+        bias_shape = (self._output_dim, )
+
+        # Use lambda to make getting variable easier
+        get_weight_variable = lambda name, fixed_zero=False: self._get_variable(
+            name, weight_shape, fixed_zero, self._weight_initializer, self.
+            _weight_regularizer)
+        get_bias_variable = lambda name, fixed_zero=False: self._get_variable(
+            name, bias_shape, fixed_zero, self._bias_initializer, self.
+            _bias_regularizer)
+
+        # Get variable
+        if dtype in [tf.complex64, tf.complex128]:
+            # Get complex weights and biases
+            self.weights = tf.complex(get_weight_variable('weights_real'),
+                                      get_weight_variable(
+                                          'weights_imag', self._force_real),
+                                      name='weights')
+            if self._use_bias:
+                self.biases = tf.complex(get_bias_variable('biases_real'),
+                                         get_bias_variable(
+                                             'biases_imag', self._force_real),
+                                         name='biases')
+        else:
+            # Get real weights and biases
+            self.weights = get_weight_variable('weights')
+            if self._use_bias:
+                self.biases = get_bias_variable('biases')
+
+        # Calculate output
+        output = tf.matmul(input_, self.weights)
+        if self._use_bias:
+            output += self.biases
+
+        # Monitor
+        # if hub.monitor_weight or hub.monitor_grad:
+        #   tfr.monitor.add_weight(self.weights)
+
+        self.output_tensor = output
+        return output
 
 
 class Rescale(Layer):
-  full_name = 'rescale'
-  abbreviation = 'rs'
+    full_name = 'rescale'
+    abbreviation = 'rs'
 
-  def __init__(self, from_scale, to_scale):
-    if not(isinstance(from_scale, list) or isinstance(from_scale, tuple)):
-      raise TypeError('from_scale must be a list or a tuple')
-    if not(isinstance(to_scale, list) or isinstance(to_scale, tuple)):
-      raise TypeError('to_scale must be a list or a tuple')
+    def __init__(self, from_scale, to_scale):
+        if not (isinstance(from_scale, list) or isinstance(from_scale, tuple)):
+            raise TypeError('from_scale must be a list or a tuple')
+        if not (isinstance(to_scale, list) or isinstance(to_scale, tuple)):
+            raise TypeError('to_scale must be a list or a tuple')
 
-    self._from_scale = from_scale
-    self._to_scale = to_scale
+        self._from_scale = from_scale
+        self._to_scale = to_scale
 
-  @single_input
-  def _link(self, input_, **kwargs):
-    from_, to_ = self._from_scale, self._to_scale
-    if from_[0] >= from_[1]:
-      raise ValueError('from_[0] should be less than from_[1]')
-    if to_[0] >= to_[1]:
-      raise ValueError('to_[0] should be less than to_[1]')
-    output = (input_ - from_[0]) / (from_[1] - from_[0])
-    output = output * (to_[1] - to_[0]) + to_[0]
+    @single_input
+    def _link(self, input_, **kwargs):
+        from_, to_ = self._from_scale, self._to_scale
+        if from_[0] >= from_[1]:
+            raise ValueError('from_[0] should be less than from_[1]')
+        if to_[0] >= to_[1]:
+            raise ValueError('to_[0] should be less than to_[1]')
+        output = (input_ - from_[0]) / (from_[1] - from_[0])
+        output = output * (to_[1] - to_[0]) + to_[0]
 
-    return output
+        return output
 
 
 class Onehot(Layer):
-  full_name = 'onehot'
-  abbreviation = 'onehot'
+    full_name = 'onehot'
+    abbreviation = 'onehot'
 
-  def __init__(self, depth):
-    assert isinstance(depth, int) and depth > 1
-    self._depth = depth
-    self.neuron_scale = [depth]
+    def __init__(self, depth):
+        assert isinstance(depth, int) and depth > 1
+        self._depth = depth
+        self.neuron_scale = [depth]
 
-
-  @single_input
-  def _link(self, indices):
-    assert isinstance(indices, tf.Tensor)
-    shape = indices.shape.as_list()
-    assert shape[-1] == 1
-    return tf.one_hot(tf.squeeze(indices, axis=-1), self._depth)
+    @single_input
+    def _link(self, indices):
+        assert isinstance(indices, tf.Tensor)
+        shape = indices.shape.as_list()
+        assert shape[-1] == 1
+        return tf.one_hot(tf.squeeze(indices, axis=-1), self._depth)
 
 
 class Reshape(Layer):
-
-  def __init__(self, shape=None):
-    """
+    def __init__(self, shape=None):
+        """
     Reshape layer. 
     :param shape: list or tuple. Shape of each example, not including 1st
                    dimension
     """
-    self.output_shape = shape
+        self.output_shape = shape
 
-  @single_input
-  def _link(self, input_, **kwargs):
-    name = 'flatten' if self.output_shape is None else 'reshape'
-    self.abbreviation = name
-    self.full_name = name
+    @single_input
+    def _link(self, input_, **kwargs):
+        name = 'flatten' if self.output_shape is None else 'reshape'
+        self.abbreviation = name
+        self.full_name = name
 
-    input_shape = input_.get_shape().as_list()
-    output_shape = ([-1, np.prod(input_shape[1:])]
-                    if self.output_shape is None
-                    else [-1] + list(self.output_shape))
+        input_shape = input_.get_shape().as_list()
+        output_shape = ([-1, np.prod(input_shape[1:])]
+                        if self.output_shape is None else [-1] +
+                        list(self.output_shape))
 
-    output = tf.reshape(input_, output_shape, name=name)
-    self.neuron_scale = get_scale(output)
-    return output
+        output = tf.reshape(input_, output_shape, name=name)
+        self.neuron_scale = get_scale(output)
+        return output
 
 
 class Input(Layer):
-
-  def __init__(
-      self,
-      sample_shape=None,
-      dtype=None,
-      name='Input',
-      group_shape=None):
-
-    # Check sample shape
-    if sample_shape is not None:
-      if not isinstance(sample_shape, (list, tuple)):
-        raise TypeError('sample_shape must be a list or a tuple')
-
-    # Initiate attributes
-    self.sample_shape = sample_shape
-    self.group_shape = None
-    self.dtype = hub.dtype if dtype is None else dtype
-    self.name = name
-    self.place_holder = None
-    self.rnn_single_step_input = None
-
-    self.set_group_shape(group_shape)
-
-
-  @property
-  def input_shape(self):
-    if self.sample_shape is None: return self.sample_shape
-    if self.group_shape is None: return [None] + list(self.sample_shape)
-    else: return list(self.group_shape) + list(self.sample_shape)
-
-
-  def set_group_shape(self, shape):
-    if shape is not None:
-      if not isinstance(shape, (tuple, list)):
-        raise TypeError('group_shape must be a list or a tuple')
-    self.group_shape = shape
-
-
-  def _link(self, *args, **kwargs):
-    assert self.place_holder is None
-    # This method is only accessible by Function.__call__ thus a None will
-    #   be given as input
-    assert len(args) == 0 and len(kwargs) == 0
-    input_ = tf.placeholder(
-      dtype=self.dtype, shape=self.input_shape, name=self.name)
-    # Update neuron scale
-    self.neuron_scale = get_scale(input_)
-    # Add input to default collection
-    tf.add_to_collection(pedia.default_feed_dict, input_)
-    # Return placeholder
-    self.place_holder = input_
-    self.rnn_single_step_input = tf.reshape(
-      input_, [-1] + list(self.sample_shape))
-    return input_
+    def __init__(self,
+                 sample_shape=None,
+                 dtype=None,
+                 name='Input',
+                 group_shape=None):
+
+        # Check sample shape
+        if sample_shape is not None:
+            if not isinstance(sample_shape, (list, tuple)):
+                raise TypeError('sample_shape must be a list or a tuple')
+
+        # Initiate attributes
+        self.sample_shape = sample_shape
+        self.group_shape = None
+        self.dtype = hub.dtype if dtype is None else dtype
+        self.name = name
+        self.place_holder = None
+        self.rnn_single_step_input = None
+
+        self.set_group_shape(group_shape)
+
+    @property
+    def input_shape(self):
+        if self.sample_shape is None: return self.sample_shape
+        if self.group_shape is None: return [None] + list(self.sample_shape)
+        else: return list(self.group_shape) + list(self.sample_shape)
+
+    def set_group_shape(self, shape):
+        if shape is not None:
+            if not isinstance(shape, (tuple, list)):
+                raise TypeError('group_shape must be a list or a tuple')
+        self.group_shape = shape
+
+    def _link(self, *args, **kwargs):
+        assert self.place_holder is None
+        # This method is only accessible by Function.__call__ thus a None will
+        #   be given as input
+        assert len(args) == 0 and len(kwargs) == 0
+        input_ = tf.placeholder(dtype=self.dtype,
+                                shape=self.input_shape,
+                                name=self.name)
+        # Update neuron scale
+        self.neuron_scale = get_scale(input_)
+        # Add input to default collection
+        tf.add_to_collection(pedia.default_feed_dict, input_)
+        # Return placeholder
+        self.place_holder = input_
+        self.rnn_single_step_input = tf.reshape(input_,
+                                                [-1] + list(self.sample_shape))
+        return input_
 
 
 Flatten = lambda: Reshape()
diff --git a/third_party_libraries/gam_rhn/tframe/layers/convolutional.py b/third_party_libraries/gam_rhn/tframe/layers/convolutional.py
index 1b6685a..af97085 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/convolutional.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/convolutional.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.core.function import Function
 from tframe.core.decorators import init_with_graph
diff --git a/third_party_libraries/gam_rhn/tframe/layers/embedding.py b/third_party_libraries/gam_rhn/tframe/layers/embedding.py
index 88562f3..789274c 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/embedding.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/embedding.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import initializers
 from tframe.layers.layer import Layer
diff --git a/third_party_libraries/gam_rhn/tframe/layers/highway.py b/third_party_libraries/gam_rhn/tframe/layers/highway.py
index 394fa95..27f0d24 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/highway.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/highway.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker, linker
 from tframe import hub, context
diff --git a/third_party_libraries/gam_rhn/tframe/layers/homogeneous.py b/third_party_libraries/gam_rhn/tframe/layers/homogeneous.py
index 6acae94..1bf544c 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/homogeneous.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/homogeneous.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.layers.layer import Layer
 from tframe.layers.layer import single_input
diff --git a/third_party_libraries/gam_rhn/tframe/layers/hyper/bilinear.py b/third_party_libraries/gam_rhn/tframe/layers/hyper/bilinear.py
index f76a726..55fe295 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/hyper/bilinear.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/hyper/bilinear.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import hub as th
diff --git a/third_party_libraries/gam_rhn/tframe/layers/hyper/dense.py b/third_party_libraries/gam_rhn/tframe/layers/hyper/dense.py
index ac419ba..c80aa19 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/hyper/dense.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/hyper/dense.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import hub, context, console
diff --git a/third_party_libraries/gam_rhn/tframe/layers/layer.py b/third_party_libraries/gam_rhn/tframe/layers/layer.py
index 8864413..ebebb0e 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/layer.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/layer.py
@@ -4,7 +4,7 @@ from __future__ import print_function
 
 from collections import OrderedDict
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.core.function import Function
 from tframe import activations, initializers, checker, linker
diff --git a/third_party_libraries/gam_rhn/tframe/layers/merge.py b/third_party_libraries/gam_rhn/tframe/layers/merge.py
index ea77275..e7b75e1 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/merge.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/merge.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import console
 from tframe.layers.layer import Layer, single_input
diff --git a/third_party_libraries/gam_rhn/tframe/layers/misc.py b/third_party_libraries/gam_rhn/tframe/layers/misc.py
index 9ae9187..21391cc 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/misc.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/misc.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker, linker
 from tframe import hub, context
diff --git a/third_party_libraries/gam_rhn/tframe/layers/normalization.py b/third_party_libraries/gam_rhn/tframe/layers/normalization.py
index b137070..5bdbcf0 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/normalization.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/normalization.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tensorflow.python.ops import init_ops
 
diff --git a/third_party_libraries/gam_rhn/tframe/layers/parametric_activation.py b/third_party_libraries/gam_rhn/tframe/layers/parametric_activation.py
index 9c4c727..05b90ee 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/parametric_activation.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/parametric_activation.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.layers.layer import Layer
 from tframe.layers.layer import single_input
diff --git a/third_party_libraries/gam_rhn/tframe/layers/pooling.py b/third_party_libraries/gam_rhn/tframe/layers/pooling.py
index 8be5a4d..0dc90d4 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/pooling.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/pooling.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.layers.layer import Layer
 from tframe.layers.layer import single_input
diff --git a/third_party_libraries/gam_rhn/tframe/layers/preprocess.py b/third_party_libraries/gam_rhn/tframe/layers/preprocess.py
index 12db7cb..bff877b 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/preprocess.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/preprocess.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import hub
 from tframe.layers.layer import Layer, single_input
diff --git a/third_party_libraries/gam_rhn/tframe/layers/slhw.py b/third_party_libraries/gam_rhn/tframe/layers/slhw.py
index b1c4120..42e4b30 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/slhw.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/slhw.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker, linker
 from tframe import hub, context
diff --git a/third_party_libraries/gam_rhn/tframe/layers/sparse/sparse_sog_n.py b/third_party_libraries/gam_rhn/tframe/layers/sparse/sparse_sog_n.py
index 1477613..8d71f96 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/sparse/sparse_sog_n.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/sparse/sparse_sog_n.py
@@ -2,8 +2,9 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
+from tframe import context
 from tframe import checker
 from tframe import hub as th
 from tframe.activations import sog
@@ -71,6 +72,9 @@ class SparseSOG(HyperBase):
       net_gate = self.dense_v2(self._num_neurons, 'seed', head)
     gate = sog(net_gate, self._group_size)
 
+    # Export gates if necessary
+    if th.export_gates: context.add_tensor_to_export('sog_gate', gate)
+
     # Apply gate
     y = tf.multiply(y_bar, gate, 'y')
     # ~
diff --git a/third_party_libraries/gam_rhn/tframe/layers/specific/lob_extraction.py b/third_party_libraries/gam_rhn/tframe/layers/specific/lob_extraction.py
index 9a6566f..5ed0e70 100644
--- a/third_party_libraries/gam_rhn/tframe/layers/specific/lob_extraction.py
+++ b/third_party_libraries/gam_rhn/tframe/layers/specific/lob_extraction.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker, context
 from tframe import hub as th
diff --git a/third_party_libraries/gam_rhn/tframe/losses.py b/third_party_libraries/gam_rhn/tframe/losses.py
index b3b340a..7e773da 100644
--- a/third_party_libraries/gam_rhn/tframe/losses.py
+++ b/third_party_libraries/gam_rhn/tframe/losses.py
@@ -4,7 +4,7 @@ from __future__ import print_function
 
 import six
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker, context, linker
 from tframe.core.quantity import Quantity
diff --git a/third_party_libraries/gam_rhn/tframe/metrics.py b/third_party_libraries/gam_rhn/tframe/metrics.py
index 7f8f16d..b133430 100644
--- a/third_party_libraries/gam_rhn/tframe/metrics.py
+++ b/third_party_libraries/gam_rhn/tframe/metrics.py
@@ -4,7 +4,7 @@ from __future__ import print_function
 
 import six
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 import tframe as tfr
 from tframe.core.quantity import Quantity
 
diff --git a/third_party_libraries/gam_rhn/tframe/models/feedforward.py b/third_party_libraries/gam_rhn/tframe/models/feedforward.py
index 0ec9ee4..c60f028 100644
--- a/third_party_libraries/gam_rhn/tframe/models/feedforward.py
+++ b/third_party_libraries/gam_rhn/tframe/models/feedforward.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 from collections import OrderedDict
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.core.decorators import with_graph
 
diff --git a/third_party_libraries/gam_rhn/tframe/models/model.py b/third_party_libraries/gam_rhn/tframe/models/model.py
index a040ddb..2b16f98 100644
--- a/third_party_libraries/gam_rhn/tframe/models/model.py
+++ b/third_party_libraries/gam_rhn/tframe/models/model.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 import numpy as np
 from collections import OrderedDict
 
diff --git a/third_party_libraries/gam_rhn/tframe/models/recurrent.py b/third_party_libraries/gam_rhn/tframe/models/recurrent.py
index f40f9a3..2d188db 100644
--- a/third_party_libraries/gam_rhn/tframe/models/recurrent.py
+++ b/third_party_libraries/gam_rhn/tframe/models/recurrent.py
@@ -4,7 +4,7 @@ from __future__ import print_function
 
 from collections import OrderedDict
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import context, hub
 from tframe import checker, console
diff --git a/third_party_libraries/gam_rhn/tframe/models/rl/td_player.py b/third_party_libraries/gam_rhn/tframe/models/rl/td_player.py
index 45041d2..4ff8e68 100644
--- a/third_party_libraries/gam_rhn/tframe/models/rl/td_player.py
+++ b/third_party_libraries/gam_rhn/tframe/models/rl/td_player.py
@@ -4,7 +4,7 @@ from __future__ import print_function
 
 import numpy as np
 import time
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.models.feedforward import Feedforward
 
diff --git a/third_party_libraries/gam_rhn/tframe/models/sl/classifier.py b/third_party_libraries/gam_rhn/tframe/models/sl/classifier.py
index c3217d1..5f1b307 100644
--- a/third_party_libraries/gam_rhn/tframe/models/sl/classifier.py
+++ b/third_party_libraries/gam_rhn/tframe/models/sl/classifier.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import hub
 from tframe import pedia
diff --git a/third_party_libraries/gam_rhn/tframe/models/sl/predictor.py b/third_party_libraries/gam_rhn/tframe/models/sl/predictor.py
index 201703b..4a8b6e1 100644
--- a/third_party_libraries/gam_rhn/tframe/models/sl/predictor.py
+++ b/third_party_libraries/gam_rhn/tframe/models/sl/predictor.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.models.model import Model
 from tframe.models.feedforward import Feedforward
diff --git a/third_party_libraries/gam_rhn/tframe/models/usl/gan.py b/third_party_libraries/gam_rhn/tframe/models/usl/gan.py
index 6c674bb..71c1563 100644
--- a/third_party_libraries/gam_rhn/tframe/models/usl/gan.py
+++ b/third_party_libraries/gam_rhn/tframe/models/usl/gan.py
@@ -4,7 +4,7 @@ from __future__ import print_function
 
 import numpy as np
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.models.model import Model
 from tframe.nets.net import Net
diff --git a/third_party_libraries/gam_rhn/tframe/models/usl/vae.py b/third_party_libraries/gam_rhn/tframe/models/usl/vae.py
index 9e2d708..3ed9c58 100644
--- a/third_party_libraries/gam_rhn/tframe/models/usl/vae.py
+++ b/third_party_libraries/gam_rhn/tframe/models/usl/vae.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.models.model import Model
 from tframe.nets.net import Net
diff --git a/third_party_libraries/gam_rhn/tframe/nets/hyper/fsrnn.py b/third_party_libraries/gam_rhn/tframe/nets/hyper/fsrnn.py
index 271bded..4a89834 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/hyper/fsrnn.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/hyper/fsrnn.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import hub as th
diff --git a/third_party_libraries/gam_rhn/tframe/nets/hyper/gam_rhn.py b/third_party_libraries/gam_rhn/tframe/nets/hyper/gam_rhn.py
index 39c0164..1846f78 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/hyper/gam_rhn.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/hyper/gam_rhn.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import hub as th
diff --git a/third_party_libraries/gam_rhn/tframe/nets/hyper/gdu_h.py b/third_party_libraries/gam_rhn/tframe/nets/hyper/gdu_h.py
index 42cfa90..6ce9df2 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/hyper/gdu_h.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/hyper/gdu_h.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import linker
diff --git a/third_party_libraries/gam_rhn/tframe/nets/hyper/hyper_gdu.py b/third_party_libraries/gam_rhn/tframe/nets/hyper/hyper_gdu.py
index dddb848..becf2c6 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/hyper/hyper_gdu.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/hyper/hyper_gdu.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import linker
diff --git a/third_party_libraries/gam_rhn/tframe/nets/hyper/mgdu.py b/third_party_libraries/gam_rhn/tframe/nets/hyper/mgdu.py
index 2a74738..04ade47 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/hyper/mgdu.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/hyper/mgdu.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import linker
diff --git a/third_party_libraries/gam_rhn/tframe/nets/hyper/rhn.py b/third_party_libraries/gam_rhn/tframe/nets/hyper/rhn.py
index f382b3a..f55be76 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/hyper/rhn.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/hyper/rhn.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import hub as th
diff --git a/third_party_libraries/gam_rhn/tframe/nets/net.py b/third_party_libraries/gam_rhn/tframe/nets/net.py
index 14da095..9d5d3ac 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/net.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/net.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import console
 from tframe.core import Function
diff --git a/third_party_libraries/gam_rhn/tframe/nets/resnet.py b/third_party_libraries/gam_rhn/tframe/nets/resnet.py
index a82ad69..1b836b7 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/resnet.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/resnet.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 import tframe.nets.net as tfr_net
 
diff --git a/third_party_libraries/gam_rhn/tframe/nets/rnet.py b/third_party_libraries/gam_rhn/tframe/nets/rnet.py
index 822a4d8..73387cc 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/rnet.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/rnet.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from collections import OrderedDict
 
diff --git a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/amu.py b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/amu.py
index 0339764..e851bbd 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/amu.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/amu.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 import numpy as np
 
 from tframe import checker
diff --git a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/ark.py b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/ark.py
index 5573380..6c1a82e 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/ark.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/ark.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import linker
diff --git a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/cell_base.py b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/cell_base.py
index 2183808..4ae7bde 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/cell_base.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/cell_base.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import hub
 from tframe import pedia
diff --git a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/conveyor.py b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/conveyor.py
index 46a0bd7..fcf5ac2 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/conveyor.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/conveyor.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker, hub
 from tframe.nets.rnn_cells.cell_base import CellBase
diff --git a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/cwrnn.py b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/cwrnn.py
index 694a8a2..ef76208 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/cwrnn.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/cwrnn.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from collections import OrderedDict
 
diff --git a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/gdu.py b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/gdu.py
index d73e5c5..f1beed6 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/gdu.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/gdu.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import linker
diff --git a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/gru.py b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/gru.py
index 903fff8..b929071 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/gru.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/gru.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe.nets.rnn_cells.cell_base import CellBase
diff --git a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/lstms.py b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/lstms.py
index c1baf99..8f7f0a4 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/lstms.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/lstms.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import activations
diff --git a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/mogrifier.py b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/mogrifier.py
index cc2314f..55b88b7 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/mogrifier.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/mogrifier.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import hub as th
 from tframe import checker
diff --git a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/on_lstm.py b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/on_lstm.py
index c3f8c99..f76d9f1 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/on_lstm.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/on_lstm.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import activations
diff --git a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/pru.py b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/pru.py
index d7c18c1..2127a66 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/pru.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/pru.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.nets.rnn_cells.srn import BasicRNNCell
 
diff --git a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/srn.py b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/srn.py
index 9a12028..afa4501 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/srn.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/srn.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import activations
 from tframe import checker
diff --git a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/tuner.py b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/tuner.py
index 0b5e0c8..fce654a 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/tuner.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/rnn_cells/tuner.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe.nets.rnn_cells.cell_base import CellBase
diff --git a/third_party_libraries/gam_rhn/tframe/nets/tapes/rtu.py b/third_party_libraries/gam_rhn/tframe/nets/tapes/rtu.py
index 0bca0f9..12c8ddc 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/tapes/rtu.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/tapes/rtu.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe.operators.apis.attention import AttentionBase
diff --git a/third_party_libraries/gam_rhn/tframe/nets/tapes/tape.py b/third_party_libraries/gam_rhn/tframe/nets/tapes/tape.py
index 9040835..b85aa5f 100644
--- a/third_party_libraries/gam_rhn/tframe/nets/tapes/tape.py
+++ b/third_party_libraries/gam_rhn/tframe/nets/tapes/tape.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe.nets.rnn_cells.cell_base import CellBase
diff --git a/third_party_libraries/gam_rhn/tframe/operators/apis/attention.py b/third_party_libraries/gam_rhn/tframe/operators/apis/attention.py
index 149a0af..098efdf 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/apis/attention.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/apis/attention.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import hub as th
 from tframe import checker
diff --git a/third_party_libraries/gam_rhn/tframe/operators/apis/distributor.py b/third_party_libraries/gam_rhn/tframe/operators/apis/distributor.py
index 8821ca6..4ffa64d 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/apis/distributor.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/apis/distributor.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe.utils import linker
diff --git a/third_party_libraries/gam_rhn/tframe/operators/apis/dynamic_weights.py b/third_party_libraries/gam_rhn/tframe/operators/apis/dynamic_weights.py
index d43ee78..f9dab08 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/apis/dynamic_weights.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/apis/dynamic_weights.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.operators.apis.generic_neurons import GenericNeurons as gn
 
diff --git a/third_party_libraries/gam_rhn/tframe/operators/apis/gam.py b/third_party_libraries/gam_rhn/tframe/operators/apis/gam.py
index 90cc707..30cb861 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/apis/gam.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/apis/gam.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import hub
diff --git a/third_party_libraries/gam_rhn/tframe/operators/apis/gam_v2.py b/third_party_libraries/gam_rhn/tframe/operators/apis/gam_v2.py
index e6ba6e1..31137af 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/apis/gam_v2.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/apis/gam_v2.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import hub as th
diff --git a/third_party_libraries/gam_rhn/tframe/operators/apis/generic_neurons.py b/third_party_libraries/gam_rhn/tframe/operators/apis/generic_neurons.py
index 88cbf1c..3132b89 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/apis/generic_neurons.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/apis/generic_neurons.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import hub
 from tframe import linker
diff --git a/third_party_libraries/gam_rhn/tframe/operators/apis/groups.py b/third_party_libraries/gam_rhn/tframe/operators/apis/groups.py
index f10ee06..2703928 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/apis/groups.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/apis/groups.py
@@ -4,7 +4,7 @@ from __future__ import print_function
 
 import re
 import inspect
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import linker
 
diff --git a/third_party_libraries/gam_rhn/tframe/operators/apis/hard_driver.py b/third_party_libraries/gam_rhn/tframe/operators/apis/hard_driver.py
index 797b280..5267528 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/apis/hard_driver.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/apis/hard_driver.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import hub
diff --git a/third_party_libraries/gam_rhn/tframe/operators/apis/hyper_kernel.py b/third_party_libraries/gam_rhn/tframe/operators/apis/hyper_kernel.py
index 6f5b452..cafd4e9 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/apis/hyper_kernel.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/apis/hyper_kernel.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import hub
 
diff --git a/third_party_libraries/gam_rhn/tframe/operators/apis/mask_1d.py b/third_party_libraries/gam_rhn/tframe/operators/apis/mask_1d.py
index b84c3df..39fc59d 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/apis/mask_1d.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/apis/mask_1d.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe.nets.rnn_cells.cell_base import CellBase
diff --git a/third_party_libraries/gam_rhn/tframe/operators/apis/mixer.py b/third_party_libraries/gam_rhn/tframe/operators/apis/mixer.py
index feb9bcb..688f9d0 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/apis/mixer.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/apis/mixer.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import re
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from .neurobase import NeuroBase
 
diff --git a/third_party_libraries/gam_rhn/tframe/operators/apis/neurobase.py b/third_party_libraries/gam_rhn/tframe/operators/apis/neurobase.py
index 2b5e146..0f1368c 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/apis/neurobase.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/apis/neurobase.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import activations
 from tframe import checker
diff --git a/third_party_libraries/gam_rhn/tframe/operators/bias_kernel.py b/third_party_libraries/gam_rhn/tframe/operators/bias_kernel.py
index 5632d8d..11e2895 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/bias_kernel.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/bias_kernel.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import hub
 from tframe import linker
diff --git a/third_party_libraries/gam_rhn/tframe/operators/kernel_base.py b/third_party_libraries/gam_rhn/tframe/operators/kernel_base.py
index 5d80686..e4f4c91 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/kernel_base.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/kernel_base.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import inspect
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import context
 from tframe import checker
diff --git a/third_party_libraries/gam_rhn/tframe/operators/masked_weights.py b/third_party_libraries/gam_rhn/tframe/operators/masked_weights.py
index 3faddcc..1908cb3 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/masked_weights.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/masked_weights.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 class MaskedWeights(object):
diff --git a/third_party_libraries/gam_rhn/tframe/operators/neurons.py b/third_party_libraries/gam_rhn/tframe/operators/neurons.py
index efc1027..3019cc5 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/neurons.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/neurons.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import hub
diff --git a/third_party_libraries/gam_rhn/tframe/operators/prune/etches/etch_kernel.py b/third_party_libraries/gam_rhn/tframe/operators/prune/etches/etch_kernel.py
index 8545cc7..064eb67 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/prune/etches/etch_kernel.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/prune/etches/etch_kernel.py
@@ -4,7 +4,7 @@ from __future__ import print_function
 
 import re
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import hub
diff --git a/third_party_libraries/gam_rhn/tframe/operators/prune/etches/lottery.py b/third_party_libraries/gam_rhn/tframe/operators/prune/etches/lottery.py
index 8ce56a5..fc08f7c 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/prune/etches/lottery.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/prune/etches/lottery.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import hub
 from tframe import monitor
diff --git a/third_party_libraries/gam_rhn/tframe/operators/psi_kernel.py b/third_party_libraries/gam_rhn/tframe/operators/psi_kernel.py
index 2cfaecd..b0d111d 100644
--- a/third_party_libraries/gam_rhn/tframe/operators/psi_kernel.py
+++ b/third_party_libraries/gam_rhn/tframe/operators/psi_kernel.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import context
diff --git a/third_party_libraries/gam_rhn/tframe/optimizers/clip_opt.py b/third_party_libraries/gam_rhn/tframe/optimizers/clip_opt.py
index bd4904d..8ab2b9b 100644
--- a/third_party_libraries/gam_rhn/tframe/optimizers/clip_opt.py
+++ b/third_party_libraries/gam_rhn/tframe/optimizers/clip_opt.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from tframe import checker
 from tframe import hub
 
diff --git a/third_party_libraries/gam_rhn/tframe/optimizers/node_register.py b/third_party_libraries/gam_rhn/tframe/optimizers/node_register.py
index a1401e5..3cb912a 100644
--- a/third_party_libraries/gam_rhn/tframe/optimizers/node_register.py
+++ b/third_party_libraries/gam_rhn/tframe/optimizers/node_register.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.nets.net import Net
 from tframe.nets.rnet import RNet
diff --git a/third_party_libraries/gam_rhn/tframe/optimizers/rtrl_opt.py b/third_party_libraries/gam_rhn/tframe/optimizers/rtrl_opt.py
index c26d670..9eeb6e0 100644
--- a/third_party_libraries/gam_rhn/tframe/optimizers/rtrl_opt.py
+++ b/third_party_libraries/gam_rhn/tframe/optimizers/rtrl_opt.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.models.recurrent import Recurrent
 from tframe import hub as th
diff --git a/third_party_libraries/gam_rhn/tframe/regularizers.py b/third_party_libraries/gam_rhn/tframe/regularizers.py
index f9a838a..cee5dd4 100644
--- a/third_party_libraries/gam_rhn/tframe/regularizers.py
+++ b/third_party_libraries/gam_rhn/tframe/regularizers.py
@@ -4,7 +4,7 @@ from __future__ import print_function
 
 import six
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.utils.arg_parser import Parser
 
diff --git a/third_party_libraries/gam_rhn/tframe/trainers/metric_slot.py b/third_party_libraries/gam_rhn/tframe/trainers/metric_slot.py
index edba183..20e67d5 100644
--- a/third_party_libraries/gam_rhn/tframe/trainers/metric_slot.py
+++ b/third_party_libraries/gam_rhn/tframe/trainers/metric_slot.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import console
 from tframe.core import TensorSlot, VariableSlot, SummarySlot
diff --git a/third_party_libraries/gam_rhn/tframe/trainers/metrics_manager.py b/third_party_libraries/gam_rhn/tframe/trainers/metrics_manager.py
index a4826b1..0bc2b6b 100644
--- a/third_party_libraries/gam_rhn/tframe/trainers/metrics_manager.py
+++ b/third_party_libraries/gam_rhn/tframe/trainers/metrics_manager.py
@@ -5,7 +5,7 @@ from __future__ import print_function
 from collections import OrderedDict
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe import checker
 from tframe import console, context
diff --git a/third_party_libraries/gam_rhn/tframe/trainers/trainer.py b/third_party_libraries/gam_rhn/tframe/trainers/trainer.py
index 72b14ed..2f2c17f 100644
--- a/third_party_libraries/gam_rhn/tframe/trainers/trainer.py
+++ b/third_party_libraries/gam_rhn/tframe/trainers/trainer.py
@@ -6,7 +6,7 @@ import time
 import numpy as np
 from collections import OrderedDict
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 import tframe as tfr
 
 from tframe import checker
diff --git a/third_party_libraries/gam_rhn/tframe/utils/checker.py b/third_party_libraries/gam_rhn/tframe/utils/checker.py
index 7228285..724590f 100644
--- a/third_party_libraries/gam_rhn/tframe/utils/checker.py
+++ b/third_party_libraries/gam_rhn/tframe/utils/checker.py
@@ -1,4 +1,4 @@
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 import numpy as np
 
 from tframe.utils import misc
diff --git a/third_party_libraries/gam_rhn/tframe/utils/console.py b/third_party_libraries/gam_rhn/tframe/utils/console.py
index dc250a8..0f3ccd0 100644
--- a/third_party_libraries/gam_rhn/tframe/utils/console.py
+++ b/third_party_libraries/gam_rhn/tframe/utils/console.py
@@ -8,7 +8,7 @@ import time
 import os
 
 from sys import stdout
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tframe.utils import misc
 
diff --git a/third_party_libraries/gam_rhn/tframe/utils/janitor.py b/third_party_libraries/gam_rhn/tframe/utils/janitor.py
index a8d33c7..8f7e6f3 100644
--- a/third_party_libraries/gam_rhn/tframe/utils/janitor.py
+++ b/third_party_libraries/gam_rhn/tframe/utils/janitor.py
@@ -5,6 +5,16 @@ from __future__ import print_function
 import numpy as np
 
 
+def wrap(obj, obj_type=None, wrap_as=list):
+  """Wrap obj into list."""
+  assert wrap_as in (list, tuple)
+  if not isinstance(obj, wrap_as): obj = wrap_as([obj])
+  if obj_type is not None:
+    from tframe import checker
+    obj = checker.check_type_v2(obj, obj_type)
+  return obj
+
+
 def recover_seq_set_outputs(outputs, seq_set):
   """Outputs of tframe batch evaluation are messed up.
      This method will help.
diff --git a/third_party_libraries/gam_rhn/tframe/utils/linker.py b/third_party_libraries/gam_rhn/tframe/utils/linker.py
index 890233e..e4925c2 100644
--- a/third_party_libraries/gam_rhn/tframe/utils/linker.py
+++ b/third_party_libraries/gam_rhn/tframe/utils/linker.py
@@ -4,7 +4,7 @@ from __future__ import print_function
 
 import numpy as np
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 import tframe.activations as activations
 import tframe.initializers as initializers
 import tframe.regularizers as regularizers
diff --git a/third_party_libraries/gam_rhn/tframe/utils/local.py b/third_party_libraries/gam_rhn/tframe/utils/local.py
index e5494f7..8aaa75f 100644
--- a/third_party_libraries/gam_rhn/tframe/utils/local.py
+++ b/third_party_libraries/gam_rhn/tframe/utils/local.py
@@ -6,7 +6,7 @@ import os
 import re
 import six
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 import tframe as tfr
 from . import console
diff --git a/third_party_libraries/gam_rhn/tframe/utils/misc.py b/third_party_libraries/gam_rhn/tframe/utils/misc.py
index 1f8350a..a6eada2 100644
--- a/third_party_libraries/gam_rhn/tframe/utils/misc.py
+++ b/third_party_libraries/gam_rhn/tframe/utils/misc.py
@@ -7,7 +7,7 @@ import inspect
 import datetime
 import math
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 def ordinal(n):
diff --git a/third_party_libraries/gam_rhn/tframe/utils/monitor.py b/third_party_libraries/gam_rhn/tframe/utils/monitor.py
index 627a5e2..88308b5 100644
--- a/third_party_libraries/gam_rhn/tframe/utils/monitor.py
+++ b/third_party_libraries/gam_rhn/tframe/utils/monitor.py
@@ -5,7 +5,7 @@ from __future__ import print_function
 import numpy as np
 import collections
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 import tframe as tfr
 
diff --git a/third_party_libraries/gam_rhn/tframe/utils/stark.py b/third_party_libraries/gam_rhn/tframe/utils/stark.py
index 35985d0..b77ff06 100644
--- a/third_party_libraries/gam_rhn/tframe/utils/stark.py
+++ b/third_party_libraries/gam_rhn/tframe/utils/stark.py
@@ -3,7 +3,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 import tframe as tfr
 
diff --git a/third_party_libraries/gam_rhn/tframe/utils/tensor_tools.py b/third_party_libraries/gam_rhn/tframe/utils/tensor_tools.py
index 157870c..e174a1d 100644
--- a/third_party_libraries/gam_rhn/tframe/utils/tensor_tools.py
+++ b/third_party_libraries/gam_rhn/tframe/utils/tensor_tools.py
@@ -2,7 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 def extract_last_wrapper(op):
diff --git a/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/main_frame.py b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/main_frame.py
index 1c1de02..354eb63 100644
--- a/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/main_frame.py
+++ b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/main_frame.py
@@ -9,6 +9,7 @@ try:
   from PIL import Image as Image_
   from PIL import ImageTk
 
+  from tframe.utils import janitor
   from tframe.utils.note import Note
   from tframe.utils.tensor_viewer import key_events
   from tframe.utils.tensor_viewer.context import Context
@@ -44,7 +45,7 @@ class TensorViewer(Viewer):
     self._global_refresh()
 
     # Set plugin (beta) (This line should be put before set_note)
-    self._plugins = kwargs.get('plugins', [])
+    self._plugins = janitor.wrap(kwargs.get('plugins', []))
 
     # If note or note_path is provided, try to load it
     if note is not None or note_path is not None:
diff --git a/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugin.py b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugin.py
index 7828e00..a83eb0e 100644
--- a/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugin.py
+++ b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugin.py
@@ -2,6 +2,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import inspect
 from collections import OrderedDict
 
 
@@ -24,3 +25,27 @@ class VariableWithView(object):
     self._view = view
 
   def display(self, vv): self._view(vv, self._value_list)
+
+
+def recursively_modify(method, v_dict, level=0, verbose=True):
+  """This method recursively modifies v_dict with a provided 'method'.
+     'method' accepts keys and values(list of numpy arrays) and returns
+     modified values (which can be a tframe.VariableViewer).
+     Sometimes method should contain logic to determine whether the input values
+     should be modified.
+  """
+  # Sanity check
+  assert callable(method) and isinstance(v_dict, dict)
+  assert inspect.getfullargspec(method).args == ['key', 'value']
+  if len(v_dict) == 0: return
+
+  # If values in v_dict are dictionaries,  recursively modify each of them
+  if isinstance(list(v_dict.values())[0], dict):
+    for e_key, e_dict in v_dict.items():
+      assert isinstance(e_dict, dict)
+      if verbose: print('*> modifying dict {} ...'.format(e_key))
+      recursively_modify(method, e_dict, level=level+1, verbose=verbose)
+    return
+
+  # At this point, values in v_dict must be lists of numpy arrays
+  for key in v_dict.keys(): v_dict[key] = method(key, v_dict[key])
diff --git a/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/activation_sparsity.py b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/activation_sparsity.py
new file mode 100644
index 0000000..437cf08
--- /dev/null
+++ b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/activation_sparsity.py
@@ -0,0 +1,67 @@
+import re
+from collections import OrderedDict
+
+import numpy as np
+
+import matplotlib.pyplot as plt
+
+from tframe import checker
+from tframe.utils.tensor_viewer.plugin import Plugin, VariableWithView
+from tframe.utils.tensor_viewer.plugin import recursively_modify
+
+from .plotter.histogram import histogram
+from .plotter.heatmap1d import linear_heatmap
+from .plotter.heatmap1dto2d import heatmap2d
+
+
+def view(self, array_list):
+  from tframe.utils.tensor_viewer.variable_viewer import VariableViewer
+  assert isinstance(array_list, list) and isinstance(self, VariableViewer)
+
+  # Handle things happens in VariableView.refresh method
+
+  # Create subplots if not exists
+  if not hasattr(self, 'sub211'):
+    self.sub211 = self.figure.add_subplot(211, autoscale_on=True)
+  if not hasattr(self, 'sub212'):
+    self.sub212 = self.figure.add_subplot(212, autoscale_on=True)
+  # Clear subplots
+  self.sub211.cla()
+  self.sub212.cla()
+  # Hide subplot
+  self.subplot.set_axis_off()
+
+  # Hide ax2
+  self.set_ax2_invisible()
+
+  # Plot histogram
+
+  # Get range
+  a_range = [np.min(array_list), np.max(array_list)]
+  # Get activation
+  activation = array_list[self.index].flatten()
+  title = 'Activation Distribution'
+  histogram(self.sub211, activation, val_range=a_range, title=title)
+
+  # Plot heat-map
+  heatmap2d(self.sub212, activation, folds=5)
+
+  # Tight layout
+  self.figure.tight_layout()
+
+
+def method(key, value):
+  assert isinstance(key, str)
+  if 'sog_gate' not in key: return value
+  checker.check_type_v2(value, np.ndarray)
+  # Make sure activation is 1-D array
+  assert len(value[0].shape) == 1
+  return VariableWithView(value, view)
+
+
+def modifier(v_dict):
+  assert isinstance(v_dict, OrderedDict)
+  recursively_modify(method, v_dict, verbose=True)
+
+
+plugin = Plugin(dict_modifier=modifier)
diff --git a/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/plotter/__init__.py b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/plotter/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/plotter/heatmap1d.py b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/plotter/heatmap1d.py
new file mode 100644
index 0000000..cc7ddaa
--- /dev/null
+++ b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/plotter/heatmap1d.py
@@ -0,0 +1,28 @@
+import numpy as np
+
+import matplotlib.pyplot as plt
+
+
+def linear_heatmap(
+    subplot, array, title=None, horizontal=True, cmap='bwr', width=2,
+    vmax=1, vmin=-1):
+  assert isinstance(subplot, plt.Axes) and isinstance(array, np.ndarray)
+  assert isinstance(width, int) and width >= 1
+
+  # Stretch image
+  img = np.stack([array.flatten()] * width, axis=0 if horizontal else 1)
+
+  # Plot image
+  subplot.imshow(img, cmap=cmap, interpolation='none', vmin=vmin, vmax=vmax)
+
+  # Hide y axis
+  subplot.yaxis.set_ticks([])
+
+  # Set grid off
+  subplot.axis('off')
+
+  # Set title if provided
+  if isinstance(title, str): subplot.set_title(title)
+
+
+
diff --git a/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/plotter/heatmap1dto2d.py b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/plotter/heatmap1dto2d.py
new file mode 100644
index 0000000..dc0a01c
--- /dev/null
+++ b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/plotter/heatmap1dto2d.py
@@ -0,0 +1,54 @@
+import numpy as np
+
+import matplotlib.pyplot as plt
+
+
+def heatmap2d(subplot, array, title=None, folds=5, v_range=None,
+              min_color=(1., 1., 1.), max_color=(1., 0., 0.), grey=0.7):
+  """
+  :param array: numpy array
+  :param folds: height of the image to plot
+  :param v_range: value range, a tuple/list of 2 float number. None by default
+  :param min_color: color of pixel with min value, a tuple/list of 3 float
+                     numbers between 0. and 1.
+  :param max_color: color of pixel with max value, a tuple/list of 3 float
+                     number between 0. and 1.
+  """
+  # Check subplot and array
+  assert isinstance(subplot, plt.Axes) and isinstance(array, np.ndarray)
+  assert isinstance(folds, int) and folds > 0
+  # Check v_range
+  v_min, v_max = v_range if v_range else (min(array), max(array))
+  assert v_max - v_min > 0.
+
+  # Create a grey line
+  size = array.size
+  width = int(np.ceil(size / folds))
+  max_color, min_color = [
+    np.reshape(v, newshape=[1, 3]) for v in (max_color, min_color)]
+  line = np.ones(shape=[width * folds, 3]) * grey
+
+  # Map array into pixels with color and put them into the line
+  values = np.maximum(0, array - v_min) / (v_max - v_min)
+  values = np.stack([values] * 3, axis=1)
+  values = values * (max_color - min_color) + min_color
+  line[:size] = values
+
+  # Fold line to image
+  img = np.reshape(line, newshape=(folds, width, 3))
+
+  # Plot image
+  subplot.imshow(img, interpolation='none')
+
+  # Hide y axis
+  subplot.yaxis.set_ticks([])
+
+  # Set grid off
+  subplot.axis('off')
+
+  # Set title if provided
+  if isinstance(title, str): subplot.set_title(title)
+
+
+
+
diff --git a/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/plotter/histogram.py b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/plotter/histogram.py
new file mode 100644
index 0000000..69fbffd
--- /dev/null
+++ b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/plotter/histogram.py
@@ -0,0 +1,35 @@
+import numpy as np
+
+import matplotlib
+import matplotlib.pyplot as plt
+from matplotlib.ticker import FuncFormatter
+
+
+def histogram(
+    subplot, values, val_range=None, title='Distribution', y_lim_pct=0.5):
+
+  assert isinstance(subplot, plt.Axes) and isinstance(values, np.ndarray)
+  # values for 1-D distribution must be flattened
+  if len(values.shape) > 1: values = values.flatten()
+
+  # Plot 1-D histogram
+  subplot.hist(values, bins=50, facecolor='#cccccc', range=val_range)
+  subplot.set_title(title)
+  subplot.set_xlabel('Magnitude')
+  subplot.set_ylabel('Density')
+
+  # ~
+  def to_percent(y, _):
+    usetex = matplotlib.rcParams['text.usetex']
+    pct = y * 100.0 / values.size
+    return '{:.1f}{}'.format(pct, r'$\%$' if usetex else '%')
+  subplot.yaxis.set_major_formatter(FuncFormatter(to_percent))
+
+  subplot.set_aspect('auto')
+  subplot.grid(True)
+
+  subplot.set_ylim([0.0, y_lim_pct * values.size])
+
+
+
+
diff --git a/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/xwy.py b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/plotter/xwy.py
similarity index 100%
rename from utils/tensor_viewer/plugins/xwy.py
rename to utils/tensor_viewer/plugins/plotter/xwy.py
diff --git a/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/weights_distribution.py b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/weights_distribution.py
index 4c52d80..5a9f01e 100644
--- a/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/weights_distribution.py
+++ b/third_party_libraries/gam_rhn/tframe/utils/tensor_viewer/plugins/weights_distribution.py
@@ -6,6 +6,7 @@ import matplotlib
 from matplotlib.ticker import FuncFormatter
 
 from tframe.utils.tensor_viewer.plugin import Plugin, VariableWithView
+from .plotter import histogram
 
 
 prefix = 'weights_'
@@ -31,27 +32,13 @@ def view(self, weights_list):
   weights = weights_list[self.index]
   assert isinstance(weights, np.ndarray)
   weights = weights.flatten()
-  # Plot
+
+  # Hide ax2
   self.set_ax2_invisible()
 
-  self.subplot.hist(weights, bins=50, facecolor='#cccccc', range=w_range)
-  self.subplot.set_title(
-    'Weights magnitude distribution ({} total)'.format(weights.size))
-  self.subplot.set_xlabel('Magnitude')
-  self.subplot.set_ylabel('Density')
-  # self.subplot.set_xlim(w_range)
-
-  def to_percent(y, _):
-    usetex = matplotlib.rcParams['text.usetex']
-    pct = y * 100.0 / weights.size
-    return '{:.1f}{}'.format(pct, r'$\%$' if usetex else '%')
-  self.subplot.yaxis.set_major_formatter(FuncFormatter(to_percent))
-
-  self.subplot.set_aspect('auto')
-  self.subplot.grid(True)
-
-  # y_lim = self.subplot.get_ylim()
-  # if y_lim[0] > y_lim[1]: self.subplot.set_ylim(y_lim[::-1])
-  self.subplot.set_ylim([0.0, 0.065 * weights.size])
+  # Plot histogram
+  title = 'Weights magnitude distribution ({} total)'.format(weights.size)
+  histogram.histogram(self, weights, val_range=w_range, title=title)
+
 
 plugin = Plugin(dict_modifier=modifier)
diff --git a/yaml/preprocessing.yaml b/yaml/preprocessing.yaml
index 18512e2..b8cdb54 100644
--- a/yaml/preprocessing.yaml
+++ b/yaml/preprocessing.yaml
@@ -10,13 +10,16 @@ stop_loss_multiplier:
   value: 1
 minimum_return:
   desc: Amount of return chosen to consider it a profitable trade
-  value: 0.001 * 1 / 23
+  value: 0.001 * 1 / 30
 vertical_barrier_seconds:
   desc: Length of the labelling window
   value: round(1 / 2, 3)
 head:
   desc: Take the first n values of dataframes. If it equals zero take the entire df
   value: 1000
+split_by:
+  desc: Number of samples to split get_events function on to avoid maxing out the ram
+  value: 100
 vol_max_modifier: 
   desc: How much extra profit above minimum return required in the face of max volatility
   value: 0.00000002
@@ -47,6 +50,13 @@ scaling_maximum:
 scaling_minimum:
   desc: null
   value: -1
+generate_features_and_labels:
+  desc: Wether to generate the triple barrier labels or use existing
+  value: False
+apply_train_test_split_and_normalize:
+  desc: null
+  value: False
+
 
 
 
diff --git a/yaml/tabl.yaml b/yaml/tabl.yaml
index a4361e3..339685a 100644
--- a/yaml/tabl.yaml
+++ b/yaml/tabl.yaml
@@ -1,7 +1,7 @@
 # sample config defaults file
 epochs:
   desc: Number of epochs to train over
-  value: 200
+  value: 1000
 batch_size:
   desc: Size of each mini-batch
   value: 256
diff --git a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/config.yaml b/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/config.yaml
deleted file mode 100644
index 8c1a8a9..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/config.yaml
+++ /dev/null
@@ -1,34 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.8.35
-    framework: keras
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.7.6
-dataset:
-  desc: null
-  value: esugj36b.h5
-params:
-  desc: null
-  value:
-    batch_size:
-      desc: Size of each mini-batch
-      value: 256
-    dropout:
-      desc: null
-      value: 0.1
-    epochs:
-      desc: Number of epochs to train over
-      value: 100
-    num_features:
-      desc: null
-      value: 40
-    window_length:
-      desc: null
-      value: 200
-yaml:
-  desc: null
-  value: yaml/tabl.yaml
diff --git a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/cp.ckpt b/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/cp.ckpt
deleted file mode 100644
index af202fe..0000000
Binary files a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/cp.ckpt and /dev/null differ
diff --git a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/diff.patch b/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/diff.patch
deleted file mode 100644
index 214be23..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/diff.patch
+++ /dev/null
@@ -1,1270 +0,0 @@
-diff --git a/pipeline/deeplob.py b/pipeline/deeplob.py
-index 2d6c076..fef42d4 100644
---- a/pipeline/deeplob.py
-+++ b/pipeline/deeplob.py
-@@ -2,6 +2,7 @@
- import pandas as pd
- import pickle
- import numpy as np
-+import tensorflow
- import tensorflow as tf
- from tensorflow.keras import layers
- 
-@@ -25,21 +26,17 @@ from keras.utils import np_utils
- import matplotlib.pyplot as plt
- import math
- from numba import njit, prange
--
--# Init wandb
--import wandb
--from wandb.keras import WandbCallback
--wandb.init(project="prodigyai")
-+import yaml
- 
- # set random seeds
- np.random.seed(1)
- tf.random.set_seed(2)
- 
- import keras
--from keras.callbacks import ModelCheckpoint
--from keras import backend as K
-+from tensorflow.keras.callbacks import ModelCheckpoint
- import h5py
--
-+import wandb
-+from wandb.keras import WandbCallback
- # check if using gpu
- gpus = tf.config.list_physical_devices()
- any_gpus = [s for s in gpus if "GPU" in s[0]]
-@@ -105,6 +102,23 @@ if cwd == home + "/":
-     cwd = cwd + "/ProdigyAI"
-     path_adjust = cwd
- 
-+# Init wandb
-+yaml_path = path_adjust + "yaml/deeplob.yaml"
-+with open(yaml_path) as file:
-+    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
-+
-+config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
-+wandb.init(dir="~/ProdigyAI/",
-+           project="prodigyai",
-+           config=config_dictionary,
-+           resume=True)
-+
-+window_length = wandb.config['params']['window_length']['value']
-+num_features = wandb.config['params']['num_features']['value']
-+epochs = wandb.config['params']['epochs']['value']
-+batch_size = wandb.config['params']['batch_size']['value']
-+number_of_lstm = wandb.config['params']['number_of_lstm']['value']
-+
- # limit gpu usage for keras
- gpu_devices = tf.config.experimental.list_physical_devices("GPU")
- for device in gpu_devices:
-@@ -216,7 +230,8 @@ import h5py
- 
- home = str(Path.home())
- 
--file_name = "arch=DLOB&name=two_model&WL=200&pt=1&sl=1&min_ret=4.347826086956522e-05&vbs=0.5&head=0&skip=0&vol_max=4.3498260869565224e-05&vol_min=4.348826086956522e-05&filter=none&cm_vol_mod=0&sw=on&fd=off&input=obook&ntb=True&tslbc=True.h5"
-+file_name = "esugj36b.h5"
-+wandb.config.update({'dataset': file_name})
- path = home + "/ProdigyAI/data/preprocessed/" + file_name
- h5f = h5py.File(path, "r")
- prices_for_window_index_array_train = h5f[
-@@ -268,6 +283,7 @@ def generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data):
- 
- class DataGenerator(tf.compat.v2.keras.utils.Sequence):
-     def __init__(self,
-+                 checkpoint_path,
-                  prices_for_window_index_array,
-                  input_features_normalized,
-                  y_data,
-@@ -277,6 +293,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
-                  to_fit,
-                  shuffle=True):
-         self.batch_size = batch_size
-+        self.checkpoint_path = checkpoint_path
-         self.prices_for_window_index_array = prices_for_window_index_array
-         self.input_features_normalized = input_features_normalized
-         self.labels = y_data
-@@ -323,6 +340,8 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
-     def on_epoch_end(self):
- 
-+        wandb.save(self.checkpoint_path)
-+
-         self.indexes = np.arange(len(self.prices_for_window_index_array))
- 
-         if self.shuffle:
-@@ -351,8 +370,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
- 
- n_classes = 3
--dim = (200, 40)
--batch_size = 64
-+dim = (window_length, num_features)
- to_fit = True
- 
- # for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,
-@@ -371,70 +389,6 @@ to_fit = True
- 
- ## PRODIGY AI HOCKUS POCKUS END
- 
--
--### Model Architecture
--def create_deeplob(T, NF, number_of_lstm):
--    input_lmd = Input(shape=(T, NF, 1))
--
--    # build the convolutional block
--    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    conv_first1 = Conv2D(32, (1, 10))(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    # build the inception module
--    convsecond_1 = Conv2D(64, (1, 1), padding="same")(conv_first1)
--    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
--    convsecond_1 = Conv2D(64, (3, 1), padding="same")(convsecond_1)
--    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
--
--    convsecond_2 = Conv2D(64, (1, 1), padding="same")(conv_first1)
--    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
--    convsecond_2 = Conv2D(64, (5, 1), padding="same")(convsecond_2)
--    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
--
--    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),
--                                padding="same")(conv_first1)
--    convsecond_3 = Conv2D(64, (1, 1), padding="same")(convsecond_3)
--    convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)
--
--    convsecond_output = concatenate([convsecond_1, convsecond_2, convsecond_3],
--                                    axis=3)
--
--    # use the MC dropout here
--    conv_reshape = Reshape(
--        (int(convsecond_output.shape[1]),
--         int(convsecond_output.shape[3])))(convsecond_output)
--
--    conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)
--
--    # build the output layer
--    out = Dense(3, activation="softmax")(conv_lstm)
--    model = Model(inputs=input_lmd, outputs=out)
--    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
--    model.compile(optimizer=adam,
--                  loss="categorical_crossentropy",
--                  metrics=["accuracy"])
--
--    return model
--
--
- checkpoint_path = path_adjust + "temp/cp.ckpt"
- # Create a callback that saves the model's weights
- cp_callback = ModelCheckpoint(filepath=checkpoint_path,
-@@ -446,9 +400,8 @@ if check_point_file.exists() and resuming == "resuming":
-     print("weights loaded")
-     model.load_weights(checkpoint_path)
- 
--batch_size = 64
--
--train_generator = DataGenerator(prices_for_window_index_array_train,
-+train_generator = DataGenerator(checkpoint_path,
-+                                prices_for_window_index_array_train,
-                                 input_features_normalized_train,
-                                 y_train,
-                                 batch_size,
-@@ -457,7 +410,8 @@ train_generator = DataGenerator(prices_for_window_index_array_train,
-                                 to_fit,
-                                 shuffle=True)
- 
--val_generator = DataGenerator(prices_for_window_index_array_val,
-+val_generator = DataGenerator(checkpoint_path,
-+                              prices_for_window_index_array_val,
-                               input_features_normalized_val,
-                               y_val,
-                               batch_size,
-@@ -469,14 +423,87 @@ val_generator = DataGenerator(prices_for_window_index_array_val,
- steps_per_epoch = len(train_generator)
- validation_steps = len(val_generator)
- 
--deeplob = create_deeplob(200, 40, 64)
--deeplob.fit(train_generator,
--            steps_per_epoch=steps_per_epoch,
--            validation_steps=validation_steps,
--            epochs=100,
--            verbose=2,
--            validation_data=val_generator,
--            callbacks=[cp_callback, WandbCallback()])
-+if wandb.run.resumed:
-+    wandb.restore("model-best.h5",run_path="garthtrickett/prodigyai/" + wandb.run.id)
-+    # restore the best model
-+    deeplob = tensorflow.keras.models.load_model(wandb.restore("model-best.h5").name)
-+    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
-+    deeplob.compile(optimizer=adam,
-+                    loss="categorical_crossentropy",
-+                    metrics=["accuracy"])
-+else:
-+    ### Model Architecture
-+    def create_deeplob(T, NF, number_of_lstm):
-+        input_lmd = Input(shape=(T, NF, 1))
-+
-+        # build the convolutional block
-+        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        conv_first1 = Conv2D(32, (1, 10))(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        # build the inception module
-+        convsecond_1 = Conv2D(64, (1, 1), padding="same")(conv_first1)
-+        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
-+        convsecond_1 = Conv2D(64, (3, 1), padding="same")(convsecond_1)
-+        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
-+
-+        convsecond_2 = Conv2D(64, (1, 1), padding="same")(conv_first1)
-+        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
-+        convsecond_2 = Conv2D(64, (5, 1), padding="same")(convsecond_2)
-+        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
-+
-+        convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),
-+                                    padding="same")(conv_first1)
-+        convsecond_3 = Conv2D(64, (1, 1), padding="same")(convsecond_3)
-+        convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)
-+
-+        convsecond_output = concatenate(
-+            [convsecond_1, convsecond_2, convsecond_3], axis=3)
-+
-+        # use the MC dropout here
-+        conv_reshape = Reshape(
-+            (int(convsecond_output.shape[1]),
-+             int(convsecond_output.shape[3])))(convsecond_output)
-+
-+        conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)
-+
-+        # build the output layer
-+        out = Dense(3, activation="softmax")(conv_lstm)
-+        model = Model(inputs=input_lmd, outputs=out)
-+        adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
-+        model.compile(optimizer=adam,
-+                      loss="categorical_crossentropy",
-+                      metrics=["accuracy"])
-+
-+        return model
-+
-+    deeplob = create_deeplob(window_length, num_features, number_of_lstm)
-+deeplob.fit(
-+    train_generator,
-+    steps_per_epoch=steps_per_epoch,
-+    validation_steps=validation_steps,
-+    epochs=100,
-+    verbose=2,
-+    validation_data=val_generator,
-+    callbacks=[cp_callback,
-+               WandbCallback(save_model=True, monitor="loss")])
- 
- finished_weights_path = path_adjust + "temp/cp_end.ckpt"
- deeplob.save_weights(finished_weights_path)
-diff --git a/pipeline/preprocessing.py b/pipeline/preprocessing.py
-index 932ae0b..714d325 100644
---- a/pipeline/preprocessing.py
-+++ b/pipeline/preprocessing.py
-@@ -136,54 +136,67 @@ if stage == 2:
-     side = X_for_all_labels["predicted_bins"]
-     # Could use the probabilities (instead of [1,0,0] use [0.2,0.55,0.25]
- 
-+import yaml
-+import wandb
-+
-+yaml_path = path_adjust + "yaml/preprocessing.yaml"
-+with open(yaml_path) as file:
-+    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
-+
-+config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
-+
-+wandb.init(
-+    dir="~/ProdigyAI/",
-+    project="prodigyai",
-+    config=config_dictionary,
-+)
-+
-+minimum_return = eval(wandb.config['params']['minimum_return']['value'])
-+vertical_barrier_seconds = eval(
-+    wandb.config['params']['vertical_barrier_seconds']['value'])
-+
- # Parameters
- parameters = dict()
--model_arch = "TABL"
--parameters["arch"] = model_arch
--parameters["name"] = model
--parameters["WL"] = 200  # WINDOW LONG
--parameters["pt"] = 1
--parameters["sl"] = 1
--parameters["min_ret"] = 0.001 * 1 / 23
--parameters["vbs"] = round(1 / 2,
--                          3)  # Increasing this decreases vertical touches
--parameters["head"] = 1000  # take only first x number of rows 0 means of
--parameters["skip"] = 0  # sample every n'th row if skip > 0
--# get even classes at fraction = 0.8 so the training set is balanced then set to 1
--# 3 million rows is about the limit of before it starts taking ages / memory maxing
--parameters["vol_max"] = (
--    parameters["min_ret"] + 0.00000002
-+wandb.config['params']['head'][
-+    'value'] = 1000  # take only first x number of rows 0 means of
-+volume_max = (
-+    minimum_return + wandb.config['params']['vol_max_modifier']['value']
- )  # The higher this is the more an increase in volatility requries an increase
- # in return to be considered buy/sell (Increasing this increases end barrier vertical touches)
--parameters["vol_min"] = parameters["min_ret"] + 0.00000001
-+volume_min = minimum_return + wandb.config['params']['vol_min_modifier'][
-+    'value']
- 
--parameters["filter"] = "none"
-+filter_type = wandb.config['params']['filter_type']['value']
- 
--if parameters["filter"] == "cm":
--    parameters["cm_vol_mod"] = 500
-+if filter_type == "cm":
-+    cusum_filter_vol_modifier = wandb.config['params'][
-+        'cusum_filter_volume_modifier']['value']
- else:
--    parameters["cm_vol_mod"] = 0
-+    cusum_filter_vol_modifier = 0
- 
--parameters["sw"] = "on"  # sample weights
--parameters["fd"] = "off"
-+use_sample_weights = wandb.config['params']['use_sample_weights']['value']
-+use_sample_weights = wandb.config['params']['use_fractional_differentiation'][
-+    'value']
- 
--parameters["input"] = "obook"
-+input_type = wandb.config['params']['input_type']['value']
- 
--parameters["ntb"] = True  # non time bars
-+# parameters["ntb"] = True  # non time bars
- 
--if parameters["ntb"] == True:
--    # Pick whether you want to add in the time since last bar input feature
--    # time since last bar column
--    parameters["tslbc"] = True  # time since last bar column
--else:
--    # Pick whether you want to add in the volume input feature
--    parameters["vbc"] = True  # volume bar column
-+# if parameters["ntb"] == True:
-+#     # Pick whether you want to add in the time since last bar input feature
-+#     # time since last bar column
-+#     parameters["tslbc"] = True  # time since last bar column
-+# else:
-+#     # Pick whether you want to add in the volume input feature
-+#     parameters["vbc"] = True  # volume bar column
- 
- # Create the txt file string
--parameter_string = "&".join("{}{}{}".format(key, "=", val)
--                            for key, val in parameters.items())
-+parameter_string = wandb.run.id
- 
--pt_sl = [parameters["pt"], parameters["sl"]]
-+pt_sl = [
-+    wandb.config['params']['profit_taking_multiplier']['value'],
-+    wandb.config['params']['stop_loss_multiplier']['value']
-+]
- cpus = cpu_count() - 1
- 
- regenerate_features_and_labels = True
-@@ -192,10 +205,10 @@ if regenerate_features_and_labels == True:
-     if stage == 1:
-         # Side
-         print("starting data load")
--        head = parameters["head"]
-+        head = wandb.config['params']['head']['value']
- 
-         # # read parquet file of dollar bars
--        if parameters["input"] == "bars":
-+        if input_type == "bars":
-             # Mlfinlab bars
-             data = pq.read_pandas(
-                 path_adjust + "data/bars/"
-@@ -216,7 +229,7 @@ if regenerate_features_and_labels == True:
-             data.index = pd.to_datetime(data.index, infer_datetime_format=True)
- 
-         # read parquet file of raw ticks
--        if parameters["input"] == "ticks":
-+        if input_type == "ticks":
-             data = pq.read_pandas(
-                 path_adjust + "data/bars/" +
-                 "btcusdt_agg_trades_raw_tick_data.parquet").to_pandas()
-@@ -238,11 +251,7 @@ if regenerate_features_and_labels == True:
-             # Should do something else than drop the duplicates (maybe trades doesnt have duplicate indexes rather than aggtrades)
-             data = data.loc[~data.index.duplicated(keep="first")]
- 
--            # skip most rows if this is > 1
--            if parameters["skip"] > 0:
--                data = data.iloc[::parameters["skip"], :]
--
--        if parameters["input"] == "obook":
-+        if input_type == "orderbook":
-             with open(path_adjust + "temp/orderbook_data_name.txt",
-                       "r") as text_file:
-                 orderbook_preprocessed_file_name = text_file.read()
-@@ -275,7 +284,7 @@ if regenerate_features_and_labels == True:
-         # duplicate_fast_search(data.index.duplicated())
- 
-         # Fractional differentiation
--        if parameters["fd"] == "on":
-+        if use_sample_weights == "on":
-             data_series = data["close"].to_frame()
-             # # generate 100 points
-             # nsample = 1000
-@@ -313,16 +322,16 @@ if regenerate_features_and_labels == True:
- 
-         start = time.time()
-         volatility_level_array = volatility_levels_numba(
--            np.ascontiguousarray(data.close.values), parameters["WL"])
-+            np.ascontiguousarray(data.close.values),
-+            wandb.config['params']['window_length']['value'])
-         data["window_volatility_level"] = volatility_level_array
- 
-         # Should adjust the max value
-         # To get more vertical touches we can
-         # either increase vol_max or
-         # decrease the window seconds
--        scaler = MinMaxScaler(
--            feature_range=(parameters["vol_min"],
--                           parameters["vol_max"]))  # normalization
-+        scaler = MinMaxScaler(feature_range=(volume_min,
-+                                             volume_max))  # normalization
- 
-         normed_window_volatility_level = scaler.fit_transform(
-             data[["window_volatility_level"]])
-@@ -338,7 +347,7 @@ if regenerate_features_and_labels == True:
-         close_np_array, close_index_np_array = pandas_series_to_numba_ready_np_arrays(
-             close_copy)
- 
--        volatility_threshold = volatility_threshold * parameters["cm_vol_mod"]
-+        volatility_threshold = volatility_threshold * cusum_filter_vol_modifier
-         print("data_len = " + str(len(data)))
-         start = time.time()
-         sampled_idx = filter_events(
-@@ -346,7 +355,7 @@ if regenerate_features_and_labels == True:
-             close_np_array,
-             close_index_np_array,
-             volatility_threshold,
--            parameters["filter"],
-+            filter_type,
-         )
-         print("sampled_idx_len = " + str(len(sampled_idx)))
-         end = time.time()
-@@ -356,7 +365,8 @@ if regenerate_features_and_labels == True:
-         # size
-         start = time.time()
-         volatility_level_array = volatility_levels_numba(
--            data.close.values, parameters["WL"])
-+            data.close.values,
-+            wandb.config['params']['window_length']['value'])
-         data["window_volatility_level"] = volatility_level_array
- 
-     # This code runs for both first and second stage preprocessing
-@@ -364,7 +374,7 @@ if regenerate_features_and_labels == True:
-     vertical_barrier_timestamps = ml.labeling.add_vertical_barrier(
-         t_events=sampled_idx,
-         close=data["close"],
--        num_seconds=parameters["vbs"])
-+        num_seconds=vertical_barrier_seconds)
-     end = time.time()
-     print("vertical barrier" + str(end - start))
- 
-@@ -376,7 +386,7 @@ if regenerate_features_and_labels == True:
-         t_events=sampled_idx,
-         pt_sl=pt_sl,
-         target=data["window_volatility_level"],
--        min_ret=parameters["min_ret"],
-+        min_ret=minimum_return,
-         num_threads=cpus * 2,
-         vertical_barrier_times=vertical_barrier_timestamps,
-         side_prediction=side,
-@@ -533,26 +543,27 @@ if stage == 1:
-     ### ORDERBOOK VOLUME DATA
-     volumes_for_all_labels = volumes.loc[data.close.index]
- 
--    ## TRADE DATA
--    input_features_trade = []
--    close_array = data.close.values
--    input_features_trade.append(close_array)
-+    # ## TRADE DATA
-+    # input_features_trade = []
-+    # close_array = data.close.values
-+    # input_features_trade.append(close_array)
- 
--    if parameters["ntb"] == False and parameters["vbc"] == True:
--        volume_array = data.volume.values
--        input_features_trade.append(volume_array)
--    if parameters["ntb"] == True and parameters["tslbc"] == True:
--        time_since_last_bar_array = data.time_since_last_bar.values
--        input_features_trade.append(time_since_last_bar_array)
-+    # if parameters["ntb"] == False and parameters["vbc"] == True:
-+    #     volume_array = data.volume.values
-+    #     input_features_trade.append(volume_array)
-+    # if parameters["ntb"] == True and parameters["tslbc"] == True:
-+    #     time_since_last_bar_array = data.time_since_last_bar.values
-+    #     input_features_trade.append(time_since_last_bar_array)
- 
-     end_time = time.time()
-     print(end_time - start_time)
- 
--    # min max limits
-+    # Type of scaling to apply
-+    scaling_type = wandb.config['params']['scaling_type']['value']
- 
--    minimum = -1
--    maximum = 1
--    scaling_type = "z_score"
-+    # min max limits
-+    minimum = wandb.config['params']['scaling_maximum']['value']
-+    maximum = wandb.config['params']['scaling_minimum']['value']
- 
-     ### Split intothe training/validation/test sets
-     print("splitting into train/va/test sets start")
-@@ -573,11 +584,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_train = volumes_for_all_labels.iloc[
-         train_close_array_integer_index[0] -
--        parameters["WL"]:train_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        train_close_array_integer_index[-1] + 2]
- 
-     close_index_array_train = close_index_array[
-         train_close_array_integer_index[0] -
--        parameters["WL"]:train_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        train_close_array_integer_index[-1] + 2]
- 
-     end_time = time.time()
- 
-@@ -623,7 +636,7 @@ if stage == 1:
-     # print("Make window started")
-     # start_time = time.time()
- 
--    # padding = parameters["WL"] * 2
-+    # padding = wandb.config['params']['window_length']['value'] * 2
- 
-     # split_by = 100000
- 
-@@ -652,7 +665,7 @@ if stage == 1:
-     #         len(prices_for_window_index_array_train[start_index:end_index]),
-     #         input_features_normalized_train[:, close_and_input_start_index:
-     #                                         close_and_input_end_index],
--    #         parameters["WL"],
-+    #         wandb.config['params']['window_length']['value'],
-     #         model_arch,
-     #     )
- 
-@@ -697,11 +710,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_val = volumes_for_all_labels.iloc[
-         val_close_array_integer_index[0] -
--        parameters["WL"]:val_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        val_close_array_integer_index[-1] + 2]
- 
-     close_index_array_val = close_index_array[
-         val_close_array_integer_index[0] -
--        parameters["WL"]:val_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        val_close_array_integer_index[-1] + 2]
- 
-     input_features_val = make_input_features_from_orderbook_data(
-         volumes_for_all_labels_val)
-@@ -721,7 +736,7 @@ if stage == 1:
-     #     #     prices_for_window_index_array_val,
-     #     #     close_index_array_val,
-     #     #     input_features_normalized_val,
--    #     #     parameters["WL"],
-+    #     #     wandb.config['params']['window_length']['value'],
-     #     #     model_arch,
-     #     # )
- 
-@@ -736,11 +751,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_test = volumes_for_all_labels.iloc[
-         test_close_array_integer_index[0] -
--        parameters["WL"]:test_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        test_close_array_integer_index[-1] + 2]
- 
-     close_index_array_test = close_index_array[
-         test_close_array_integer_index[0] -
--        parameters["WL"]:test_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        test_close_array_integer_index[-1] + 2]
- 
-     input_features_test = make_input_features_from_orderbook_data(
-         volumes_for_all_labels_test)
-@@ -761,7 +778,7 @@ if stage == 1:
- #     #     prices_for_window_index_array_test,
- #     #     close_index_array_test,
- #     #     input_features_normalized_test,
--#     #     parameters["WL"],
-+#     #     wandb.config['params']['window_length']['value'],
- #     #     model_arch,
- #     # )
- 
-@@ -782,8 +799,9 @@ ax = plt.gca()
- 
- data.iloc[window_index - 200:window_index + 200].plot(y="close",
-                                                       use_index=True)
--plot_window_and_touch_and_label(window_index, parameters["WL"], data,
--                                triple_barrier_events, labels)
-+plot_window_and_touch_and_label(
-+    window_index, wandb.config['params']['window_length']['value'], data,
-+    triple_barrier_events, labels)
- 
- data.iloc[window_index - 10:window_index + 30]
- 
-@@ -837,9 +855,9 @@ print("writing train/val/test to .h5 files finished taking " +
- #     h5f.create_dataset("P", data=P)
- # h5f.create_dataset("y", data=y)
- # h5f.create_dataset("sampled_idx_epoch", data=sampled_idx_epoch)
--# if parameters["sw"] == "on":
-+# if use_sample_weights == "on":
- #     h5f.create_dataset("sample_weights", data=sample_weights)
--# elif parameters["sw"] == "off":
-+# elif use_sample_weights == "off":
- #     h5f.create_dataset("sample_weights", data=np.zeros(1))
- # h5f.close()
- 
-diff --git a/pipeline/tabl.py b/pipeline/tabl.py
-index 87d51f8..f0244ec 100644
---- a/pipeline/tabl.py
-+++ b/pipeline/tabl.py
-@@ -6,7 +6,7 @@ import sys
- 
- import imblearn
- from imblearn.over_sampling import SMOTE
--
-+import tensorflow
- import matplotlib.pyplot as plt
- 
- sys.path.append("..")
-@@ -17,29 +17,26 @@ home = str(Path.home())
- sys.path.append(home + "/ProdigyAI")
- 
- from numba import njit, prange
--import tensorflow as tf
-+
- import math
- 
--import keras
- import numpy as np
- from third_party_libraries.TABL import Models
- from third_party_libraries.keras_lr_finder.lr_finder import LRFinder
- from third_party_libraries.CLR.clr_callbacks import CyclicLR
- 
--from tensorflow.keras.losses import categorical_crossentropy
--from tensorflow.keras.optimizers import SGD, Adam
-+import keras
-+from keras.losses import categorical_crossentropy
-+from keras.optimizers import SGD, Adam
- from keras.callbacks import ModelCheckpoint
- 
-+import tensorflow as tf
- # Init wandb
- import wandb
- from wandb.keras import WandbCallback
- 
- import yaml
- 
--# import tensorflow.keras as keras
--# import tensorflow.keras.layers as layers
--# import tensorflow.keras.backend as K
--
- # Sorting out whether we are using the ipython kernel or not
- try:
-     get_ipython()
-@@ -95,25 +92,24 @@ with open(yaml_path) as file:
-     yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
- 
- config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
--wandb.init(
--    dir="~/ProdigyAI/",
--    project="prodigyai",
--    config=config_dictionary,
--)
--
--# try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes
--
--# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1
--template = [[40, 200], [60, 10], [120, 5], [3, 1]]
--
--# example data
--example_x = np.random.rand(1000, 40, 10)
--np.min(example_x)
--np.max(example_x)
--np.mean(example_x)
--example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)
--template = [[40, 200], [60, 10], [120, 5], [3, 1]]
--# 200(WINDOW LENGTH)
-+wandb.init(dir="~/ProdigyAI/",
-+           project="prodigyai",
-+           config=config_dictionary,
-+           resume=True)
-+
-+window_length = wandb.config['params']['window_length']['value']
-+num_features = wandb.config['params']['num_features']['value']
-+epochs = wandb.config['params']['epochs']['value']
-+batch_size = wandb.config['params']['batch_size']['value']
-+dropout = wandb.config['params']['dropout']['value']
-+
-+# # try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes
-+# # example data
-+# example_x = np.random.rand(1000, 40, 10)
-+# np.min(example_x)
-+# np.max(example_x)
-+# np.mean(example_x)
-+# example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)
- 
- ## PRODIGY AI HOCKUS POCKUS START
- from pathlib import Path
-@@ -121,7 +117,7 @@ import h5py
- 
- home = str(Path.home())
- 
--file_name = "arch=TABL&name=two_model&WL=200&pt=1&sl=1&min_ret=4.347826086956522e-05&vbs=0.5&head=1000&skip=0&vol_max=4.3498260869565224e-05&vol_min=4.348826086956522e-05&filter=none&cm_vol_mod=0&sw=on&fd=off&input=obook&ntb=True&tslbc=True.h5"
-+file_name = "esugj36b.h5"
- wandb.config.update({'dataset': file_name})
- path = home + "/ProdigyAI/data/preprocessed/" + file_name
- h5f = h5py.File(path, "r")
-@@ -265,8 +261,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
- 
- n_classes = 3
--dim = (200, 40)
--batch_size = 64
-+dim = (window_length, num_features)
- to_fit = True
- 
- # for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,
-@@ -280,22 +275,15 @@ to_fit = True
- #     import pdb
- #     pdb.set_trace()
- 
-+# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1
-+
-+template = [[num_features, window_length], [60, 10], [120, 5], [3, 1]]
-+
- # get Bilinear model
- projection_regularizer = None
- projection_constraint = keras.constraints.max_norm(3.0, axis=0)
- attention_regularizer = None
- attention_constraint = keras.constraints.max_norm(5.0, axis=1)
--dropout = 0.1
--
--model = Models.TABL(
--    template,
--    dropout,
--    projection_regularizer,
--    projection_constraint,
--    attention_regularizer,
--    attention_constraint,
--)
--model.summary()
- 
- # path = path_adjust + "data/lob_2010/train_and_test_tabl.h5"
- # h5f = h5py.File(path, "r")
-@@ -442,6 +430,18 @@ clr = CyclicLR(base_lr=base_lr,
- 
- optimizer = keras.optimizers.Adam()
- 
-+model = Models.TABL(
-+    template,
-+    dropout,
-+    projection_regularizer,
-+    projection_constraint,
-+    attention_regularizer,
-+    attention_constraint,
-+)
-+model.summary()
-+
-+
-+
- # Compile the model
- model.compile(loss=loss_function, optimizer=optimizer, metrics=["accuracy"])
- # Create a callback that saves the model's weights
-@@ -449,10 +449,15 @@ cp_callback = ModelCheckpoint(filepath=checkpoint_path,
-                               save_weights_only=True,
-                               verbose=1)
- 
--check_point_file = Path(checkpoint_path)
--if check_point_file.exists() and resuming == "resuming":
--    print("weights loaded")
--    model.load_weights(checkpoint_path)
-+if wandb.run.resumed:
-+    weights = wandb.restore("cp.ckpt",
-+                            run_path="garthtrickett/prodigyai/" + wandb.run.id)
-+    model.load_weights(weights.name)
-+
-+# check_point_file = Path(checkpoint_path)
-+# if check_point_file.exists() and resuming == "resuming":
-+#     print("weights loaded")
-+#     model.load_weights(checkpoint_path)
- # Fit data to model
- # history = model.fit(trainX_CNN,
- #                     trainY_CNN,
-@@ -460,9 +465,6 @@ if check_point_file.exists() and resuming == "resuming":
- #                     epochs=20,
- #                     callbacks=[clr, cp_callback])
- 
--finished_weights_path = path_adjust + "temp/cp_end.ckpt"
--model.save_weights(finished_weights_path)
--
- # create class weight
- # class_weight = {0: 1e6 / 300.0, 1: 1e6 / 400.0, 2: 1e6 / 300.0}
- train_generator = DataGenerator(checkpoint_path,
-@@ -489,12 +491,14 @@ steps_per_epoch = len(train_generator)
- validation_steps = len(val_generator)
- 
- # example sata training
--model.fit(train_generator,
--          steps_per_epoch=steps_per_epoch,
--          validation_steps=validation_steps,
--          epochs=1000,
--          validation_data=val_generator,
--          callbacks=[cp_callback, WandbCallback()])
-+model.fit(
-+    train_generator,
-+    steps_per_epoch=steps_per_epoch,
-+    validation_steps=validation_steps,
-+    epochs=epochs,
-+    validation_data=val_generator,
-+    callbacks=[cp_callback,
-+               WandbCallback(save_model=True, monitor="loss")])
- 
- model.save(os.path.join(wandb.run.dir, "model.h5"))
- # no class weight
-@@ -603,16 +607,16 @@ model.save(os.path.join(wandb.run.dir, "model.h5"))
- # X_val = X_val.reshape((X_val.shape[0], 40, 10))
- 
- # train one epoch to find the learning rate
--model.fit(
--    X_train,
--    y_train,
--    validation_data=(X_val, y_val),
--    batch_size=256,
--    epochs=1,
--    shuffle=False,
--)  # no class weight
--
--score = model.evaluate(x=X_test, y=y_test, batch_size=256)
-+# model.fit(
-+#     X_train,
-+#     y_train,
-+#     validation_data=(X_val, y_val),
-+#     batch_size=256,
-+#     epochs=1,
-+#     shuffle=False,
-+# )  # no class weight
-+
-+# score = model.evaluate(x=X_test, y=y_test, batch_size=256)
- 
- # Save model to wandb
- 
-diff --git a/temp/cp.ckpt.data-00000-of-00001 b/temp/cp.ckpt.data-00000-of-00001
-index 56c00e4..3d9600c 100644
-Binary files a/temp/cp.ckpt.data-00000-of-00001 and b/temp/cp.ckpt.data-00000-of-00001 differ
-diff --git a/temp/cp.ckpt.index b/temp/cp.ckpt.index
-index 3770713..d6d3b4b 100644
-Binary files a/temp/cp.ckpt.index and b/temp/cp.ckpt.index differ
-diff --git a/temp/cp_end.ckpt b/temp/cp_end.ckpt
-index c1c1d9e..62e0a67 100644
-Binary files a/temp/cp_end.ckpt and b/temp/cp_end.ckpt differ
-diff --git a/temp/cp_end.ckpt.data-00000-of-00001 b/temp/cp_end.ckpt.data-00000-of-00001
-index 1d572d0..12d3157 100644
-Binary files a/temp/cp_end.ckpt.data-00000-of-00001 and b/temp/cp_end.ckpt.data-00000-of-00001 differ
-diff --git a/temp/cp_end.ckpt.index b/temp/cp_end.ckpt.index
-index 15de09a..0ab67e1 100644
-Binary files a/temp/cp_end.ckpt.index and b/temp/cp_end.ckpt.index differ
-diff --git a/temp/is_script_finished.txt b/temp/is_script_finished.txt
-index 40ca0d3..bec7a08 100644
---- a/temp/is_script_finished.txt
-+++ b/temp/is_script_finished.txt
-@@ -1 +1 @@
--full_script_time64.39192986488342
-\ No newline at end of file
-+full_script_time25.707124948501587
-\ No newline at end of file
-Submodule third_party_libraries/TABL contains modified content
-Submodule third_party_libraries/TABL 7f25314..df3e92d:
-diff --git a/third_party_libraries/TABL/Layers.py b/third_party_libraries/TABL/Layers.py
-index 9020002..b299e57 100644
---- a/third_party_libraries/TABL/Layers.py
-+++ b/third_party_libraries/TABL/Layers.py
-@@ -3,8 +3,9 @@
- """
- @author: Dat Tran (dat.tranthanh@tut.fi)
- """
-+import keras
- from keras import backend as K
--from keras.engine.topology import Layer
-+from keras.layers import Layer
- from keras import activations as Activations
- from keras import initializers as Initializers
- 
-@@ -18,25 +19,25 @@ class Constraint(object):
- 
-     def get_config(self):
-         return {}
--    
-+
-+
- class MinMax(Constraint):
-     """
-     Customized min-max constraint for scalar
-     """
--
-     def __init__(self, min_value=0.0, max_value=10.0):
-         self.min_value = min_value
-         self.max_value = max_value
-+
-     def __call__(self, w):
--        
--        return K.clip(w,self.min_value,self.max_value)
-+
-+        return K.clip(w, self.min_value, self.max_value)
- 
-     def get_config(self):
--        return {'min_value': self.min_value,
--                'max_value': self.max_value}
--        
-+        return {'min_value': self.min_value, 'max_value': self.max_value}
- 
--def nmodeproduct(x,w,mode):
-+
-+def nmodeproduct(x, w, mode):
-     """
-     n-mode product for 2D matrices
-     x: NxHxW
-@@ -45,74 +46,83 @@ def nmodeproduct(x,w,mode):
-     
-     output: NxhxW (mode1) or NxHxw (mode2)
-     """
--    if mode==2:
--        x=K.dot(x,w)
-+    if mode == 2:
-+        x = K.dot(x, w)
-     else:
--        x=K.permute_dimensions(x,(0,2,1))
--        x=K.dot(x,w)
--        x=K.permute_dimensions(x,(0,2,1))
-+        x = K.permute_dimensions(x, (0, 2, 1))
-+        x = K.dot(x, w)
-+        x = K.permute_dimensions(x, (0, 2, 1))
-     return x
- 
-+
- class BL(Layer):
-     """
-     Bilinear Layer
-     """
--    def __init__(self, output_dim,
-+    def __init__(self,
-+                 output_dim,
-                  kernel_regularizer=None,
--                 kernel_constraint=None,**kwargs):
-+                 kernel_constraint=None,
-+                 **kwargs):
-         """
-         output_dim : output dimensions of 2D tensor, should be a list of len 2, e.g. [30,20]
-         kernel_regularizer : keras regularizer object
-         kernel_constraint: keras constraint object
-         """
--        
-+
-         self.output_dim = output_dim
--        self.kernel_regularizer=kernel_regularizer
--        self.kernel_constraint=kernel_constraint
--        
-+        self.kernel_regularizer = kernel_regularizer
-+        self.kernel_constraint = kernel_constraint
-+
-         super(BL, self).__init__(**kwargs)
- 
-     def build(self, input_shape):
--        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
--                                      initializer='he_uniform',
--                                      regularizer=self.kernel_regularizer,
--                                      constraint=self.kernel_constraint,
--                                      trainable=True)
--        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
--                                      initializer='he_uniform',
--                                      regularizer=self.kernel_regularizer,
--                                      constraint=self.kernel_constraint,
--                                      trainable=True)
--
--        self.bias=self.add_weight(name='bias',shape=(self.output_dim[0],self.output_dim[1]),
--                              initializer='zeros',trainable=True)
-+        self.W1 = self.add_weight(name='W1',
-+                                  shape=(input_shape[1], self.output_dim[0]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.kernel_regularizer,
-+                                  constraint=self.kernel_constraint,
-+                                  trainable=True)
-+        self.W2 = self.add_weight(name='W2',
-+                                  shape=(input_shape[2], self.output_dim[1]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.kernel_regularizer,
-+                                  constraint=self.kernel_constraint,
-+                                  trainable=True)
-+
-+        self.bias = self.add_weight(name='bias',
-+                                    shape=(self.output_dim[0],
-+                                           self.output_dim[1]),
-+                                    initializer='zeros',
-+                                    trainable=True)
-         super(BL, self).build(input_shape)
- 
-     def call(self, x):
-         print(K.int_shape(x))
--        x = nmodeproduct(x,self.W1,1)
--        x = nmodeproduct(x,self.W2,2)
--        x = K.bias_add(x,self.bias)
--        
--        if self.output_dim[1]==1:
--            x = K.squeeze(x,axis=-1)
-+        x = nmodeproduct(x, self.W1, 1)
-+        x = nmodeproduct(x, self.W2, 2)
-+        x = K.bias_add(x, self.bias)
-+
-+        if self.output_dim[1] == 1:
-+            x = K.squeeze(x, axis=-1)
-         print(K.int_shape(x))
-         return x
- 
-     def compute_output_shape(self, input_shape):
--        if self.output_dim[1]==1:
-+        if self.output_dim[1] == 1:
-             return (input_shape[0], self.output_dim[0])
-         else:
-             return (input_shape[0], self.output_dim[0], self.output_dim[1])
--        
--        
-+
-+
- class TABL(Layer):
-     """
-     Temporal Attention augmented Bilinear Layer
-     https://arxiv.org/abs/1712.00975
-     
-     """
--    def __init__(self, output_dim,
-+    def __init__(self,
-+                 output_dim,
-                  projection_regularizer=None,
-                  projection_constraint=None,
-                  attention_regularizer=None,
-@@ -125,45 +135,52 @@ class TABL(Layer):
-         attention_regularizer: keras regularizer object for attention matrix
-         attention_constraint: keras constraint object for attention matrix
-         """
--        
-+
-         self.output_dim = output_dim
-         self.projection_regularizer = projection_regularizer
-         self.projection_constraint = projection_constraint
-         self.attention_regularizer = attention_regularizer
-         self.attention_constraint = attention_constraint
--        
-+
-         super(TABL, self).__init__(**kwargs)
- 
-     def build(self, input_shape):
--        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
--                                      initializer='he_uniform',
--                                      regularizer=self.projection_regularizer,
--                                      constraint=self.projection_constraint,
--                                      trainable=True)
--        
--        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
--                                      initializer='he_uniform',
--                                      regularizer=self.projection_regularizer,
--                                      constraint=self.projection_constraint,
--                                      trainable=True)
--        
--        self.W = self.add_weight(name='W',shape=(input_shape[2], input_shape[2]),
--                                      initializer=Initializers.Constant(1.0/input_shape[2]),
--                                      regularizer=self.attention_regularizer,
--                                      constraint=self.attention_constraint,
--                                      trainable=True)
--        
--        self.alpha = self.add_weight(name='alpha',shape=(1,),
--                                      initializer=Initializers.Constant(0.5),
--                                      constraint=MinMax(),
--                                      trainable=True)
--
--
--        self.bias=self.add_weight(name='bias',shape=(1, self.output_dim[0],self.output_dim[1]),
--                              initializer='zeros',trainable=True)
--        
-+        self.W1 = self.add_weight(name='W1',
-+                                  shape=(input_shape[1], self.output_dim[0]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.projection_regularizer,
-+                                  constraint=self.projection_constraint,
-+                                  trainable=True)
-+
-+        self.W2 = self.add_weight(name='W2',
-+                                  shape=(input_shape[2], self.output_dim[1]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.projection_regularizer,
-+                                  constraint=self.projection_constraint,
-+                                  trainable=True)
-+
-+        self.W = self.add_weight(name='W',
-+                                 shape=(input_shape[2], input_shape[2]),
-+                                 initializer=Initializers.Constant(
-+                                     1.0 / input_shape[2]),
-+                                 regularizer=self.attention_regularizer,
-+                                 constraint=self.attention_constraint,
-+                                 trainable=True)
-+
-+        self.alpha = self.add_weight(name='alpha',
-+                                     shape=(1, ),
-+                                     initializer=Initializers.Constant(0.5),
-+                                     constraint=MinMax(),
-+                                     trainable=True)
-+
-+        self.bias = self.add_weight(name='bias',
-+                                    shape=(1, self.output_dim[0],
-+                                           self.output_dim[1]),
-+                                    initializer='zeros',
-+                                    trainable=True)
-+
-         self.in_shape = input_shape
--        
-+
-         super(TABL, self).build(input_shape)
- 
-     def call(self, x):
-@@ -174,27 +191,26 @@ class TABL(Layer):
-         W: D2 x D2
-         """
-         # first mode projection
--        x = nmodeproduct(x,self.W1,1) # N x d1 x D2
--        # enforcing constant (1) on the diagonal 
--        W = self.W-self.W*K.eye(self.in_shape[2],dtype='float32')+K.eye(self.in_shape[2],dtype='float32')/self.in_shape[2]
--        # calculate attention 
--        attention = Activations.softmax(nmodeproduct(x,W,2),axis=-1) # N x d1 x D2
-+        x = nmodeproduct(x, self.W1, 1)  # N x d1 x D2
-+        # enforcing constant (1) on the diagonal
-+        W = self.W - self.W * K.eye(self.in_shape[2], dtype='float32') + K.eye(
-+            self.in_shape[2], dtype='float32') / self.in_shape[2]
-+        # calculate attention
-+        attention = Activations.softmax(nmodeproduct(x, W, 2),
-+                                        axis=-1)  # N x d1 x D2
-         # apply attention
--        x = self.alpha*x + (1.0 - self.alpha)*x*attention
-+        x = self.alpha * x + (1.0 - self.alpha) * x * attention
-         # second mode projection
--        x = nmodeproduct(x,self.W2,2)
-+        x = nmodeproduct(x, self.W2, 2)
-         # bias add
-         x = x + self.bias
--        
--        if self.output_dim[1]==1:
--            x = K.squeeze(x,axis=-1)
-+
-+        if self.output_dim[1] == 1:
-+            x = K.squeeze(x, axis=-1)
-         return x
- 
-     def compute_output_shape(self, input_shape):
--        if self.output_dim[1]==1:
-+        if self.output_dim[1] == 1:
-             return (input_shape[0], self.output_dim[0])
-         else:
-             return (input_shape[0], self.output_dim[0], self.output_dim[1])
--
--
--
-diff --git a/third_party_libraries/TABL/Models.py b/third_party_libraries/TABL/Models.py
-index e2bf62c..d5c4c45 100644
---- a/third_party_libraries/TABL/Models.py
-+++ b/third_party_libraries/TABL/Models.py
-@@ -4,7 +4,6 @@
- @author: Dat Tran (dat.tranthanh@tut.fi)
- """
- 
--
- from third_party_libraries.TABL import Layers
- import keras
- 
-@@ -74,7 +73,8 @@ def TABL(
- 
-     x = inputs
-     for k in range(1, len(template) - 1):
--        x = Layers.BL(template[k], projection_regularizer, projection_constraint)(x)
-+        x = Layers.BL(template[k], projection_regularizer,
-+                      projection_constraint)(x)
-         x = keras.layers.Activation("relu")(x)
-         x = keras.layers.Dropout(dropout)(x)
- 
-@@ -94,4 +94,3 @@ def TABL(
-     model.compile(optimizer, "categorical_crossentropy", ["acc"])
- 
-     return model
--
-diff --git a/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc
-new file mode 100644
-index 0000000..ff0a753
-Binary files /dev/null and b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc differ
-diff --git a/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc
-new file mode 100644
-index 0000000..a57f1ba
-Binary files /dev/null and b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc differ
-diff --git a/yaml/tabl.yaml b/yaml/tabl.yaml
-index 2cdc7b2..456e287 100644
---- a/yaml/tabl.yaml
-+++ b/yaml/tabl.yaml
-@@ -4,4 +4,14 @@ epochs:
-   value: 100
- batch_size:
-   desc: Size of each mini-batch
--  value: 32
-+  value: 256
-+window_length:
-+  desc: null
-+  value: 200
-+num_features:
-+  desc: null
-+  value: 40
-+dropout:
-+  desc: null
-+  value: 0.1
-+
diff --git a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/media/graph/graph_summary_6f818560.graph.json b/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/media/graph/graph_summary_6f818560.graph.json
deleted file mode 100644
index fcd5b7c..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/media/graph/graph_summary_6f818560.graph.json
+++ /dev/null
@@ -1 +0,0 @@
-{"format": "keras", "nodes": [{"name": "input_1", "id": "input_1", "class_name": "InputLayer", "output_shape": [null, 40, 200], "num_parameters": 0}, {"name": "bl_1", "id": "bl_1", "class_name": "BL", "output_shape": [null, 60, 10], "num_parameters": 5000}, {"name": "activation_1", "id": "activation_1", "class_name": "Activation", "output_shape": [null, 60, 10], "num_parameters": 0}, {"name": "dropout_1", "id": "dropout_1", "class_name": "Dropout", "output_shape": [null, 60, 10], "num_parameters": 0}, {"name": "bl_2", "id": "bl_2", "class_name": "BL", "output_shape": [null, 120, 5], "num_parameters": 7850}, {"name": "activation_2", "id": "activation_2", "class_name": "Activation", "output_shape": [null, 120, 5], "num_parameters": 0}, {"name": "dropout_2", "id": "dropout_2", "class_name": "Dropout", "output_shape": [null, 120, 5], "num_parameters": 0}, {"name": "tabl_1", "id": "tabl_1", "class_name": "TABL", "output_shape": [null, 3], "num_parameters": 394}, {"name": "activation_3", "id": "activation_3", "class_name": "Activation", "output_shape": [null, 3], "num_parameters": 0}], "edges": [["input_1", "bl_1"], ["bl_1", "activation_1"], ["activation_1", "dropout_1"], ["dropout_1", "bl_2"], ["bl_2", "activation_2"], ["activation_2", "dropout_2"], ["dropout_2", "tabl_1"], ["tabl_1", "activation_3"]]}
\ No newline at end of file
diff --git a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/model-best.h5 b/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/model-best.h5
deleted file mode 100644
index 2748002..0000000
Binary files a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/model-best.h5 and /dev/null differ
diff --git a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/model.h5 b/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/model.h5
deleted file mode 100644
index 3b73ad8..0000000
Binary files a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/model.h5 and /dev/null differ
diff --git a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/requirements.txt b/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/requirements.txt
deleted file mode 100644
index c4244a7..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/requirements.txt
+++ /dev/null
@@ -1,339 +0,0 @@
-absl-py==0.9.0
-alabaster==0.7.12
-anaconda-client==1.7.2
-anaconda-project==0.8.3
-appdirs==1.4.3
-argparse==1.4.0
-asn1crypto==1.3.0
-astor==0.8.1
-astroid==2.3.3
-astropy==4.0
-asyncore-wsgi==0.0.9
-atomicwrites==1.3.0
-attrs==19.3.0
-autobahn==20.2.1
-autoflake==1.3.1
-automat==0.8.0
-babel==2.8.0
-backcall==0.1.0
-backports.os==0.1.1
-backports.shutil-get-terminal-size==1.0.0
-beautifulsoup4==4.8.2
-bitarray==1.2.1
-bkcharts==0.2
-black==19.10b0
-bleach==3.1.0
-blis==0.4.1
-bokeh==1.4.0
-boto3==1.12.37
-boto==2.49.0
-botocore==1.15.37
-bottle==0.12.18
-bottleneck==1.3.1
-cachetools==4.0.0
-catalogue==1.0.0
-cchardet==2.1.6
-certifi==2019.11.28
-cffi==1.13.2
-chardet==3.0.4
-click==7.0
-cloudpickle==1.2.2
-clyent==1.2.2
-colorama==0.4.3
-configparser==5.0.0
-constantly==15.1.0
-contextlib2==0.6.0.post1
-cryptography==2.8
-cvxpy==1.0.31
-cycler==0.10.0
-cymem==2.0.3
-cython==0.29.14
-cytoolz==0.10.1
-dask==2.10.1
-dateparser==0.7.2
-decorator==4.4.1
-defusedxml==0.6.0
-dill==0.3.1.1
-distributed==2.10.0
-docker-pycreds==0.4.0
-docutils==0.15.2
-easyprocess==0.2.10
-ecos==2.0.7.post1
-entrypoints==0.3
-et-xmlfile==1.0.1
-fastai2==0.0.17
-fastcache==1.1.0
-fastcore==0.1.17
-fastprogress==0.2.2
-filelock==3.0.12
-flake8==3.7.9
-flask==1.1.1
-fsspec==0.6.2
-future==0.18.2
-gast==0.2.2
-gevent==1.4.0
-gitdb==4.0.5
-gitpython==3.1.2
-glob2==0.7
-gmpy2==2.0.8
-google-api-core==1.16.0
-google-api-python-client==1.8.0
-google-auth-httplib2==0.0.3
-google-auth-oauthlib==0.4.1
-google-auth==1.13.1
-google-cloud-profiler==1.0.9
-google-pasta==0.1.8
-googleapis-common-protos==1.51.0
-gql==0.2.0
-graphql-core==1.1
-greenlet==0.4.15
-grpcio==1.27.1
-h5py==2.10.0
-hanging-threads==2.0.5
-heapdict==1.0.1
-html5lib==1.0.1
-httplib2==0.17.2
-hyperlink==19.0.0
-hypothesis==5.4.1
-idna==2.8
-imageio==2.6.1
-imagesize==1.2.0
-imbalanced-learn==0.6.1
-imblearn==0.0
-importlib-metadata==1.5.0
-incremental==17.5.0
-inflect==4.1.0
-ipykernel==5.1.4
-ipython-genutils==0.2.0
-ipython==7.12.0
-ipywidgets==7.5.1
-isort==4.3.21
-itsdangerous==1.1.0
-jaraco.itertools==5.0.0
-jdcal==1.4.1
-jedi==0.16.0
-jeepney==0.4.2
-jieba==0.42.1
-jinja2==2.11.1
-jmespath==0.9.5
-joblib==0.14.1
-json5==0.9.0
-jsonschema==3.2.0
-jupyter-client==5.3.4
-jupyter-console==6.1.0
-jupyter-core==4.6.1
-jupyter==1.0.0
-jupyterlab-server==1.0.6
-jupyterlab==1.2.6
-keras-applications==1.0.8
-keras-bert==0.81.0
-keras-embed-sim==0.7.0
-keras-layer-normalization==0.14.0
-keras-multi-head==0.22.0
-keras-pos-embd==0.11.0
-keras-position-wise-feed-forward==0.6.0
-keras-preprocessing==1.1.0
-keras-self-attention==0.41.0
-keras-transformer==0.32.0
-keras==2.3.1
-keyring==21.1.0
-kiwisolver==1.1.0
-ktrain==0.12.3
-langdetect==1.0.8
-lazy-object-proxy==1.4.3
-libarchive-c==2.8
-lief==0.9.0
-llvmlite==0.31.0
-locket==0.2.0
-lxml==4.5.0
-markdown==3.2
-markupsafe==1.1.1
-matplotlib==3.1.3
-mccabe==0.6.1
-mistune==0.8.4
-mkl-fft==1.0.15
-mkl-random==1.1.0
-mkl-service==2.3.0
-mock==3.0.5
-more-itertools==8.2.0
-mpmath==1.1.0
-msgpack==1.0.0
-multipledispatch==0.6.0
-multiprocess==0.70.9
-murmurhash==1.0.2
-nbconvert==5.6.1
-nbformat==5.0.4
-networkx==2.4
-nltk==3.4.5
-nose==1.3.7
-notebook==6.0.3
-numba==0.48.0
-numexpr==2.7.1
-numpy==1.18.1
-numpydoc==0.9.2
-nvidia-ml-py3==7.352.0
-oauthlib==3.1.0
-olefile==0.46
-opencv-python==4.2.0.34
-openpyxl==3.0.3
-opt-einsum==3.1.0
-osqp==0.6.1
-packaging==20.1
-pandas==1.0.3
-pandocfilters==1.4.2
-parse==1.6.6
-parso==0.6.0
-partd==1.1.0
-path==13.1.0
-pathlib2==2.3.5
-pathspec==0.7.0
-pathtools==0.1.2
-patsy==0.5.1
-pep8==1.7.1
-pexpect==4.8.0
-pickleshare==0.7.5
-pillow==7.0.0
-pip==20.0.2
-pkginfo==1.5.0.1
-plac==1.1.3
-pluggy==0.13.1
-ply==3.11
-preshed==3.0.2
-prometheus-client==0.7.1
-promise==2.3
-prompt-toolkit==3.0.3
-protobuf==3.11.3
-psutil==5.6.7
-psycopg2-binary==2.8.4
-ptyprocess==0.6.0
-py==1.8.1
-pyarrow==0.16.0
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycodestyle==2.5.0
-pycosat==0.6.3
-pycparser==2.19
-pycrypto==2.6.1
-pycurl==7.43.0.5
-pydbus==0.6.0
-pyflakes==2.1.1
-pygments==2.5.2
-pyhamcrest==2.0.0
-pylint==2.4.4
-pyodbc==4.0.28
-pyopenssl==19.1.0
-pyparsing==2.4.6
-pyrsistent==0.15.7
-pysocks==1.7.1
-pytest-arraydiff==0.3
-pytest-astropy-header==0.1.2
-pytest-astropy==0.7.0
-pytest-doctestplus==0.5.0
-pytest-openfiles==0.4.0
-pytest-remotedata==0.3.2
-pytest==5.3.5
-python-dateutil==2.8.1
-pyts==0.10.0
-pytz==2019.3
-pyunpack==0.1.2
-pywavelets==1.1.1
-pyyaml==5.3
-pyzmq==18.1.1
-qtawesome==0.6.1
-qtconsole==4.6.0
-qtpy==1.9.0
-regex==2020.1.8
-requests-oauthlib==1.3.0
-requests==2.22.0
-rope==0.16.0
-rsa==4.0
-ruamel-yaml==0.15.87
-s3transfer==0.3.3
-sacremoses==0.0.38
-scikit-image==0.16.2
-scikit-learn==0.22.1
-scipy==1.4.1
-scoop==0.7.1.1
-scs==2.1.2
-seaborn==0.10.0
-secretstorage==3.1.2
-send2trash==1.5.0
-sentencepiece==0.1.85
-sentry-sdk==0.14.3
-seqeval==0.0.12
-service-identity==18.1.0
-setuptools==45.1.0.post20200127
-shortuuid==1.0.1
-simplegeneric==0.8.1
-singledispatch==3.4.0.3
-six==1.14.0
-smmap==3.0.4
-snowballstemmer==2.0.0
-sortedcollections==1.1.2
-sortedcontainers==2.1.0
-soupsieve==1.9.5
-spacy==2.2.3
-sphinx==2.3.1
-sphinxcontrib-applehelp==1.0.1
-sphinxcontrib-devhelp==1.0.1
-sphinxcontrib-htmlhelp==1.0.2
-sphinxcontrib-jsmath==1.0.1
-sphinxcontrib-qthelp==1.0.2
-sphinxcontrib-serializinghtml==1.1.3
-sphinxcontrib-websupport==1.1.2
-spyder-kernels==0.5.2
-spyder==3.3.6
-sqlalchemy==1.3.13
-srsly==1.0.1
-statsmodels==0.11.0
-subprocess32==3.5.4
-sympy==1.5.1
-syntok==1.2.2
-tables==3.6.1
-tblib==1.6.0
-tensorboard==2.1.1
-tensorflow-datasets==2.1.0
-tensorflow-estimator==2.1.0
-tensorflow-metadata==0.21.1
-tensorflow==2.1.0
-termcolor==1.1.0
-terminado==0.8.3
-testpath==0.4.4
-thinc==7.3.1
-tokenizers==0.5.2
-toml==0.10.0
-toolz==0.10.0
-torch==1.4.0
-torchvision==0.5.0
-tornado==6.0.3
-tqdm==4.42.0
-traitlets==4.3.3
-transformers==2.8.0
-twisted==19.10.0
-txaio==20.1.1
-typed-ast==1.4.1
-tzlocal==2.0.0
-unicodecsv==0.14.1
-uritemplate==3.0.1
-urllib3==1.25.8
-wandb==0.8.35
-wasabi==0.6.0
-watchdog==0.10.2
-wcwidth==0.1.8
-web-pdb==1.5.1
-webencodings==0.5.1
-websocket-client==0.57.0
-weresync==1.1.5
-werkzeug==0.16.1
-wheel==0.34.2
-widgetsnbextension==3.5.1
-wrapt==1.11.2
-wurlitzer==2.0.0
-xlrd==1.2.0
-xlsxwriter==1.2.7
-xlwt==1.3.0
-yapf==0.29.0
-yapsy==1.11.223
-zict==1.0.0
-zipp==2.1.0
-zope.interface==4.7.1
\ No newline at end of file
diff --git a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/wandb-events.jsonl b/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/wandb-events.jsonl
deleted file mode 100644
index b097cdf..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/wandb-events.jsonl
+++ /dev/null
@@ -1 +0,0 @@
-{"system.cpu": 36.6, "system.memory": 12.12, "system.disk": 23.5, "system.proc.memory.availableMB": 34386.79, "system.proc.memory.rssMB": 750.72, "system.proc.memory.percent": 1.92, "system.proc.cpu.threads": 33.18, "system.network.sent": 502001, "system.network.recv": 177783, "_wandb": true, "_timestamp": 1589165788, "_runtime": 19}
diff --git a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/wandb-history.jsonl b/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/wandb-history.jsonl
deleted file mode 100644
index d6bb4d7..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/wandb-history.jsonl
+++ /dev/null
@@ -1,100 +0,0 @@
-{"epoch": 0, "val_loss": NaN, "val_accuracy": 0.40234375, "loss": 2.859574794769287, "accuracy": 0.349609375, "_runtime": 4.397034168243408, "_timestamp": 1589165773.4107883, "_step": 0}
-{"epoch": 1, "val_loss": 0.718055009841919, "val_accuracy": 0.75390625, "loss": 2.497468590736389, "accuracy": 0.3359375, "_runtime": 4.5527026653289795, "_timestamp": 1589165773.5664568, "_step": 1}
-{"epoch": 2, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.35546875, "_runtime": 4.757652282714844, "_timestamp": 1589165773.7714064, "_step": 2}
-{"epoch": 3, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 4.877859115600586, "_timestamp": 1589165773.8916132, "_step": 3}
-{"epoch": 4, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.042803525924683, "_timestamp": 1589165774.0565577, "_step": 4}
-{"epoch": 5, "val_loss": NaN, "val_accuracy": 0.5859375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.168975114822388, "_timestamp": 1589165774.1827292, "_step": 5}
-{"epoch": 6, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.2804625034332275, "_timestamp": 1589165774.2942166, "_step": 6}
-{"epoch": 7, "val_loss": NaN, "val_accuracy": 0.5859375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.4512553215026855, "_timestamp": 1589165774.4650095, "_step": 7}
-{"epoch": 8, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.57910418510437, "_timestamp": 1589165774.5928583, "_step": 8}
-{"epoch": 9, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.697619915008545, "_timestamp": 1589165774.711374, "_step": 9}
-{"epoch": 10, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.879847049713135, "_timestamp": 1589165774.8936012, "_step": 10}
-{"epoch": 11, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.003344297409058, "_timestamp": 1589165775.0170984, "_step": 11}
-{"epoch": 12, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.170013189315796, "_timestamp": 1589165775.1837673, "_step": 12}
-{"epoch": 13, "val_loss": NaN, "val_accuracy": 0.5859375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.291700601577759, "_timestamp": 1589165775.3054547, "_step": 13}
-{"epoch": 14, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.403789281845093, "_timestamp": 1589165775.4175434, "_step": 14}
-{"epoch": 15, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.592066526412964, "_timestamp": 1589165775.6058207, "_step": 15}
-{"epoch": 16, "val_loss": NaN, "val_accuracy": 0.57421875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.714038372039795, "_timestamp": 1589165775.7277925, "_step": 16}
-{"epoch": 17, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.893545389175415, "_timestamp": 1589165775.9072995, "_step": 17}
-{"epoch": 18, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.030042409896851, "_timestamp": 1589165776.0437965, "_step": 18}
-{"epoch": 19, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.208301782608032, "_timestamp": 1589165776.222056, "_step": 19}
-{"epoch": 20, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.324555158615112, "_timestamp": 1589165776.3383093, "_step": 20}
-{"epoch": 21, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.498993158340454, "_timestamp": 1589165776.5127473, "_step": 21}
-{"epoch": 22, "val_loss": NaN, "val_accuracy": 0.57421875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.615614891052246, "_timestamp": 1589165776.629369, "_step": 22}
-{"epoch": 23, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.805122375488281, "_timestamp": 1589165776.8188765, "_step": 23}
-{"epoch": 24, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.937185049057007, "_timestamp": 1589165776.9509392, "_step": 24}
-{"epoch": 25, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 8.115554571151733, "_timestamp": 1589165777.1293087, "_step": 25}
-{"epoch": 26, "val_loss": NaN, "val_accuracy": 0.57421875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.232769250869751, "_timestamp": 1589165777.2465234, "_step": 26}
-{"epoch": 27, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.411670446395874, "_timestamp": 1589165777.4254246, "_step": 27}
-{"epoch": 28, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.555281162261963, "_timestamp": 1589165777.5690353, "_step": 28}
-{"epoch": 29, "val_loss": NaN, "val_accuracy": 0.85546875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.699794054031372, "_timestamp": 1589165777.7135482, "_step": 29}
-{"epoch": 30, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.854174137115479, "_timestamp": 1589165777.8679283, "_step": 30}
-{"epoch": 31, "val_loss": NaN, "val_accuracy": 0.82421875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.97042989730835, "_timestamp": 1589165777.984184, "_step": 31}
-{"epoch": 32, "val_loss": NaN, "val_accuracy": 0.61328125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.119382858276367, "_timestamp": 1589165778.133137, "_step": 32}
-{"epoch": 33, "val_loss": NaN, "val_accuracy": 0.69921875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.249987363815308, "_timestamp": 1589165778.2637415, "_step": 33}
-{"epoch": 34, "val_loss": NaN, "val_accuracy": 0.85546875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.366937637329102, "_timestamp": 1589165778.3806918, "_step": 34}
-{"epoch": 35, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.610982418060303, "_timestamp": 1589165778.6247365, "_step": 35}
-{"epoch": 36, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.769228458404541, "_timestamp": 1589165778.7829826, "_step": 36}
-{"epoch": 37, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.913050651550293, "_timestamp": 1589165778.9268048, "_step": 37}
-{"epoch": 38, "val_loss": NaN, "val_accuracy": 0.70703125, "loss": NaN, "accuracy": 0.3671875, "_runtime": 10.0595383644104, "_timestamp": 1589165779.0732925, "_step": 38}
-{"epoch": 39, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 10.17861294746399, "_timestamp": 1589165779.192367, "_step": 39}
-{"epoch": 40, "val_loss": NaN, "val_accuracy": 0.70703125, "loss": NaN, "accuracy": 0.3671875, "_runtime": 10.347944259643555, "_timestamp": 1589165779.3616984, "_step": 40}
-{"epoch": 41, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 10.507499933242798, "_timestamp": 1589165779.521254, "_step": 41}
-{"epoch": 42, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 10.65637493133545, "_timestamp": 1589165779.670129, "_step": 42}
-{"epoch": 43, "val_loss": NaN, "val_accuracy": 0.70703125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 10.874603271484375, "_timestamp": 1589165779.8883574, "_step": 43}
-{"epoch": 44, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 11.017180919647217, "_timestamp": 1589165780.030935, "_step": 44}
-{"epoch": 45, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 11.181081295013428, "_timestamp": 1589165780.1948354, "_step": 45}
-{"epoch": 46, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 11.325828075408936, "_timestamp": 1589165780.3395822, "_step": 46}
-{"epoch": 47, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 11.459490537643433, "_timestamp": 1589165780.4732447, "_step": 47}
-{"epoch": 48, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 11.62859320640564, "_timestamp": 1589165780.6423473, "_step": 48}
-{"epoch": 49, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 11.75622296333313, "_timestamp": 1589165780.769977, "_step": 49}
-{"epoch": 50, "val_loss": NaN, "val_accuracy": 0.703125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 11.917980670928955, "_timestamp": 1589165780.9317348, "_step": 50}
-{"epoch": 51, "val_loss": NaN, "val_accuracy": 0.578125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 12.04407787322998, "_timestamp": 1589165781.057832, "_step": 51}
-{"epoch": 52, "val_loss": NaN, "val_accuracy": 0.5703125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 12.177000284194946, "_timestamp": 1589165781.1907544, "_step": 52}
-{"epoch": 53, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 12.34967851638794, "_timestamp": 1589165781.3634326, "_step": 53}
-{"epoch": 54, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 12.466973066329956, "_timestamp": 1589165781.4807272, "_step": 54}
-{"epoch": 55, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 12.64769697189331, "_timestamp": 1589165781.661451, "_step": 55}
-{"epoch": 56, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 12.77220892906189, "_timestamp": 1589165781.785963, "_step": 56}
-{"epoch": 57, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 12.94745135307312, "_timestamp": 1589165781.9612055, "_step": 57}
-{"epoch": 58, "val_loss": NaN, "val_accuracy": 0.703125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 13.071347713470459, "_timestamp": 1589165782.0851018, "_step": 58}
-{"epoch": 59, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 13.200873613357544, "_timestamp": 1589165782.2146277, "_step": 59}
-{"epoch": 60, "val_loss": NaN, "val_accuracy": 0.5703125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 13.358224153518677, "_timestamp": 1589165782.3719783, "_step": 60}
-{"epoch": 61, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 13.484898567199707, "_timestamp": 1589165782.4986527, "_step": 61}
-{"epoch": 62, "val_loss": NaN, "val_accuracy": 0.578125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 13.672421932220459, "_timestamp": 1589165782.686176, "_step": 62}
-{"epoch": 63, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 13.794801712036133, "_timestamp": 1589165782.8085558, "_step": 63}
-{"epoch": 64, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.3671875, "_runtime": 13.968115329742432, "_timestamp": 1589165782.9818695, "_step": 64}
-{"epoch": 65, "val_loss": NaN, "val_accuracy": 0.78515625, "loss": NaN, "accuracy": 0.37109375, "_runtime": 14.08385705947876, "_timestamp": 1589165783.0976112, "_step": 65}
-{"epoch": 66, "val_loss": NaN, "val_accuracy": 0.5703125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 14.202195882797241, "_timestamp": 1589165783.21595, "_step": 66}
-{"epoch": 67, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 14.390702724456787, "_timestamp": 1589165783.4044569, "_step": 67}
-{"epoch": 68, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 14.517786264419556, "_timestamp": 1589165783.5315404, "_step": 68}
-{"epoch": 69, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 14.697016477584839, "_timestamp": 1589165783.7107706, "_step": 69}
-{"epoch": 70, "val_loss": NaN, "val_accuracy": 0.578125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 14.81390118598938, "_timestamp": 1589165783.8276553, "_step": 70}
-{"epoch": 71, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 14.990318536758423, "_timestamp": 1589165784.0040727, "_step": 71}
-{"epoch": 72, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 15.123985528945923, "_timestamp": 1589165784.1377397, "_step": 72}
-{"epoch": 73, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 15.240610361099243, "_timestamp": 1589165784.2543645, "_step": 73}
-{"epoch": 74, "val_loss": NaN, "val_accuracy": 0.578125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 15.408331871032715, "_timestamp": 1589165784.422086, "_step": 74}
-{"epoch": 75, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 15.53018069267273, "_timestamp": 1589165784.5439348, "_step": 75}
-{"epoch": 76, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 15.723777532577515, "_timestamp": 1589165784.7375317, "_step": 76}
-{"epoch": 77, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 15.838160991668701, "_timestamp": 1589165784.8519151, "_step": 77}
-{"epoch": 78, "val_loss": NaN, "val_accuracy": 0.578125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 16.031083583831787, "_timestamp": 1589165785.0448377, "_step": 78}
-{"epoch": 79, "val_loss": NaN, "val_accuracy": 0.5859375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 16.149405002593994, "_timestamp": 1589165785.1631591, "_step": 79}
-{"epoch": 80, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 16.34160614013672, "_timestamp": 1589165785.3553603, "_step": 80}
-{"epoch": 81, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 16.477399587631226, "_timestamp": 1589165785.4911537, "_step": 81}
-{"epoch": 82, "val_loss": NaN, "val_accuracy": 0.5703125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 16.642279624938965, "_timestamp": 1589165785.6560338, "_step": 82}
-{"epoch": 83, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 16.763620615005493, "_timestamp": 1589165785.7773747, "_step": 83}
-{"epoch": 84, "val_loss": NaN, "val_accuracy": 0.578125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 16.953686475753784, "_timestamp": 1589165785.9674406, "_step": 84}
-{"epoch": 85, "val_loss": NaN, "val_accuracy": 0.83984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 17.07484769821167, "_timestamp": 1589165786.0886018, "_step": 85}
-{"epoch": 86, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 17.25295066833496, "_timestamp": 1589165786.2667048, "_step": 86}
-{"epoch": 87, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 17.366127252578735, "_timestamp": 1589165786.3798814, "_step": 87}
-{"epoch": 88, "val_loss": NaN, "val_accuracy": 0.5703125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 17.566930055618286, "_timestamp": 1589165786.5806842, "_step": 88}
-{"epoch": 89, "val_loss": NaN, "val_accuracy": 0.83984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 17.689023971557617, "_timestamp": 1589165786.702778, "_step": 89}
-{"epoch": 90, "val_loss": NaN, "val_accuracy": 0.578125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 17.852903842926025, "_timestamp": 1589165786.866658, "_step": 90}
-{"epoch": 91, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 17.975466012954712, "_timestamp": 1589165786.9892201, "_step": 91}
-{"epoch": 92, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 18.09291434288025, "_timestamp": 1589165787.1066685, "_step": 92}
-{"epoch": 93, "val_loss": NaN, "val_accuracy": 0.83984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 18.264099836349487, "_timestamp": 1589165787.277854, "_step": 93}
-{"epoch": 94, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 18.382726669311523, "_timestamp": 1589165787.3964808, "_step": 94}
-{"epoch": 95, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 18.500207662582397, "_timestamp": 1589165787.5139618, "_step": 95}
-{"epoch": 96, "val_loss": NaN, "val_accuracy": 0.5703125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 18.672690391540527, "_timestamp": 1589165787.6864445, "_step": 96}
-{"epoch": 97, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 18.791967391967773, "_timestamp": 1589165787.8057215, "_step": 97}
-{"epoch": 98, "val_loss": NaN, "val_accuracy": 0.5703125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 18.908215761184692, "_timestamp": 1589165787.92197, "_step": 98}
-{"epoch": 99, "val_loss": NaN, "val_accuracy": 0.83984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 19.085763216018677, "_timestamp": 1589165788.0995173, "_step": 99}
diff --git a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/wandb-metadata.json b/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/wandb-metadata.json
deleted file mode 100644
index 5d01db0..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/wandb-metadata.json
+++ /dev/null
@@ -1,22 +0,0 @@
-{
-    "root": "/home/garthtrickett/ProdigyAI",
-    "program": "tabl.py",
-    "git": {
-        "remote": "https://github.com/garthtrickett/ProdigyAI",
-        "commit": "7667fefed2018a0664418c25c97e3c08866a6f71"
-    },
-    "email": "garthtrickett@gmail.com",
-    "startedAt": "2020-05-11T02:56:09.276001",
-    "host": "preemp-cpu-1",
-    "username": "garthtrickett",
-    "executable": "/home/garthtrickett/miniconda3/envs/ProdigyAI/bin/python",
-    "os": "Linux-5.3.0-1018-gcp-x86_64-with-debian-buster-sid",
-    "python": "3.7.6",
-    "cpu_count": 6,
-    "args": [],
-    "state": "running",
-    "jobType": null,
-    "mode": "run",
-    "project": "prodigyai",
-    "heartbeatAt": "2020-05-11T02:56:26.051093"
-}
diff --git a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/wandb-summary.json b/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/wandb-summary.json
deleted file mode 100644
index dcec69b..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025609-2r8v7bep/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"graph": {"_type": "graph-file", "path": "media/graph/graph_summary_6f818560.graph.json", "sha256": "6f81856020e576034c23528bd5c8cc156872ed948db82ecb89f721130ae37682", "size": 1318}, "loss": NaN, "_step": 99, "val_loss": NaN, "accuracy": 0.37109375, "epoch": 99, "_timestamp": 1589165788.0995173, "val_accuracy": 0.83984375, "_runtime": 19.085763216018677, "best_loss": 2.497468590736389, "best_epoch": 1}
diff --git a/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/config.yaml b/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/config.yaml
deleted file mode 100644
index 8c1a8a9..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/config.yaml
+++ /dev/null
@@ -1,34 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.8.35
-    framework: keras
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.7.6
-dataset:
-  desc: null
-  value: esugj36b.h5
-params:
-  desc: null
-  value:
-    batch_size:
-      desc: Size of each mini-batch
-      value: 256
-    dropout:
-      desc: null
-      value: 0.1
-    epochs:
-      desc: Number of epochs to train over
-      value: 100
-    num_features:
-      desc: null
-      value: 40
-    window_length:
-      desc: null
-      value: 200
-yaml:
-  desc: null
-  value: yaml/tabl.yaml
diff --git a/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/diff.patch b/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/diff.patch
deleted file mode 100644
index 214be23..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/diff.patch
+++ /dev/null
@@ -1,1270 +0,0 @@
-diff --git a/pipeline/deeplob.py b/pipeline/deeplob.py
-index 2d6c076..fef42d4 100644
---- a/pipeline/deeplob.py
-+++ b/pipeline/deeplob.py
-@@ -2,6 +2,7 @@
- import pandas as pd
- import pickle
- import numpy as np
-+import tensorflow
- import tensorflow as tf
- from tensorflow.keras import layers
- 
-@@ -25,21 +26,17 @@ from keras.utils import np_utils
- import matplotlib.pyplot as plt
- import math
- from numba import njit, prange
--
--# Init wandb
--import wandb
--from wandb.keras import WandbCallback
--wandb.init(project="prodigyai")
-+import yaml
- 
- # set random seeds
- np.random.seed(1)
- tf.random.set_seed(2)
- 
- import keras
--from keras.callbacks import ModelCheckpoint
--from keras import backend as K
-+from tensorflow.keras.callbacks import ModelCheckpoint
- import h5py
--
-+import wandb
-+from wandb.keras import WandbCallback
- # check if using gpu
- gpus = tf.config.list_physical_devices()
- any_gpus = [s for s in gpus if "GPU" in s[0]]
-@@ -105,6 +102,23 @@ if cwd == home + "/":
-     cwd = cwd + "/ProdigyAI"
-     path_adjust = cwd
- 
-+# Init wandb
-+yaml_path = path_adjust + "yaml/deeplob.yaml"
-+with open(yaml_path) as file:
-+    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
-+
-+config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
-+wandb.init(dir="~/ProdigyAI/",
-+           project="prodigyai",
-+           config=config_dictionary,
-+           resume=True)
-+
-+window_length = wandb.config['params']['window_length']['value']
-+num_features = wandb.config['params']['num_features']['value']
-+epochs = wandb.config['params']['epochs']['value']
-+batch_size = wandb.config['params']['batch_size']['value']
-+number_of_lstm = wandb.config['params']['number_of_lstm']['value']
-+
- # limit gpu usage for keras
- gpu_devices = tf.config.experimental.list_physical_devices("GPU")
- for device in gpu_devices:
-@@ -216,7 +230,8 @@ import h5py
- 
- home = str(Path.home())
- 
--file_name = "arch=DLOB&name=two_model&WL=200&pt=1&sl=1&min_ret=4.347826086956522e-05&vbs=0.5&head=0&skip=0&vol_max=4.3498260869565224e-05&vol_min=4.348826086956522e-05&filter=none&cm_vol_mod=0&sw=on&fd=off&input=obook&ntb=True&tslbc=True.h5"
-+file_name = "esugj36b.h5"
-+wandb.config.update({'dataset': file_name})
- path = home + "/ProdigyAI/data/preprocessed/" + file_name
- h5f = h5py.File(path, "r")
- prices_for_window_index_array_train = h5f[
-@@ -268,6 +283,7 @@ def generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data):
- 
- class DataGenerator(tf.compat.v2.keras.utils.Sequence):
-     def __init__(self,
-+                 checkpoint_path,
-                  prices_for_window_index_array,
-                  input_features_normalized,
-                  y_data,
-@@ -277,6 +293,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
-                  to_fit,
-                  shuffle=True):
-         self.batch_size = batch_size
-+        self.checkpoint_path = checkpoint_path
-         self.prices_for_window_index_array = prices_for_window_index_array
-         self.input_features_normalized = input_features_normalized
-         self.labels = y_data
-@@ -323,6 +340,8 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
-     def on_epoch_end(self):
- 
-+        wandb.save(self.checkpoint_path)
-+
-         self.indexes = np.arange(len(self.prices_for_window_index_array))
- 
-         if self.shuffle:
-@@ -351,8 +370,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
- 
- n_classes = 3
--dim = (200, 40)
--batch_size = 64
-+dim = (window_length, num_features)
- to_fit = True
- 
- # for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,
-@@ -371,70 +389,6 @@ to_fit = True
- 
- ## PRODIGY AI HOCKUS POCKUS END
- 
--
--### Model Architecture
--def create_deeplob(T, NF, number_of_lstm):
--    input_lmd = Input(shape=(T, NF, 1))
--
--    # build the convolutional block
--    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    conv_first1 = Conv2D(32, (1, 10))(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    # build the inception module
--    convsecond_1 = Conv2D(64, (1, 1), padding="same")(conv_first1)
--    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
--    convsecond_1 = Conv2D(64, (3, 1), padding="same")(convsecond_1)
--    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
--
--    convsecond_2 = Conv2D(64, (1, 1), padding="same")(conv_first1)
--    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
--    convsecond_2 = Conv2D(64, (5, 1), padding="same")(convsecond_2)
--    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
--
--    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),
--                                padding="same")(conv_first1)
--    convsecond_3 = Conv2D(64, (1, 1), padding="same")(convsecond_3)
--    convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)
--
--    convsecond_output = concatenate([convsecond_1, convsecond_2, convsecond_3],
--                                    axis=3)
--
--    # use the MC dropout here
--    conv_reshape = Reshape(
--        (int(convsecond_output.shape[1]),
--         int(convsecond_output.shape[3])))(convsecond_output)
--
--    conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)
--
--    # build the output layer
--    out = Dense(3, activation="softmax")(conv_lstm)
--    model = Model(inputs=input_lmd, outputs=out)
--    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
--    model.compile(optimizer=adam,
--                  loss="categorical_crossentropy",
--                  metrics=["accuracy"])
--
--    return model
--
--
- checkpoint_path = path_adjust + "temp/cp.ckpt"
- # Create a callback that saves the model's weights
- cp_callback = ModelCheckpoint(filepath=checkpoint_path,
-@@ -446,9 +400,8 @@ if check_point_file.exists() and resuming == "resuming":
-     print("weights loaded")
-     model.load_weights(checkpoint_path)
- 
--batch_size = 64
--
--train_generator = DataGenerator(prices_for_window_index_array_train,
-+train_generator = DataGenerator(checkpoint_path,
-+                                prices_for_window_index_array_train,
-                                 input_features_normalized_train,
-                                 y_train,
-                                 batch_size,
-@@ -457,7 +410,8 @@ train_generator = DataGenerator(prices_for_window_index_array_train,
-                                 to_fit,
-                                 shuffle=True)
- 
--val_generator = DataGenerator(prices_for_window_index_array_val,
-+val_generator = DataGenerator(checkpoint_path,
-+                              prices_for_window_index_array_val,
-                               input_features_normalized_val,
-                               y_val,
-                               batch_size,
-@@ -469,14 +423,87 @@ val_generator = DataGenerator(prices_for_window_index_array_val,
- steps_per_epoch = len(train_generator)
- validation_steps = len(val_generator)
- 
--deeplob = create_deeplob(200, 40, 64)
--deeplob.fit(train_generator,
--            steps_per_epoch=steps_per_epoch,
--            validation_steps=validation_steps,
--            epochs=100,
--            verbose=2,
--            validation_data=val_generator,
--            callbacks=[cp_callback, WandbCallback()])
-+if wandb.run.resumed:
-+    wandb.restore("model-best.h5",run_path="garthtrickett/prodigyai/" + wandb.run.id)
-+    # restore the best model
-+    deeplob = tensorflow.keras.models.load_model(wandb.restore("model-best.h5").name)
-+    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
-+    deeplob.compile(optimizer=adam,
-+                    loss="categorical_crossentropy",
-+                    metrics=["accuracy"])
-+else:
-+    ### Model Architecture
-+    def create_deeplob(T, NF, number_of_lstm):
-+        input_lmd = Input(shape=(T, NF, 1))
-+
-+        # build the convolutional block
-+        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        conv_first1 = Conv2D(32, (1, 10))(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        # build the inception module
-+        convsecond_1 = Conv2D(64, (1, 1), padding="same")(conv_first1)
-+        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
-+        convsecond_1 = Conv2D(64, (3, 1), padding="same")(convsecond_1)
-+        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
-+
-+        convsecond_2 = Conv2D(64, (1, 1), padding="same")(conv_first1)
-+        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
-+        convsecond_2 = Conv2D(64, (5, 1), padding="same")(convsecond_2)
-+        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
-+
-+        convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),
-+                                    padding="same")(conv_first1)
-+        convsecond_3 = Conv2D(64, (1, 1), padding="same")(convsecond_3)
-+        convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)
-+
-+        convsecond_output = concatenate(
-+            [convsecond_1, convsecond_2, convsecond_3], axis=3)
-+
-+        # use the MC dropout here
-+        conv_reshape = Reshape(
-+            (int(convsecond_output.shape[1]),
-+             int(convsecond_output.shape[3])))(convsecond_output)
-+
-+        conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)
-+
-+        # build the output layer
-+        out = Dense(3, activation="softmax")(conv_lstm)
-+        model = Model(inputs=input_lmd, outputs=out)
-+        adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
-+        model.compile(optimizer=adam,
-+                      loss="categorical_crossentropy",
-+                      metrics=["accuracy"])
-+
-+        return model
-+
-+    deeplob = create_deeplob(window_length, num_features, number_of_lstm)
-+deeplob.fit(
-+    train_generator,
-+    steps_per_epoch=steps_per_epoch,
-+    validation_steps=validation_steps,
-+    epochs=100,
-+    verbose=2,
-+    validation_data=val_generator,
-+    callbacks=[cp_callback,
-+               WandbCallback(save_model=True, monitor="loss")])
- 
- finished_weights_path = path_adjust + "temp/cp_end.ckpt"
- deeplob.save_weights(finished_weights_path)
-diff --git a/pipeline/preprocessing.py b/pipeline/preprocessing.py
-index 932ae0b..714d325 100644
---- a/pipeline/preprocessing.py
-+++ b/pipeline/preprocessing.py
-@@ -136,54 +136,67 @@ if stage == 2:
-     side = X_for_all_labels["predicted_bins"]
-     # Could use the probabilities (instead of [1,0,0] use [0.2,0.55,0.25]
- 
-+import yaml
-+import wandb
-+
-+yaml_path = path_adjust + "yaml/preprocessing.yaml"
-+with open(yaml_path) as file:
-+    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
-+
-+config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
-+
-+wandb.init(
-+    dir="~/ProdigyAI/",
-+    project="prodigyai",
-+    config=config_dictionary,
-+)
-+
-+minimum_return = eval(wandb.config['params']['minimum_return']['value'])
-+vertical_barrier_seconds = eval(
-+    wandb.config['params']['vertical_barrier_seconds']['value'])
-+
- # Parameters
- parameters = dict()
--model_arch = "TABL"
--parameters["arch"] = model_arch
--parameters["name"] = model
--parameters["WL"] = 200  # WINDOW LONG
--parameters["pt"] = 1
--parameters["sl"] = 1
--parameters["min_ret"] = 0.001 * 1 / 23
--parameters["vbs"] = round(1 / 2,
--                          3)  # Increasing this decreases vertical touches
--parameters["head"] = 1000  # take only first x number of rows 0 means of
--parameters["skip"] = 0  # sample every n'th row if skip > 0
--# get even classes at fraction = 0.8 so the training set is balanced then set to 1
--# 3 million rows is about the limit of before it starts taking ages / memory maxing
--parameters["vol_max"] = (
--    parameters["min_ret"] + 0.00000002
-+wandb.config['params']['head'][
-+    'value'] = 1000  # take only first x number of rows 0 means of
-+volume_max = (
-+    minimum_return + wandb.config['params']['vol_max_modifier']['value']
- )  # The higher this is the more an increase in volatility requries an increase
- # in return to be considered buy/sell (Increasing this increases end barrier vertical touches)
--parameters["vol_min"] = parameters["min_ret"] + 0.00000001
-+volume_min = minimum_return + wandb.config['params']['vol_min_modifier'][
-+    'value']
- 
--parameters["filter"] = "none"
-+filter_type = wandb.config['params']['filter_type']['value']
- 
--if parameters["filter"] == "cm":
--    parameters["cm_vol_mod"] = 500
-+if filter_type == "cm":
-+    cusum_filter_vol_modifier = wandb.config['params'][
-+        'cusum_filter_volume_modifier']['value']
- else:
--    parameters["cm_vol_mod"] = 0
-+    cusum_filter_vol_modifier = 0
- 
--parameters["sw"] = "on"  # sample weights
--parameters["fd"] = "off"
-+use_sample_weights = wandb.config['params']['use_sample_weights']['value']
-+use_sample_weights = wandb.config['params']['use_fractional_differentiation'][
-+    'value']
- 
--parameters["input"] = "obook"
-+input_type = wandb.config['params']['input_type']['value']
- 
--parameters["ntb"] = True  # non time bars
-+# parameters["ntb"] = True  # non time bars
- 
--if parameters["ntb"] == True:
--    # Pick whether you want to add in the time since last bar input feature
--    # time since last bar column
--    parameters["tslbc"] = True  # time since last bar column
--else:
--    # Pick whether you want to add in the volume input feature
--    parameters["vbc"] = True  # volume bar column
-+# if parameters["ntb"] == True:
-+#     # Pick whether you want to add in the time since last bar input feature
-+#     # time since last bar column
-+#     parameters["tslbc"] = True  # time since last bar column
-+# else:
-+#     # Pick whether you want to add in the volume input feature
-+#     parameters["vbc"] = True  # volume bar column
- 
- # Create the txt file string
--parameter_string = "&".join("{}{}{}".format(key, "=", val)
--                            for key, val in parameters.items())
-+parameter_string = wandb.run.id
- 
--pt_sl = [parameters["pt"], parameters["sl"]]
-+pt_sl = [
-+    wandb.config['params']['profit_taking_multiplier']['value'],
-+    wandb.config['params']['stop_loss_multiplier']['value']
-+]
- cpus = cpu_count() - 1
- 
- regenerate_features_and_labels = True
-@@ -192,10 +205,10 @@ if regenerate_features_and_labels == True:
-     if stage == 1:
-         # Side
-         print("starting data load")
--        head = parameters["head"]
-+        head = wandb.config['params']['head']['value']
- 
-         # # read parquet file of dollar bars
--        if parameters["input"] == "bars":
-+        if input_type == "bars":
-             # Mlfinlab bars
-             data = pq.read_pandas(
-                 path_adjust + "data/bars/"
-@@ -216,7 +229,7 @@ if regenerate_features_and_labels == True:
-             data.index = pd.to_datetime(data.index, infer_datetime_format=True)
- 
-         # read parquet file of raw ticks
--        if parameters["input"] == "ticks":
-+        if input_type == "ticks":
-             data = pq.read_pandas(
-                 path_adjust + "data/bars/" +
-                 "btcusdt_agg_trades_raw_tick_data.parquet").to_pandas()
-@@ -238,11 +251,7 @@ if regenerate_features_and_labels == True:
-             # Should do something else than drop the duplicates (maybe trades doesnt have duplicate indexes rather than aggtrades)
-             data = data.loc[~data.index.duplicated(keep="first")]
- 
--            # skip most rows if this is > 1
--            if parameters["skip"] > 0:
--                data = data.iloc[::parameters["skip"], :]
--
--        if parameters["input"] == "obook":
-+        if input_type == "orderbook":
-             with open(path_adjust + "temp/orderbook_data_name.txt",
-                       "r") as text_file:
-                 orderbook_preprocessed_file_name = text_file.read()
-@@ -275,7 +284,7 @@ if regenerate_features_and_labels == True:
-         # duplicate_fast_search(data.index.duplicated())
- 
-         # Fractional differentiation
--        if parameters["fd"] == "on":
-+        if use_sample_weights == "on":
-             data_series = data["close"].to_frame()
-             # # generate 100 points
-             # nsample = 1000
-@@ -313,16 +322,16 @@ if regenerate_features_and_labels == True:
- 
-         start = time.time()
-         volatility_level_array = volatility_levels_numba(
--            np.ascontiguousarray(data.close.values), parameters["WL"])
-+            np.ascontiguousarray(data.close.values),
-+            wandb.config['params']['window_length']['value'])
-         data["window_volatility_level"] = volatility_level_array
- 
-         # Should adjust the max value
-         # To get more vertical touches we can
-         # either increase vol_max or
-         # decrease the window seconds
--        scaler = MinMaxScaler(
--            feature_range=(parameters["vol_min"],
--                           parameters["vol_max"]))  # normalization
-+        scaler = MinMaxScaler(feature_range=(volume_min,
-+                                             volume_max))  # normalization
- 
-         normed_window_volatility_level = scaler.fit_transform(
-             data[["window_volatility_level"]])
-@@ -338,7 +347,7 @@ if regenerate_features_and_labels == True:
-         close_np_array, close_index_np_array = pandas_series_to_numba_ready_np_arrays(
-             close_copy)
- 
--        volatility_threshold = volatility_threshold * parameters["cm_vol_mod"]
-+        volatility_threshold = volatility_threshold * cusum_filter_vol_modifier
-         print("data_len = " + str(len(data)))
-         start = time.time()
-         sampled_idx = filter_events(
-@@ -346,7 +355,7 @@ if regenerate_features_and_labels == True:
-             close_np_array,
-             close_index_np_array,
-             volatility_threshold,
--            parameters["filter"],
-+            filter_type,
-         )
-         print("sampled_idx_len = " + str(len(sampled_idx)))
-         end = time.time()
-@@ -356,7 +365,8 @@ if regenerate_features_and_labels == True:
-         # size
-         start = time.time()
-         volatility_level_array = volatility_levels_numba(
--            data.close.values, parameters["WL"])
-+            data.close.values,
-+            wandb.config['params']['window_length']['value'])
-         data["window_volatility_level"] = volatility_level_array
- 
-     # This code runs for both first and second stage preprocessing
-@@ -364,7 +374,7 @@ if regenerate_features_and_labels == True:
-     vertical_barrier_timestamps = ml.labeling.add_vertical_barrier(
-         t_events=sampled_idx,
-         close=data["close"],
--        num_seconds=parameters["vbs"])
-+        num_seconds=vertical_barrier_seconds)
-     end = time.time()
-     print("vertical barrier" + str(end - start))
- 
-@@ -376,7 +386,7 @@ if regenerate_features_and_labels == True:
-         t_events=sampled_idx,
-         pt_sl=pt_sl,
-         target=data["window_volatility_level"],
--        min_ret=parameters["min_ret"],
-+        min_ret=minimum_return,
-         num_threads=cpus * 2,
-         vertical_barrier_times=vertical_barrier_timestamps,
-         side_prediction=side,
-@@ -533,26 +543,27 @@ if stage == 1:
-     ### ORDERBOOK VOLUME DATA
-     volumes_for_all_labels = volumes.loc[data.close.index]
- 
--    ## TRADE DATA
--    input_features_trade = []
--    close_array = data.close.values
--    input_features_trade.append(close_array)
-+    # ## TRADE DATA
-+    # input_features_trade = []
-+    # close_array = data.close.values
-+    # input_features_trade.append(close_array)
- 
--    if parameters["ntb"] == False and parameters["vbc"] == True:
--        volume_array = data.volume.values
--        input_features_trade.append(volume_array)
--    if parameters["ntb"] == True and parameters["tslbc"] == True:
--        time_since_last_bar_array = data.time_since_last_bar.values
--        input_features_trade.append(time_since_last_bar_array)
-+    # if parameters["ntb"] == False and parameters["vbc"] == True:
-+    #     volume_array = data.volume.values
-+    #     input_features_trade.append(volume_array)
-+    # if parameters["ntb"] == True and parameters["tslbc"] == True:
-+    #     time_since_last_bar_array = data.time_since_last_bar.values
-+    #     input_features_trade.append(time_since_last_bar_array)
- 
-     end_time = time.time()
-     print(end_time - start_time)
- 
--    # min max limits
-+    # Type of scaling to apply
-+    scaling_type = wandb.config['params']['scaling_type']['value']
- 
--    minimum = -1
--    maximum = 1
--    scaling_type = "z_score"
-+    # min max limits
-+    minimum = wandb.config['params']['scaling_maximum']['value']
-+    maximum = wandb.config['params']['scaling_minimum']['value']
- 
-     ### Split intothe training/validation/test sets
-     print("splitting into train/va/test sets start")
-@@ -573,11 +584,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_train = volumes_for_all_labels.iloc[
-         train_close_array_integer_index[0] -
--        parameters["WL"]:train_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        train_close_array_integer_index[-1] + 2]
- 
-     close_index_array_train = close_index_array[
-         train_close_array_integer_index[0] -
--        parameters["WL"]:train_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        train_close_array_integer_index[-1] + 2]
- 
-     end_time = time.time()
- 
-@@ -623,7 +636,7 @@ if stage == 1:
-     # print("Make window started")
-     # start_time = time.time()
- 
--    # padding = parameters["WL"] * 2
-+    # padding = wandb.config['params']['window_length']['value'] * 2
- 
-     # split_by = 100000
- 
-@@ -652,7 +665,7 @@ if stage == 1:
-     #         len(prices_for_window_index_array_train[start_index:end_index]),
-     #         input_features_normalized_train[:, close_and_input_start_index:
-     #                                         close_and_input_end_index],
--    #         parameters["WL"],
-+    #         wandb.config['params']['window_length']['value'],
-     #         model_arch,
-     #     )
- 
-@@ -697,11 +710,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_val = volumes_for_all_labels.iloc[
-         val_close_array_integer_index[0] -
--        parameters["WL"]:val_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        val_close_array_integer_index[-1] + 2]
- 
-     close_index_array_val = close_index_array[
-         val_close_array_integer_index[0] -
--        parameters["WL"]:val_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        val_close_array_integer_index[-1] + 2]
- 
-     input_features_val = make_input_features_from_orderbook_data(
-         volumes_for_all_labels_val)
-@@ -721,7 +736,7 @@ if stage == 1:
-     #     #     prices_for_window_index_array_val,
-     #     #     close_index_array_val,
-     #     #     input_features_normalized_val,
--    #     #     parameters["WL"],
-+    #     #     wandb.config['params']['window_length']['value'],
-     #     #     model_arch,
-     #     # )
- 
-@@ -736,11 +751,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_test = volumes_for_all_labels.iloc[
-         test_close_array_integer_index[0] -
--        parameters["WL"]:test_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        test_close_array_integer_index[-1] + 2]
- 
-     close_index_array_test = close_index_array[
-         test_close_array_integer_index[0] -
--        parameters["WL"]:test_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        test_close_array_integer_index[-1] + 2]
- 
-     input_features_test = make_input_features_from_orderbook_data(
-         volumes_for_all_labels_test)
-@@ -761,7 +778,7 @@ if stage == 1:
- #     #     prices_for_window_index_array_test,
- #     #     close_index_array_test,
- #     #     input_features_normalized_test,
--#     #     parameters["WL"],
-+#     #     wandb.config['params']['window_length']['value'],
- #     #     model_arch,
- #     # )
- 
-@@ -782,8 +799,9 @@ ax = plt.gca()
- 
- data.iloc[window_index - 200:window_index + 200].plot(y="close",
-                                                       use_index=True)
--plot_window_and_touch_and_label(window_index, parameters["WL"], data,
--                                triple_barrier_events, labels)
-+plot_window_and_touch_and_label(
-+    window_index, wandb.config['params']['window_length']['value'], data,
-+    triple_barrier_events, labels)
- 
- data.iloc[window_index - 10:window_index + 30]
- 
-@@ -837,9 +855,9 @@ print("writing train/val/test to .h5 files finished taking " +
- #     h5f.create_dataset("P", data=P)
- # h5f.create_dataset("y", data=y)
- # h5f.create_dataset("sampled_idx_epoch", data=sampled_idx_epoch)
--# if parameters["sw"] == "on":
-+# if use_sample_weights == "on":
- #     h5f.create_dataset("sample_weights", data=sample_weights)
--# elif parameters["sw"] == "off":
-+# elif use_sample_weights == "off":
- #     h5f.create_dataset("sample_weights", data=np.zeros(1))
- # h5f.close()
- 
-diff --git a/pipeline/tabl.py b/pipeline/tabl.py
-index 87d51f8..f0244ec 100644
---- a/pipeline/tabl.py
-+++ b/pipeline/tabl.py
-@@ -6,7 +6,7 @@ import sys
- 
- import imblearn
- from imblearn.over_sampling import SMOTE
--
-+import tensorflow
- import matplotlib.pyplot as plt
- 
- sys.path.append("..")
-@@ -17,29 +17,26 @@ home = str(Path.home())
- sys.path.append(home + "/ProdigyAI")
- 
- from numba import njit, prange
--import tensorflow as tf
-+
- import math
- 
--import keras
- import numpy as np
- from third_party_libraries.TABL import Models
- from third_party_libraries.keras_lr_finder.lr_finder import LRFinder
- from third_party_libraries.CLR.clr_callbacks import CyclicLR
- 
--from tensorflow.keras.losses import categorical_crossentropy
--from tensorflow.keras.optimizers import SGD, Adam
-+import keras
-+from keras.losses import categorical_crossentropy
-+from keras.optimizers import SGD, Adam
- from keras.callbacks import ModelCheckpoint
- 
-+import tensorflow as tf
- # Init wandb
- import wandb
- from wandb.keras import WandbCallback
- 
- import yaml
- 
--# import tensorflow.keras as keras
--# import tensorflow.keras.layers as layers
--# import tensorflow.keras.backend as K
--
- # Sorting out whether we are using the ipython kernel or not
- try:
-     get_ipython()
-@@ -95,25 +92,24 @@ with open(yaml_path) as file:
-     yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
- 
- config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
--wandb.init(
--    dir="~/ProdigyAI/",
--    project="prodigyai",
--    config=config_dictionary,
--)
--
--# try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes
--
--# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1
--template = [[40, 200], [60, 10], [120, 5], [3, 1]]
--
--# example data
--example_x = np.random.rand(1000, 40, 10)
--np.min(example_x)
--np.max(example_x)
--np.mean(example_x)
--example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)
--template = [[40, 200], [60, 10], [120, 5], [3, 1]]
--# 200(WINDOW LENGTH)
-+wandb.init(dir="~/ProdigyAI/",
-+           project="prodigyai",
-+           config=config_dictionary,
-+           resume=True)
-+
-+window_length = wandb.config['params']['window_length']['value']
-+num_features = wandb.config['params']['num_features']['value']
-+epochs = wandb.config['params']['epochs']['value']
-+batch_size = wandb.config['params']['batch_size']['value']
-+dropout = wandb.config['params']['dropout']['value']
-+
-+# # try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes
-+# # example data
-+# example_x = np.random.rand(1000, 40, 10)
-+# np.min(example_x)
-+# np.max(example_x)
-+# np.mean(example_x)
-+# example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)
- 
- ## PRODIGY AI HOCKUS POCKUS START
- from pathlib import Path
-@@ -121,7 +117,7 @@ import h5py
- 
- home = str(Path.home())
- 
--file_name = "arch=TABL&name=two_model&WL=200&pt=1&sl=1&min_ret=4.347826086956522e-05&vbs=0.5&head=1000&skip=0&vol_max=4.3498260869565224e-05&vol_min=4.348826086956522e-05&filter=none&cm_vol_mod=0&sw=on&fd=off&input=obook&ntb=True&tslbc=True.h5"
-+file_name = "esugj36b.h5"
- wandb.config.update({'dataset': file_name})
- path = home + "/ProdigyAI/data/preprocessed/" + file_name
- h5f = h5py.File(path, "r")
-@@ -265,8 +261,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
- 
- n_classes = 3
--dim = (200, 40)
--batch_size = 64
-+dim = (window_length, num_features)
- to_fit = True
- 
- # for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,
-@@ -280,22 +275,15 @@ to_fit = True
- #     import pdb
- #     pdb.set_trace()
- 
-+# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1
-+
-+template = [[num_features, window_length], [60, 10], [120, 5], [3, 1]]
-+
- # get Bilinear model
- projection_regularizer = None
- projection_constraint = keras.constraints.max_norm(3.0, axis=0)
- attention_regularizer = None
- attention_constraint = keras.constraints.max_norm(5.0, axis=1)
--dropout = 0.1
--
--model = Models.TABL(
--    template,
--    dropout,
--    projection_regularizer,
--    projection_constraint,
--    attention_regularizer,
--    attention_constraint,
--)
--model.summary()
- 
- # path = path_adjust + "data/lob_2010/train_and_test_tabl.h5"
- # h5f = h5py.File(path, "r")
-@@ -442,6 +430,18 @@ clr = CyclicLR(base_lr=base_lr,
- 
- optimizer = keras.optimizers.Adam()
- 
-+model = Models.TABL(
-+    template,
-+    dropout,
-+    projection_regularizer,
-+    projection_constraint,
-+    attention_regularizer,
-+    attention_constraint,
-+)
-+model.summary()
-+
-+
-+
- # Compile the model
- model.compile(loss=loss_function, optimizer=optimizer, metrics=["accuracy"])
- # Create a callback that saves the model's weights
-@@ -449,10 +449,15 @@ cp_callback = ModelCheckpoint(filepath=checkpoint_path,
-                               save_weights_only=True,
-                               verbose=1)
- 
--check_point_file = Path(checkpoint_path)
--if check_point_file.exists() and resuming == "resuming":
--    print("weights loaded")
--    model.load_weights(checkpoint_path)
-+if wandb.run.resumed:
-+    weights = wandb.restore("cp.ckpt",
-+                            run_path="garthtrickett/prodigyai/" + wandb.run.id)
-+    model.load_weights(weights.name)
-+
-+# check_point_file = Path(checkpoint_path)
-+# if check_point_file.exists() and resuming == "resuming":
-+#     print("weights loaded")
-+#     model.load_weights(checkpoint_path)
- # Fit data to model
- # history = model.fit(trainX_CNN,
- #                     trainY_CNN,
-@@ -460,9 +465,6 @@ if check_point_file.exists() and resuming == "resuming":
- #                     epochs=20,
- #                     callbacks=[clr, cp_callback])
- 
--finished_weights_path = path_adjust + "temp/cp_end.ckpt"
--model.save_weights(finished_weights_path)
--
- # create class weight
- # class_weight = {0: 1e6 / 300.0, 1: 1e6 / 400.0, 2: 1e6 / 300.0}
- train_generator = DataGenerator(checkpoint_path,
-@@ -489,12 +491,14 @@ steps_per_epoch = len(train_generator)
- validation_steps = len(val_generator)
- 
- # example sata training
--model.fit(train_generator,
--          steps_per_epoch=steps_per_epoch,
--          validation_steps=validation_steps,
--          epochs=1000,
--          validation_data=val_generator,
--          callbacks=[cp_callback, WandbCallback()])
-+model.fit(
-+    train_generator,
-+    steps_per_epoch=steps_per_epoch,
-+    validation_steps=validation_steps,
-+    epochs=epochs,
-+    validation_data=val_generator,
-+    callbacks=[cp_callback,
-+               WandbCallback(save_model=True, monitor="loss")])
- 
- model.save(os.path.join(wandb.run.dir, "model.h5"))
- # no class weight
-@@ -603,16 +607,16 @@ model.save(os.path.join(wandb.run.dir, "model.h5"))
- # X_val = X_val.reshape((X_val.shape[0], 40, 10))
- 
- # train one epoch to find the learning rate
--model.fit(
--    X_train,
--    y_train,
--    validation_data=(X_val, y_val),
--    batch_size=256,
--    epochs=1,
--    shuffle=False,
--)  # no class weight
--
--score = model.evaluate(x=X_test, y=y_test, batch_size=256)
-+# model.fit(
-+#     X_train,
-+#     y_train,
-+#     validation_data=(X_val, y_val),
-+#     batch_size=256,
-+#     epochs=1,
-+#     shuffle=False,
-+# )  # no class weight
-+
-+# score = model.evaluate(x=X_test, y=y_test, batch_size=256)
- 
- # Save model to wandb
- 
-diff --git a/temp/cp.ckpt.data-00000-of-00001 b/temp/cp.ckpt.data-00000-of-00001
-index 56c00e4..3d9600c 100644
-Binary files a/temp/cp.ckpt.data-00000-of-00001 and b/temp/cp.ckpt.data-00000-of-00001 differ
-diff --git a/temp/cp.ckpt.index b/temp/cp.ckpt.index
-index 3770713..d6d3b4b 100644
-Binary files a/temp/cp.ckpt.index and b/temp/cp.ckpt.index differ
-diff --git a/temp/cp_end.ckpt b/temp/cp_end.ckpt
-index c1c1d9e..62e0a67 100644
-Binary files a/temp/cp_end.ckpt and b/temp/cp_end.ckpt differ
-diff --git a/temp/cp_end.ckpt.data-00000-of-00001 b/temp/cp_end.ckpt.data-00000-of-00001
-index 1d572d0..12d3157 100644
-Binary files a/temp/cp_end.ckpt.data-00000-of-00001 and b/temp/cp_end.ckpt.data-00000-of-00001 differ
-diff --git a/temp/cp_end.ckpt.index b/temp/cp_end.ckpt.index
-index 15de09a..0ab67e1 100644
-Binary files a/temp/cp_end.ckpt.index and b/temp/cp_end.ckpt.index differ
-diff --git a/temp/is_script_finished.txt b/temp/is_script_finished.txt
-index 40ca0d3..bec7a08 100644
---- a/temp/is_script_finished.txt
-+++ b/temp/is_script_finished.txt
-@@ -1 +1 @@
--full_script_time64.39192986488342
-\ No newline at end of file
-+full_script_time25.707124948501587
-\ No newline at end of file
-Submodule third_party_libraries/TABL contains modified content
-Submodule third_party_libraries/TABL 7f25314..df3e92d:
-diff --git a/third_party_libraries/TABL/Layers.py b/third_party_libraries/TABL/Layers.py
-index 9020002..b299e57 100644
---- a/third_party_libraries/TABL/Layers.py
-+++ b/third_party_libraries/TABL/Layers.py
-@@ -3,8 +3,9 @@
- """
- @author: Dat Tran (dat.tranthanh@tut.fi)
- """
-+import keras
- from keras import backend as K
--from keras.engine.topology import Layer
-+from keras.layers import Layer
- from keras import activations as Activations
- from keras import initializers as Initializers
- 
-@@ -18,25 +19,25 @@ class Constraint(object):
- 
-     def get_config(self):
-         return {}
--    
-+
-+
- class MinMax(Constraint):
-     """
-     Customized min-max constraint for scalar
-     """
--
-     def __init__(self, min_value=0.0, max_value=10.0):
-         self.min_value = min_value
-         self.max_value = max_value
-+
-     def __call__(self, w):
--        
--        return K.clip(w,self.min_value,self.max_value)
-+
-+        return K.clip(w, self.min_value, self.max_value)
- 
-     def get_config(self):
--        return {'min_value': self.min_value,
--                'max_value': self.max_value}
--        
-+        return {'min_value': self.min_value, 'max_value': self.max_value}
- 
--def nmodeproduct(x,w,mode):
-+
-+def nmodeproduct(x, w, mode):
-     """
-     n-mode product for 2D matrices
-     x: NxHxW
-@@ -45,74 +46,83 @@ def nmodeproduct(x,w,mode):
-     
-     output: NxhxW (mode1) or NxHxw (mode2)
-     """
--    if mode==2:
--        x=K.dot(x,w)
-+    if mode == 2:
-+        x = K.dot(x, w)
-     else:
--        x=K.permute_dimensions(x,(0,2,1))
--        x=K.dot(x,w)
--        x=K.permute_dimensions(x,(0,2,1))
-+        x = K.permute_dimensions(x, (0, 2, 1))
-+        x = K.dot(x, w)
-+        x = K.permute_dimensions(x, (0, 2, 1))
-     return x
- 
-+
- class BL(Layer):
-     """
-     Bilinear Layer
-     """
--    def __init__(self, output_dim,
-+    def __init__(self,
-+                 output_dim,
-                  kernel_regularizer=None,
--                 kernel_constraint=None,**kwargs):
-+                 kernel_constraint=None,
-+                 **kwargs):
-         """
-         output_dim : output dimensions of 2D tensor, should be a list of len 2, e.g. [30,20]
-         kernel_regularizer : keras regularizer object
-         kernel_constraint: keras constraint object
-         """
--        
-+
-         self.output_dim = output_dim
--        self.kernel_regularizer=kernel_regularizer
--        self.kernel_constraint=kernel_constraint
--        
-+        self.kernel_regularizer = kernel_regularizer
-+        self.kernel_constraint = kernel_constraint
-+
-         super(BL, self).__init__(**kwargs)
- 
-     def build(self, input_shape):
--        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
--                                      initializer='he_uniform',
--                                      regularizer=self.kernel_regularizer,
--                                      constraint=self.kernel_constraint,
--                                      trainable=True)
--        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
--                                      initializer='he_uniform',
--                                      regularizer=self.kernel_regularizer,
--                                      constraint=self.kernel_constraint,
--                                      trainable=True)
--
--        self.bias=self.add_weight(name='bias',shape=(self.output_dim[0],self.output_dim[1]),
--                              initializer='zeros',trainable=True)
-+        self.W1 = self.add_weight(name='W1',
-+                                  shape=(input_shape[1], self.output_dim[0]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.kernel_regularizer,
-+                                  constraint=self.kernel_constraint,
-+                                  trainable=True)
-+        self.W2 = self.add_weight(name='W2',
-+                                  shape=(input_shape[2], self.output_dim[1]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.kernel_regularizer,
-+                                  constraint=self.kernel_constraint,
-+                                  trainable=True)
-+
-+        self.bias = self.add_weight(name='bias',
-+                                    shape=(self.output_dim[0],
-+                                           self.output_dim[1]),
-+                                    initializer='zeros',
-+                                    trainable=True)
-         super(BL, self).build(input_shape)
- 
-     def call(self, x):
-         print(K.int_shape(x))
--        x = nmodeproduct(x,self.W1,1)
--        x = nmodeproduct(x,self.W2,2)
--        x = K.bias_add(x,self.bias)
--        
--        if self.output_dim[1]==1:
--            x = K.squeeze(x,axis=-1)
-+        x = nmodeproduct(x, self.W1, 1)
-+        x = nmodeproduct(x, self.W2, 2)
-+        x = K.bias_add(x, self.bias)
-+
-+        if self.output_dim[1] == 1:
-+            x = K.squeeze(x, axis=-1)
-         print(K.int_shape(x))
-         return x
- 
-     def compute_output_shape(self, input_shape):
--        if self.output_dim[1]==1:
-+        if self.output_dim[1] == 1:
-             return (input_shape[0], self.output_dim[0])
-         else:
-             return (input_shape[0], self.output_dim[0], self.output_dim[1])
--        
--        
-+
-+
- class TABL(Layer):
-     """
-     Temporal Attention augmented Bilinear Layer
-     https://arxiv.org/abs/1712.00975
-     
-     """
--    def __init__(self, output_dim,
-+    def __init__(self,
-+                 output_dim,
-                  projection_regularizer=None,
-                  projection_constraint=None,
-                  attention_regularizer=None,
-@@ -125,45 +135,52 @@ class TABL(Layer):
-         attention_regularizer: keras regularizer object for attention matrix
-         attention_constraint: keras constraint object for attention matrix
-         """
--        
-+
-         self.output_dim = output_dim
-         self.projection_regularizer = projection_regularizer
-         self.projection_constraint = projection_constraint
-         self.attention_regularizer = attention_regularizer
-         self.attention_constraint = attention_constraint
--        
-+
-         super(TABL, self).__init__(**kwargs)
- 
-     def build(self, input_shape):
--        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
--                                      initializer='he_uniform',
--                                      regularizer=self.projection_regularizer,
--                                      constraint=self.projection_constraint,
--                                      trainable=True)
--        
--        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
--                                      initializer='he_uniform',
--                                      regularizer=self.projection_regularizer,
--                                      constraint=self.projection_constraint,
--                                      trainable=True)
--        
--        self.W = self.add_weight(name='W',shape=(input_shape[2], input_shape[2]),
--                                      initializer=Initializers.Constant(1.0/input_shape[2]),
--                                      regularizer=self.attention_regularizer,
--                                      constraint=self.attention_constraint,
--                                      trainable=True)
--        
--        self.alpha = self.add_weight(name='alpha',shape=(1,),
--                                      initializer=Initializers.Constant(0.5),
--                                      constraint=MinMax(),
--                                      trainable=True)
--
--
--        self.bias=self.add_weight(name='bias',shape=(1, self.output_dim[0],self.output_dim[1]),
--                              initializer='zeros',trainable=True)
--        
-+        self.W1 = self.add_weight(name='W1',
-+                                  shape=(input_shape[1], self.output_dim[0]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.projection_regularizer,
-+                                  constraint=self.projection_constraint,
-+                                  trainable=True)
-+
-+        self.W2 = self.add_weight(name='W2',
-+                                  shape=(input_shape[2], self.output_dim[1]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.projection_regularizer,
-+                                  constraint=self.projection_constraint,
-+                                  trainable=True)
-+
-+        self.W = self.add_weight(name='W',
-+                                 shape=(input_shape[2], input_shape[2]),
-+                                 initializer=Initializers.Constant(
-+                                     1.0 / input_shape[2]),
-+                                 regularizer=self.attention_regularizer,
-+                                 constraint=self.attention_constraint,
-+                                 trainable=True)
-+
-+        self.alpha = self.add_weight(name='alpha',
-+                                     shape=(1, ),
-+                                     initializer=Initializers.Constant(0.5),
-+                                     constraint=MinMax(),
-+                                     trainable=True)
-+
-+        self.bias = self.add_weight(name='bias',
-+                                    shape=(1, self.output_dim[0],
-+                                           self.output_dim[1]),
-+                                    initializer='zeros',
-+                                    trainable=True)
-+
-         self.in_shape = input_shape
--        
-+
-         super(TABL, self).build(input_shape)
- 
-     def call(self, x):
-@@ -174,27 +191,26 @@ class TABL(Layer):
-         W: D2 x D2
-         """
-         # first mode projection
--        x = nmodeproduct(x,self.W1,1) # N x d1 x D2
--        # enforcing constant (1) on the diagonal 
--        W = self.W-self.W*K.eye(self.in_shape[2],dtype='float32')+K.eye(self.in_shape[2],dtype='float32')/self.in_shape[2]
--        # calculate attention 
--        attention = Activations.softmax(nmodeproduct(x,W,2),axis=-1) # N x d1 x D2
-+        x = nmodeproduct(x, self.W1, 1)  # N x d1 x D2
-+        # enforcing constant (1) on the diagonal
-+        W = self.W - self.W * K.eye(self.in_shape[2], dtype='float32') + K.eye(
-+            self.in_shape[2], dtype='float32') / self.in_shape[2]
-+        # calculate attention
-+        attention = Activations.softmax(nmodeproduct(x, W, 2),
-+                                        axis=-1)  # N x d1 x D2
-         # apply attention
--        x = self.alpha*x + (1.0 - self.alpha)*x*attention
-+        x = self.alpha * x + (1.0 - self.alpha) * x * attention
-         # second mode projection
--        x = nmodeproduct(x,self.W2,2)
-+        x = nmodeproduct(x, self.W2, 2)
-         # bias add
-         x = x + self.bias
--        
--        if self.output_dim[1]==1:
--            x = K.squeeze(x,axis=-1)
-+
-+        if self.output_dim[1] == 1:
-+            x = K.squeeze(x, axis=-1)
-         return x
- 
-     def compute_output_shape(self, input_shape):
--        if self.output_dim[1]==1:
-+        if self.output_dim[1] == 1:
-             return (input_shape[0], self.output_dim[0])
-         else:
-             return (input_shape[0], self.output_dim[0], self.output_dim[1])
--
--
--
-diff --git a/third_party_libraries/TABL/Models.py b/third_party_libraries/TABL/Models.py
-index e2bf62c..d5c4c45 100644
---- a/third_party_libraries/TABL/Models.py
-+++ b/third_party_libraries/TABL/Models.py
-@@ -4,7 +4,6 @@
- @author: Dat Tran (dat.tranthanh@tut.fi)
- """
- 
--
- from third_party_libraries.TABL import Layers
- import keras
- 
-@@ -74,7 +73,8 @@ def TABL(
- 
-     x = inputs
-     for k in range(1, len(template) - 1):
--        x = Layers.BL(template[k], projection_regularizer, projection_constraint)(x)
-+        x = Layers.BL(template[k], projection_regularizer,
-+                      projection_constraint)(x)
-         x = keras.layers.Activation("relu")(x)
-         x = keras.layers.Dropout(dropout)(x)
- 
-@@ -94,4 +94,3 @@ def TABL(
-     model.compile(optimizer, "categorical_crossentropy", ["acc"])
- 
-     return model
--
-diff --git a/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc
-new file mode 100644
-index 0000000..ff0a753
-Binary files /dev/null and b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc differ
-diff --git a/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc
-new file mode 100644
-index 0000000..a57f1ba
-Binary files /dev/null and b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc differ
-diff --git a/yaml/tabl.yaml b/yaml/tabl.yaml
-index 2cdc7b2..456e287 100644
---- a/yaml/tabl.yaml
-+++ b/yaml/tabl.yaml
-@@ -4,4 +4,14 @@ epochs:
-   value: 100
- batch_size:
-   desc: Size of each mini-batch
--  value: 32
-+  value: 256
-+window_length:
-+  desc: null
-+  value: 200
-+num_features:
-+  desc: null
-+  value: 40
-+dropout:
-+  desc: null
-+  value: 0.1
-+
diff --git a/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/requirements.txt b/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/requirements.txt
deleted file mode 100644
index c4244a7..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/requirements.txt
+++ /dev/null
@@ -1,339 +0,0 @@
-absl-py==0.9.0
-alabaster==0.7.12
-anaconda-client==1.7.2
-anaconda-project==0.8.3
-appdirs==1.4.3
-argparse==1.4.0
-asn1crypto==1.3.0
-astor==0.8.1
-astroid==2.3.3
-astropy==4.0
-asyncore-wsgi==0.0.9
-atomicwrites==1.3.0
-attrs==19.3.0
-autobahn==20.2.1
-autoflake==1.3.1
-automat==0.8.0
-babel==2.8.0
-backcall==0.1.0
-backports.os==0.1.1
-backports.shutil-get-terminal-size==1.0.0
-beautifulsoup4==4.8.2
-bitarray==1.2.1
-bkcharts==0.2
-black==19.10b0
-bleach==3.1.0
-blis==0.4.1
-bokeh==1.4.0
-boto3==1.12.37
-boto==2.49.0
-botocore==1.15.37
-bottle==0.12.18
-bottleneck==1.3.1
-cachetools==4.0.0
-catalogue==1.0.0
-cchardet==2.1.6
-certifi==2019.11.28
-cffi==1.13.2
-chardet==3.0.4
-click==7.0
-cloudpickle==1.2.2
-clyent==1.2.2
-colorama==0.4.3
-configparser==5.0.0
-constantly==15.1.0
-contextlib2==0.6.0.post1
-cryptography==2.8
-cvxpy==1.0.31
-cycler==0.10.0
-cymem==2.0.3
-cython==0.29.14
-cytoolz==0.10.1
-dask==2.10.1
-dateparser==0.7.2
-decorator==4.4.1
-defusedxml==0.6.0
-dill==0.3.1.1
-distributed==2.10.0
-docker-pycreds==0.4.0
-docutils==0.15.2
-easyprocess==0.2.10
-ecos==2.0.7.post1
-entrypoints==0.3
-et-xmlfile==1.0.1
-fastai2==0.0.17
-fastcache==1.1.0
-fastcore==0.1.17
-fastprogress==0.2.2
-filelock==3.0.12
-flake8==3.7.9
-flask==1.1.1
-fsspec==0.6.2
-future==0.18.2
-gast==0.2.2
-gevent==1.4.0
-gitdb==4.0.5
-gitpython==3.1.2
-glob2==0.7
-gmpy2==2.0.8
-google-api-core==1.16.0
-google-api-python-client==1.8.0
-google-auth-httplib2==0.0.3
-google-auth-oauthlib==0.4.1
-google-auth==1.13.1
-google-cloud-profiler==1.0.9
-google-pasta==0.1.8
-googleapis-common-protos==1.51.0
-gql==0.2.0
-graphql-core==1.1
-greenlet==0.4.15
-grpcio==1.27.1
-h5py==2.10.0
-hanging-threads==2.0.5
-heapdict==1.0.1
-html5lib==1.0.1
-httplib2==0.17.2
-hyperlink==19.0.0
-hypothesis==5.4.1
-idna==2.8
-imageio==2.6.1
-imagesize==1.2.0
-imbalanced-learn==0.6.1
-imblearn==0.0
-importlib-metadata==1.5.0
-incremental==17.5.0
-inflect==4.1.0
-ipykernel==5.1.4
-ipython-genutils==0.2.0
-ipython==7.12.0
-ipywidgets==7.5.1
-isort==4.3.21
-itsdangerous==1.1.0
-jaraco.itertools==5.0.0
-jdcal==1.4.1
-jedi==0.16.0
-jeepney==0.4.2
-jieba==0.42.1
-jinja2==2.11.1
-jmespath==0.9.5
-joblib==0.14.1
-json5==0.9.0
-jsonschema==3.2.0
-jupyter-client==5.3.4
-jupyter-console==6.1.0
-jupyter-core==4.6.1
-jupyter==1.0.0
-jupyterlab-server==1.0.6
-jupyterlab==1.2.6
-keras-applications==1.0.8
-keras-bert==0.81.0
-keras-embed-sim==0.7.0
-keras-layer-normalization==0.14.0
-keras-multi-head==0.22.0
-keras-pos-embd==0.11.0
-keras-position-wise-feed-forward==0.6.0
-keras-preprocessing==1.1.0
-keras-self-attention==0.41.0
-keras-transformer==0.32.0
-keras==2.3.1
-keyring==21.1.0
-kiwisolver==1.1.0
-ktrain==0.12.3
-langdetect==1.0.8
-lazy-object-proxy==1.4.3
-libarchive-c==2.8
-lief==0.9.0
-llvmlite==0.31.0
-locket==0.2.0
-lxml==4.5.0
-markdown==3.2
-markupsafe==1.1.1
-matplotlib==3.1.3
-mccabe==0.6.1
-mistune==0.8.4
-mkl-fft==1.0.15
-mkl-random==1.1.0
-mkl-service==2.3.0
-mock==3.0.5
-more-itertools==8.2.0
-mpmath==1.1.0
-msgpack==1.0.0
-multipledispatch==0.6.0
-multiprocess==0.70.9
-murmurhash==1.0.2
-nbconvert==5.6.1
-nbformat==5.0.4
-networkx==2.4
-nltk==3.4.5
-nose==1.3.7
-notebook==6.0.3
-numba==0.48.0
-numexpr==2.7.1
-numpy==1.18.1
-numpydoc==0.9.2
-nvidia-ml-py3==7.352.0
-oauthlib==3.1.0
-olefile==0.46
-opencv-python==4.2.0.34
-openpyxl==3.0.3
-opt-einsum==3.1.0
-osqp==0.6.1
-packaging==20.1
-pandas==1.0.3
-pandocfilters==1.4.2
-parse==1.6.6
-parso==0.6.0
-partd==1.1.0
-path==13.1.0
-pathlib2==2.3.5
-pathspec==0.7.0
-pathtools==0.1.2
-patsy==0.5.1
-pep8==1.7.1
-pexpect==4.8.0
-pickleshare==0.7.5
-pillow==7.0.0
-pip==20.0.2
-pkginfo==1.5.0.1
-plac==1.1.3
-pluggy==0.13.1
-ply==3.11
-preshed==3.0.2
-prometheus-client==0.7.1
-promise==2.3
-prompt-toolkit==3.0.3
-protobuf==3.11.3
-psutil==5.6.7
-psycopg2-binary==2.8.4
-ptyprocess==0.6.0
-py==1.8.1
-pyarrow==0.16.0
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycodestyle==2.5.0
-pycosat==0.6.3
-pycparser==2.19
-pycrypto==2.6.1
-pycurl==7.43.0.5
-pydbus==0.6.0
-pyflakes==2.1.1
-pygments==2.5.2
-pyhamcrest==2.0.0
-pylint==2.4.4
-pyodbc==4.0.28
-pyopenssl==19.1.0
-pyparsing==2.4.6
-pyrsistent==0.15.7
-pysocks==1.7.1
-pytest-arraydiff==0.3
-pytest-astropy-header==0.1.2
-pytest-astropy==0.7.0
-pytest-doctestplus==0.5.0
-pytest-openfiles==0.4.0
-pytest-remotedata==0.3.2
-pytest==5.3.5
-python-dateutil==2.8.1
-pyts==0.10.0
-pytz==2019.3
-pyunpack==0.1.2
-pywavelets==1.1.1
-pyyaml==5.3
-pyzmq==18.1.1
-qtawesome==0.6.1
-qtconsole==4.6.0
-qtpy==1.9.0
-regex==2020.1.8
-requests-oauthlib==1.3.0
-requests==2.22.0
-rope==0.16.0
-rsa==4.0
-ruamel-yaml==0.15.87
-s3transfer==0.3.3
-sacremoses==0.0.38
-scikit-image==0.16.2
-scikit-learn==0.22.1
-scipy==1.4.1
-scoop==0.7.1.1
-scs==2.1.2
-seaborn==0.10.0
-secretstorage==3.1.2
-send2trash==1.5.0
-sentencepiece==0.1.85
-sentry-sdk==0.14.3
-seqeval==0.0.12
-service-identity==18.1.0
-setuptools==45.1.0.post20200127
-shortuuid==1.0.1
-simplegeneric==0.8.1
-singledispatch==3.4.0.3
-six==1.14.0
-smmap==3.0.4
-snowballstemmer==2.0.0
-sortedcollections==1.1.2
-sortedcontainers==2.1.0
-soupsieve==1.9.5
-spacy==2.2.3
-sphinx==2.3.1
-sphinxcontrib-applehelp==1.0.1
-sphinxcontrib-devhelp==1.0.1
-sphinxcontrib-htmlhelp==1.0.2
-sphinxcontrib-jsmath==1.0.1
-sphinxcontrib-qthelp==1.0.2
-sphinxcontrib-serializinghtml==1.1.3
-sphinxcontrib-websupport==1.1.2
-spyder-kernels==0.5.2
-spyder==3.3.6
-sqlalchemy==1.3.13
-srsly==1.0.1
-statsmodels==0.11.0
-subprocess32==3.5.4
-sympy==1.5.1
-syntok==1.2.2
-tables==3.6.1
-tblib==1.6.0
-tensorboard==2.1.1
-tensorflow-datasets==2.1.0
-tensorflow-estimator==2.1.0
-tensorflow-metadata==0.21.1
-tensorflow==2.1.0
-termcolor==1.1.0
-terminado==0.8.3
-testpath==0.4.4
-thinc==7.3.1
-tokenizers==0.5.2
-toml==0.10.0
-toolz==0.10.0
-torch==1.4.0
-torchvision==0.5.0
-tornado==6.0.3
-tqdm==4.42.0
-traitlets==4.3.3
-transformers==2.8.0
-twisted==19.10.0
-txaio==20.1.1
-typed-ast==1.4.1
-tzlocal==2.0.0
-unicodecsv==0.14.1
-uritemplate==3.0.1
-urllib3==1.25.8
-wandb==0.8.35
-wasabi==0.6.0
-watchdog==0.10.2
-wcwidth==0.1.8
-web-pdb==1.5.1
-webencodings==0.5.1
-websocket-client==0.57.0
-weresync==1.1.5
-werkzeug==0.16.1
-wheel==0.34.2
-widgetsnbextension==3.5.1
-wrapt==1.11.2
-wurlitzer==2.0.0
-xlrd==1.2.0
-xlsxwriter==1.2.7
-xlwt==1.3.0
-yapf==0.29.0
-yapsy==1.11.223
-zict==1.0.0
-zipp==2.1.0
-zope.interface==4.7.1
\ No newline at end of file
diff --git a/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/wandb-events.jsonl b/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/wandb-events.jsonl
deleted file mode 100644
index 426485e..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/wandb-events.jsonl
+++ /dev/null
@@ -1 +0,0 @@
-{"system.cpu": 13.85, "system.memory": 11.3, "system.disk": 23.5, "system.proc.memory.availableMB": 34706.06, "system.proc.memory.rssMB": 422.82, "system.proc.memory.percent": 1.08, "system.proc.cpu.threads": 12.0, "system.network.sent": 24177, "system.network.recv": 75003, "_wandb": true, "_timestamp": 1589165797, "_runtime": 1}
diff --git a/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/wandb-history.jsonl b/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/wandb-history.jsonl
deleted file mode 100644
index e69de29..0000000
diff --git a/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/wandb-metadata.json b/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/wandb-metadata.json
deleted file mode 100644
index b4e6f9d..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025635-33cb2k5u/wandb-metadata.json
+++ /dev/null
@@ -1,23 +0,0 @@
-{
-    "root": "/home/garthtrickett/ProdigyAI",
-    "program": "tabl.py",
-    "git": {
-        "remote": "https://github.com/garthtrickett/ProdigyAI",
-        "commit": "7667fefed2018a0664418c25c97e3c08866a6f71"
-    },
-    "email": "garthtrickett@gmail.com",
-    "startedAt": "2020-05-11T02:56:35.302120",
-    "host": "preemp-cpu-1",
-    "username": "garthtrickett",
-    "executable": "/home/garthtrickett/miniconda3/envs/ProdigyAI/bin/python",
-    "os": "Linux-5.3.0-1018-gcp-x86_64-with-debian-buster-sid",
-    "python": "3.7.6",
-    "cpu_count": 6,
-    "args": [],
-    "state": "killed",
-    "jobType": null,
-    "mode": "run",
-    "project": "prodigyai",
-    "heartbeatAt": "2020-05-11T02:56:38.053447",
-    "exitcode": 255
-}
diff --git a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/config.yaml b/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/config.yaml
deleted file mode 100644
index 8c1a8a9..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/config.yaml
+++ /dev/null
@@ -1,34 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.8.35
-    framework: keras
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.7.6
-dataset:
-  desc: null
-  value: esugj36b.h5
-params:
-  desc: null
-  value:
-    batch_size:
-      desc: Size of each mini-batch
-      value: 256
-    dropout:
-      desc: null
-      value: 0.1
-    epochs:
-      desc: Number of epochs to train over
-      value: 100
-    num_features:
-      desc: null
-      value: 40
-    window_length:
-      desc: null
-      value: 200
-yaml:
-  desc: null
-  value: yaml/tabl.yaml
diff --git a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/cp.ckpt b/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/cp.ckpt
deleted file mode 100644
index 82daee0..0000000
Binary files a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/cp.ckpt and /dev/null differ
diff --git a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/diff.patch b/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/diff.patch
deleted file mode 100644
index 214be23..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/diff.patch
+++ /dev/null
@@ -1,1270 +0,0 @@
-diff --git a/pipeline/deeplob.py b/pipeline/deeplob.py
-index 2d6c076..fef42d4 100644
---- a/pipeline/deeplob.py
-+++ b/pipeline/deeplob.py
-@@ -2,6 +2,7 @@
- import pandas as pd
- import pickle
- import numpy as np
-+import tensorflow
- import tensorflow as tf
- from tensorflow.keras import layers
- 
-@@ -25,21 +26,17 @@ from keras.utils import np_utils
- import matplotlib.pyplot as plt
- import math
- from numba import njit, prange
--
--# Init wandb
--import wandb
--from wandb.keras import WandbCallback
--wandb.init(project="prodigyai")
-+import yaml
- 
- # set random seeds
- np.random.seed(1)
- tf.random.set_seed(2)
- 
- import keras
--from keras.callbacks import ModelCheckpoint
--from keras import backend as K
-+from tensorflow.keras.callbacks import ModelCheckpoint
- import h5py
--
-+import wandb
-+from wandb.keras import WandbCallback
- # check if using gpu
- gpus = tf.config.list_physical_devices()
- any_gpus = [s for s in gpus if "GPU" in s[0]]
-@@ -105,6 +102,23 @@ if cwd == home + "/":
-     cwd = cwd + "/ProdigyAI"
-     path_adjust = cwd
- 
-+# Init wandb
-+yaml_path = path_adjust + "yaml/deeplob.yaml"
-+with open(yaml_path) as file:
-+    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
-+
-+config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
-+wandb.init(dir="~/ProdigyAI/",
-+           project="prodigyai",
-+           config=config_dictionary,
-+           resume=True)
-+
-+window_length = wandb.config['params']['window_length']['value']
-+num_features = wandb.config['params']['num_features']['value']
-+epochs = wandb.config['params']['epochs']['value']
-+batch_size = wandb.config['params']['batch_size']['value']
-+number_of_lstm = wandb.config['params']['number_of_lstm']['value']
-+
- # limit gpu usage for keras
- gpu_devices = tf.config.experimental.list_physical_devices("GPU")
- for device in gpu_devices:
-@@ -216,7 +230,8 @@ import h5py
- 
- home = str(Path.home())
- 
--file_name = "arch=DLOB&name=two_model&WL=200&pt=1&sl=1&min_ret=4.347826086956522e-05&vbs=0.5&head=0&skip=0&vol_max=4.3498260869565224e-05&vol_min=4.348826086956522e-05&filter=none&cm_vol_mod=0&sw=on&fd=off&input=obook&ntb=True&tslbc=True.h5"
-+file_name = "esugj36b.h5"
-+wandb.config.update({'dataset': file_name})
- path = home + "/ProdigyAI/data/preprocessed/" + file_name
- h5f = h5py.File(path, "r")
- prices_for_window_index_array_train = h5f[
-@@ -268,6 +283,7 @@ def generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data):
- 
- class DataGenerator(tf.compat.v2.keras.utils.Sequence):
-     def __init__(self,
-+                 checkpoint_path,
-                  prices_for_window_index_array,
-                  input_features_normalized,
-                  y_data,
-@@ -277,6 +293,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
-                  to_fit,
-                  shuffle=True):
-         self.batch_size = batch_size
-+        self.checkpoint_path = checkpoint_path
-         self.prices_for_window_index_array = prices_for_window_index_array
-         self.input_features_normalized = input_features_normalized
-         self.labels = y_data
-@@ -323,6 +340,8 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
-     def on_epoch_end(self):
- 
-+        wandb.save(self.checkpoint_path)
-+
-         self.indexes = np.arange(len(self.prices_for_window_index_array))
- 
-         if self.shuffle:
-@@ -351,8 +370,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
- 
- n_classes = 3
--dim = (200, 40)
--batch_size = 64
-+dim = (window_length, num_features)
- to_fit = True
- 
- # for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,
-@@ -371,70 +389,6 @@ to_fit = True
- 
- ## PRODIGY AI HOCKUS POCKUS END
- 
--
--### Model Architecture
--def create_deeplob(T, NF, number_of_lstm):
--    input_lmd = Input(shape=(T, NF, 1))
--
--    # build the convolutional block
--    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    conv_first1 = Conv2D(32, (1, 10))(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    # build the inception module
--    convsecond_1 = Conv2D(64, (1, 1), padding="same")(conv_first1)
--    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
--    convsecond_1 = Conv2D(64, (3, 1), padding="same")(convsecond_1)
--    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
--
--    convsecond_2 = Conv2D(64, (1, 1), padding="same")(conv_first1)
--    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
--    convsecond_2 = Conv2D(64, (5, 1), padding="same")(convsecond_2)
--    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
--
--    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),
--                                padding="same")(conv_first1)
--    convsecond_3 = Conv2D(64, (1, 1), padding="same")(convsecond_3)
--    convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)
--
--    convsecond_output = concatenate([convsecond_1, convsecond_2, convsecond_3],
--                                    axis=3)
--
--    # use the MC dropout here
--    conv_reshape = Reshape(
--        (int(convsecond_output.shape[1]),
--         int(convsecond_output.shape[3])))(convsecond_output)
--
--    conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)
--
--    # build the output layer
--    out = Dense(3, activation="softmax")(conv_lstm)
--    model = Model(inputs=input_lmd, outputs=out)
--    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
--    model.compile(optimizer=adam,
--                  loss="categorical_crossentropy",
--                  metrics=["accuracy"])
--
--    return model
--
--
- checkpoint_path = path_adjust + "temp/cp.ckpt"
- # Create a callback that saves the model's weights
- cp_callback = ModelCheckpoint(filepath=checkpoint_path,
-@@ -446,9 +400,8 @@ if check_point_file.exists() and resuming == "resuming":
-     print("weights loaded")
-     model.load_weights(checkpoint_path)
- 
--batch_size = 64
--
--train_generator = DataGenerator(prices_for_window_index_array_train,
-+train_generator = DataGenerator(checkpoint_path,
-+                                prices_for_window_index_array_train,
-                                 input_features_normalized_train,
-                                 y_train,
-                                 batch_size,
-@@ -457,7 +410,8 @@ train_generator = DataGenerator(prices_for_window_index_array_train,
-                                 to_fit,
-                                 shuffle=True)
- 
--val_generator = DataGenerator(prices_for_window_index_array_val,
-+val_generator = DataGenerator(checkpoint_path,
-+                              prices_for_window_index_array_val,
-                               input_features_normalized_val,
-                               y_val,
-                               batch_size,
-@@ -469,14 +423,87 @@ val_generator = DataGenerator(prices_for_window_index_array_val,
- steps_per_epoch = len(train_generator)
- validation_steps = len(val_generator)
- 
--deeplob = create_deeplob(200, 40, 64)
--deeplob.fit(train_generator,
--            steps_per_epoch=steps_per_epoch,
--            validation_steps=validation_steps,
--            epochs=100,
--            verbose=2,
--            validation_data=val_generator,
--            callbacks=[cp_callback, WandbCallback()])
-+if wandb.run.resumed:
-+    wandb.restore("model-best.h5",run_path="garthtrickett/prodigyai/" + wandb.run.id)
-+    # restore the best model
-+    deeplob = tensorflow.keras.models.load_model(wandb.restore("model-best.h5").name)
-+    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
-+    deeplob.compile(optimizer=adam,
-+                    loss="categorical_crossentropy",
-+                    metrics=["accuracy"])
-+else:
-+    ### Model Architecture
-+    def create_deeplob(T, NF, number_of_lstm):
-+        input_lmd = Input(shape=(T, NF, 1))
-+
-+        # build the convolutional block
-+        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        conv_first1 = Conv2D(32, (1, 10))(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        # build the inception module
-+        convsecond_1 = Conv2D(64, (1, 1), padding="same")(conv_first1)
-+        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
-+        convsecond_1 = Conv2D(64, (3, 1), padding="same")(convsecond_1)
-+        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
-+
-+        convsecond_2 = Conv2D(64, (1, 1), padding="same")(conv_first1)
-+        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
-+        convsecond_2 = Conv2D(64, (5, 1), padding="same")(convsecond_2)
-+        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
-+
-+        convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),
-+                                    padding="same")(conv_first1)
-+        convsecond_3 = Conv2D(64, (1, 1), padding="same")(convsecond_3)
-+        convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)
-+
-+        convsecond_output = concatenate(
-+            [convsecond_1, convsecond_2, convsecond_3], axis=3)
-+
-+        # use the MC dropout here
-+        conv_reshape = Reshape(
-+            (int(convsecond_output.shape[1]),
-+             int(convsecond_output.shape[3])))(convsecond_output)
-+
-+        conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)
-+
-+        # build the output layer
-+        out = Dense(3, activation="softmax")(conv_lstm)
-+        model = Model(inputs=input_lmd, outputs=out)
-+        adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
-+        model.compile(optimizer=adam,
-+                      loss="categorical_crossentropy",
-+                      metrics=["accuracy"])
-+
-+        return model
-+
-+    deeplob = create_deeplob(window_length, num_features, number_of_lstm)
-+deeplob.fit(
-+    train_generator,
-+    steps_per_epoch=steps_per_epoch,
-+    validation_steps=validation_steps,
-+    epochs=100,
-+    verbose=2,
-+    validation_data=val_generator,
-+    callbacks=[cp_callback,
-+               WandbCallback(save_model=True, monitor="loss")])
- 
- finished_weights_path = path_adjust + "temp/cp_end.ckpt"
- deeplob.save_weights(finished_weights_path)
-diff --git a/pipeline/preprocessing.py b/pipeline/preprocessing.py
-index 932ae0b..714d325 100644
---- a/pipeline/preprocessing.py
-+++ b/pipeline/preprocessing.py
-@@ -136,54 +136,67 @@ if stage == 2:
-     side = X_for_all_labels["predicted_bins"]
-     # Could use the probabilities (instead of [1,0,0] use [0.2,0.55,0.25]
- 
-+import yaml
-+import wandb
-+
-+yaml_path = path_adjust + "yaml/preprocessing.yaml"
-+with open(yaml_path) as file:
-+    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
-+
-+config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
-+
-+wandb.init(
-+    dir="~/ProdigyAI/",
-+    project="prodigyai",
-+    config=config_dictionary,
-+)
-+
-+minimum_return = eval(wandb.config['params']['minimum_return']['value'])
-+vertical_barrier_seconds = eval(
-+    wandb.config['params']['vertical_barrier_seconds']['value'])
-+
- # Parameters
- parameters = dict()
--model_arch = "TABL"
--parameters["arch"] = model_arch
--parameters["name"] = model
--parameters["WL"] = 200  # WINDOW LONG
--parameters["pt"] = 1
--parameters["sl"] = 1
--parameters["min_ret"] = 0.001 * 1 / 23
--parameters["vbs"] = round(1 / 2,
--                          3)  # Increasing this decreases vertical touches
--parameters["head"] = 1000  # take only first x number of rows 0 means of
--parameters["skip"] = 0  # sample every n'th row if skip > 0
--# get even classes at fraction = 0.8 so the training set is balanced then set to 1
--# 3 million rows is about the limit of before it starts taking ages / memory maxing
--parameters["vol_max"] = (
--    parameters["min_ret"] + 0.00000002
-+wandb.config['params']['head'][
-+    'value'] = 1000  # take only first x number of rows 0 means of
-+volume_max = (
-+    minimum_return + wandb.config['params']['vol_max_modifier']['value']
- )  # The higher this is the more an increase in volatility requries an increase
- # in return to be considered buy/sell (Increasing this increases end barrier vertical touches)
--parameters["vol_min"] = parameters["min_ret"] + 0.00000001
-+volume_min = minimum_return + wandb.config['params']['vol_min_modifier'][
-+    'value']
- 
--parameters["filter"] = "none"
-+filter_type = wandb.config['params']['filter_type']['value']
- 
--if parameters["filter"] == "cm":
--    parameters["cm_vol_mod"] = 500
-+if filter_type == "cm":
-+    cusum_filter_vol_modifier = wandb.config['params'][
-+        'cusum_filter_volume_modifier']['value']
- else:
--    parameters["cm_vol_mod"] = 0
-+    cusum_filter_vol_modifier = 0
- 
--parameters["sw"] = "on"  # sample weights
--parameters["fd"] = "off"
-+use_sample_weights = wandb.config['params']['use_sample_weights']['value']
-+use_sample_weights = wandb.config['params']['use_fractional_differentiation'][
-+    'value']
- 
--parameters["input"] = "obook"
-+input_type = wandb.config['params']['input_type']['value']
- 
--parameters["ntb"] = True  # non time bars
-+# parameters["ntb"] = True  # non time bars
- 
--if parameters["ntb"] == True:
--    # Pick whether you want to add in the time since last bar input feature
--    # time since last bar column
--    parameters["tslbc"] = True  # time since last bar column
--else:
--    # Pick whether you want to add in the volume input feature
--    parameters["vbc"] = True  # volume bar column
-+# if parameters["ntb"] == True:
-+#     # Pick whether you want to add in the time since last bar input feature
-+#     # time since last bar column
-+#     parameters["tslbc"] = True  # time since last bar column
-+# else:
-+#     # Pick whether you want to add in the volume input feature
-+#     parameters["vbc"] = True  # volume bar column
- 
- # Create the txt file string
--parameter_string = "&".join("{}{}{}".format(key, "=", val)
--                            for key, val in parameters.items())
-+parameter_string = wandb.run.id
- 
--pt_sl = [parameters["pt"], parameters["sl"]]
-+pt_sl = [
-+    wandb.config['params']['profit_taking_multiplier']['value'],
-+    wandb.config['params']['stop_loss_multiplier']['value']
-+]
- cpus = cpu_count() - 1
- 
- regenerate_features_and_labels = True
-@@ -192,10 +205,10 @@ if regenerate_features_and_labels == True:
-     if stage == 1:
-         # Side
-         print("starting data load")
--        head = parameters["head"]
-+        head = wandb.config['params']['head']['value']
- 
-         # # read parquet file of dollar bars
--        if parameters["input"] == "bars":
-+        if input_type == "bars":
-             # Mlfinlab bars
-             data = pq.read_pandas(
-                 path_adjust + "data/bars/"
-@@ -216,7 +229,7 @@ if regenerate_features_and_labels == True:
-             data.index = pd.to_datetime(data.index, infer_datetime_format=True)
- 
-         # read parquet file of raw ticks
--        if parameters["input"] == "ticks":
-+        if input_type == "ticks":
-             data = pq.read_pandas(
-                 path_adjust + "data/bars/" +
-                 "btcusdt_agg_trades_raw_tick_data.parquet").to_pandas()
-@@ -238,11 +251,7 @@ if regenerate_features_and_labels == True:
-             # Should do something else than drop the duplicates (maybe trades doesnt have duplicate indexes rather than aggtrades)
-             data = data.loc[~data.index.duplicated(keep="first")]
- 
--            # skip most rows if this is > 1
--            if parameters["skip"] > 0:
--                data = data.iloc[::parameters["skip"], :]
--
--        if parameters["input"] == "obook":
-+        if input_type == "orderbook":
-             with open(path_adjust + "temp/orderbook_data_name.txt",
-                       "r") as text_file:
-                 orderbook_preprocessed_file_name = text_file.read()
-@@ -275,7 +284,7 @@ if regenerate_features_and_labels == True:
-         # duplicate_fast_search(data.index.duplicated())
- 
-         # Fractional differentiation
--        if parameters["fd"] == "on":
-+        if use_sample_weights == "on":
-             data_series = data["close"].to_frame()
-             # # generate 100 points
-             # nsample = 1000
-@@ -313,16 +322,16 @@ if regenerate_features_and_labels == True:
- 
-         start = time.time()
-         volatility_level_array = volatility_levels_numba(
--            np.ascontiguousarray(data.close.values), parameters["WL"])
-+            np.ascontiguousarray(data.close.values),
-+            wandb.config['params']['window_length']['value'])
-         data["window_volatility_level"] = volatility_level_array
- 
-         # Should adjust the max value
-         # To get more vertical touches we can
-         # either increase vol_max or
-         # decrease the window seconds
--        scaler = MinMaxScaler(
--            feature_range=(parameters["vol_min"],
--                           parameters["vol_max"]))  # normalization
-+        scaler = MinMaxScaler(feature_range=(volume_min,
-+                                             volume_max))  # normalization
- 
-         normed_window_volatility_level = scaler.fit_transform(
-             data[["window_volatility_level"]])
-@@ -338,7 +347,7 @@ if regenerate_features_and_labels == True:
-         close_np_array, close_index_np_array = pandas_series_to_numba_ready_np_arrays(
-             close_copy)
- 
--        volatility_threshold = volatility_threshold * parameters["cm_vol_mod"]
-+        volatility_threshold = volatility_threshold * cusum_filter_vol_modifier
-         print("data_len = " + str(len(data)))
-         start = time.time()
-         sampled_idx = filter_events(
-@@ -346,7 +355,7 @@ if regenerate_features_and_labels == True:
-             close_np_array,
-             close_index_np_array,
-             volatility_threshold,
--            parameters["filter"],
-+            filter_type,
-         )
-         print("sampled_idx_len = " + str(len(sampled_idx)))
-         end = time.time()
-@@ -356,7 +365,8 @@ if regenerate_features_and_labels == True:
-         # size
-         start = time.time()
-         volatility_level_array = volatility_levels_numba(
--            data.close.values, parameters["WL"])
-+            data.close.values,
-+            wandb.config['params']['window_length']['value'])
-         data["window_volatility_level"] = volatility_level_array
- 
-     # This code runs for both first and second stage preprocessing
-@@ -364,7 +374,7 @@ if regenerate_features_and_labels == True:
-     vertical_barrier_timestamps = ml.labeling.add_vertical_barrier(
-         t_events=sampled_idx,
-         close=data["close"],
--        num_seconds=parameters["vbs"])
-+        num_seconds=vertical_barrier_seconds)
-     end = time.time()
-     print("vertical barrier" + str(end - start))
- 
-@@ -376,7 +386,7 @@ if regenerate_features_and_labels == True:
-         t_events=sampled_idx,
-         pt_sl=pt_sl,
-         target=data["window_volatility_level"],
--        min_ret=parameters["min_ret"],
-+        min_ret=minimum_return,
-         num_threads=cpus * 2,
-         vertical_barrier_times=vertical_barrier_timestamps,
-         side_prediction=side,
-@@ -533,26 +543,27 @@ if stage == 1:
-     ### ORDERBOOK VOLUME DATA
-     volumes_for_all_labels = volumes.loc[data.close.index]
- 
--    ## TRADE DATA
--    input_features_trade = []
--    close_array = data.close.values
--    input_features_trade.append(close_array)
-+    # ## TRADE DATA
-+    # input_features_trade = []
-+    # close_array = data.close.values
-+    # input_features_trade.append(close_array)
- 
--    if parameters["ntb"] == False and parameters["vbc"] == True:
--        volume_array = data.volume.values
--        input_features_trade.append(volume_array)
--    if parameters["ntb"] == True and parameters["tslbc"] == True:
--        time_since_last_bar_array = data.time_since_last_bar.values
--        input_features_trade.append(time_since_last_bar_array)
-+    # if parameters["ntb"] == False and parameters["vbc"] == True:
-+    #     volume_array = data.volume.values
-+    #     input_features_trade.append(volume_array)
-+    # if parameters["ntb"] == True and parameters["tslbc"] == True:
-+    #     time_since_last_bar_array = data.time_since_last_bar.values
-+    #     input_features_trade.append(time_since_last_bar_array)
- 
-     end_time = time.time()
-     print(end_time - start_time)
- 
--    # min max limits
-+    # Type of scaling to apply
-+    scaling_type = wandb.config['params']['scaling_type']['value']
- 
--    minimum = -1
--    maximum = 1
--    scaling_type = "z_score"
-+    # min max limits
-+    minimum = wandb.config['params']['scaling_maximum']['value']
-+    maximum = wandb.config['params']['scaling_minimum']['value']
- 
-     ### Split intothe training/validation/test sets
-     print("splitting into train/va/test sets start")
-@@ -573,11 +584,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_train = volumes_for_all_labels.iloc[
-         train_close_array_integer_index[0] -
--        parameters["WL"]:train_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        train_close_array_integer_index[-1] + 2]
- 
-     close_index_array_train = close_index_array[
-         train_close_array_integer_index[0] -
--        parameters["WL"]:train_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        train_close_array_integer_index[-1] + 2]
- 
-     end_time = time.time()
- 
-@@ -623,7 +636,7 @@ if stage == 1:
-     # print("Make window started")
-     # start_time = time.time()
- 
--    # padding = parameters["WL"] * 2
-+    # padding = wandb.config['params']['window_length']['value'] * 2
- 
-     # split_by = 100000
- 
-@@ -652,7 +665,7 @@ if stage == 1:
-     #         len(prices_for_window_index_array_train[start_index:end_index]),
-     #         input_features_normalized_train[:, close_and_input_start_index:
-     #                                         close_and_input_end_index],
--    #         parameters["WL"],
-+    #         wandb.config['params']['window_length']['value'],
-     #         model_arch,
-     #     )
- 
-@@ -697,11 +710,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_val = volumes_for_all_labels.iloc[
-         val_close_array_integer_index[0] -
--        parameters["WL"]:val_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        val_close_array_integer_index[-1] + 2]
- 
-     close_index_array_val = close_index_array[
-         val_close_array_integer_index[0] -
--        parameters["WL"]:val_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        val_close_array_integer_index[-1] + 2]
- 
-     input_features_val = make_input_features_from_orderbook_data(
-         volumes_for_all_labels_val)
-@@ -721,7 +736,7 @@ if stage == 1:
-     #     #     prices_for_window_index_array_val,
-     #     #     close_index_array_val,
-     #     #     input_features_normalized_val,
--    #     #     parameters["WL"],
-+    #     #     wandb.config['params']['window_length']['value'],
-     #     #     model_arch,
-     #     # )
- 
-@@ -736,11 +751,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_test = volumes_for_all_labels.iloc[
-         test_close_array_integer_index[0] -
--        parameters["WL"]:test_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        test_close_array_integer_index[-1] + 2]
- 
-     close_index_array_test = close_index_array[
-         test_close_array_integer_index[0] -
--        parameters["WL"]:test_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        test_close_array_integer_index[-1] + 2]
- 
-     input_features_test = make_input_features_from_orderbook_data(
-         volumes_for_all_labels_test)
-@@ -761,7 +778,7 @@ if stage == 1:
- #     #     prices_for_window_index_array_test,
- #     #     close_index_array_test,
- #     #     input_features_normalized_test,
--#     #     parameters["WL"],
-+#     #     wandb.config['params']['window_length']['value'],
- #     #     model_arch,
- #     # )
- 
-@@ -782,8 +799,9 @@ ax = plt.gca()
- 
- data.iloc[window_index - 200:window_index + 200].plot(y="close",
-                                                       use_index=True)
--plot_window_and_touch_and_label(window_index, parameters["WL"], data,
--                                triple_barrier_events, labels)
-+plot_window_and_touch_and_label(
-+    window_index, wandb.config['params']['window_length']['value'], data,
-+    triple_barrier_events, labels)
- 
- data.iloc[window_index - 10:window_index + 30]
- 
-@@ -837,9 +855,9 @@ print("writing train/val/test to .h5 files finished taking " +
- #     h5f.create_dataset("P", data=P)
- # h5f.create_dataset("y", data=y)
- # h5f.create_dataset("sampled_idx_epoch", data=sampled_idx_epoch)
--# if parameters["sw"] == "on":
-+# if use_sample_weights == "on":
- #     h5f.create_dataset("sample_weights", data=sample_weights)
--# elif parameters["sw"] == "off":
-+# elif use_sample_weights == "off":
- #     h5f.create_dataset("sample_weights", data=np.zeros(1))
- # h5f.close()
- 
-diff --git a/pipeline/tabl.py b/pipeline/tabl.py
-index 87d51f8..f0244ec 100644
---- a/pipeline/tabl.py
-+++ b/pipeline/tabl.py
-@@ -6,7 +6,7 @@ import sys
- 
- import imblearn
- from imblearn.over_sampling import SMOTE
--
-+import tensorflow
- import matplotlib.pyplot as plt
- 
- sys.path.append("..")
-@@ -17,29 +17,26 @@ home = str(Path.home())
- sys.path.append(home + "/ProdigyAI")
- 
- from numba import njit, prange
--import tensorflow as tf
-+
- import math
- 
--import keras
- import numpy as np
- from third_party_libraries.TABL import Models
- from third_party_libraries.keras_lr_finder.lr_finder import LRFinder
- from third_party_libraries.CLR.clr_callbacks import CyclicLR
- 
--from tensorflow.keras.losses import categorical_crossentropy
--from tensorflow.keras.optimizers import SGD, Adam
-+import keras
-+from keras.losses import categorical_crossentropy
-+from keras.optimizers import SGD, Adam
- from keras.callbacks import ModelCheckpoint
- 
-+import tensorflow as tf
- # Init wandb
- import wandb
- from wandb.keras import WandbCallback
- 
- import yaml
- 
--# import tensorflow.keras as keras
--# import tensorflow.keras.layers as layers
--# import tensorflow.keras.backend as K
--
- # Sorting out whether we are using the ipython kernel or not
- try:
-     get_ipython()
-@@ -95,25 +92,24 @@ with open(yaml_path) as file:
-     yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
- 
- config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
--wandb.init(
--    dir="~/ProdigyAI/",
--    project="prodigyai",
--    config=config_dictionary,
--)
--
--# try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes
--
--# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1
--template = [[40, 200], [60, 10], [120, 5], [3, 1]]
--
--# example data
--example_x = np.random.rand(1000, 40, 10)
--np.min(example_x)
--np.max(example_x)
--np.mean(example_x)
--example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)
--template = [[40, 200], [60, 10], [120, 5], [3, 1]]
--# 200(WINDOW LENGTH)
-+wandb.init(dir="~/ProdigyAI/",
-+           project="prodigyai",
-+           config=config_dictionary,
-+           resume=True)
-+
-+window_length = wandb.config['params']['window_length']['value']
-+num_features = wandb.config['params']['num_features']['value']
-+epochs = wandb.config['params']['epochs']['value']
-+batch_size = wandb.config['params']['batch_size']['value']
-+dropout = wandb.config['params']['dropout']['value']
-+
-+# # try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes
-+# # example data
-+# example_x = np.random.rand(1000, 40, 10)
-+# np.min(example_x)
-+# np.max(example_x)
-+# np.mean(example_x)
-+# example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)
- 
- ## PRODIGY AI HOCKUS POCKUS START
- from pathlib import Path
-@@ -121,7 +117,7 @@ import h5py
- 
- home = str(Path.home())
- 
--file_name = "arch=TABL&name=two_model&WL=200&pt=1&sl=1&min_ret=4.347826086956522e-05&vbs=0.5&head=1000&skip=0&vol_max=4.3498260869565224e-05&vol_min=4.348826086956522e-05&filter=none&cm_vol_mod=0&sw=on&fd=off&input=obook&ntb=True&tslbc=True.h5"
-+file_name = "esugj36b.h5"
- wandb.config.update({'dataset': file_name})
- path = home + "/ProdigyAI/data/preprocessed/" + file_name
- h5f = h5py.File(path, "r")
-@@ -265,8 +261,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
- 
- n_classes = 3
--dim = (200, 40)
--batch_size = 64
-+dim = (window_length, num_features)
- to_fit = True
- 
- # for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,
-@@ -280,22 +275,15 @@ to_fit = True
- #     import pdb
- #     pdb.set_trace()
- 
-+# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1
-+
-+template = [[num_features, window_length], [60, 10], [120, 5], [3, 1]]
-+
- # get Bilinear model
- projection_regularizer = None
- projection_constraint = keras.constraints.max_norm(3.0, axis=0)
- attention_regularizer = None
- attention_constraint = keras.constraints.max_norm(5.0, axis=1)
--dropout = 0.1
--
--model = Models.TABL(
--    template,
--    dropout,
--    projection_regularizer,
--    projection_constraint,
--    attention_regularizer,
--    attention_constraint,
--)
--model.summary()
- 
- # path = path_adjust + "data/lob_2010/train_and_test_tabl.h5"
- # h5f = h5py.File(path, "r")
-@@ -442,6 +430,18 @@ clr = CyclicLR(base_lr=base_lr,
- 
- optimizer = keras.optimizers.Adam()
- 
-+model = Models.TABL(
-+    template,
-+    dropout,
-+    projection_regularizer,
-+    projection_constraint,
-+    attention_regularizer,
-+    attention_constraint,
-+)
-+model.summary()
-+
-+
-+
- # Compile the model
- model.compile(loss=loss_function, optimizer=optimizer, metrics=["accuracy"])
- # Create a callback that saves the model's weights
-@@ -449,10 +449,15 @@ cp_callback = ModelCheckpoint(filepath=checkpoint_path,
-                               save_weights_only=True,
-                               verbose=1)
- 
--check_point_file = Path(checkpoint_path)
--if check_point_file.exists() and resuming == "resuming":
--    print("weights loaded")
--    model.load_weights(checkpoint_path)
-+if wandb.run.resumed:
-+    weights = wandb.restore("cp.ckpt",
-+                            run_path="garthtrickett/prodigyai/" + wandb.run.id)
-+    model.load_weights(weights.name)
-+
-+# check_point_file = Path(checkpoint_path)
-+# if check_point_file.exists() and resuming == "resuming":
-+#     print("weights loaded")
-+#     model.load_weights(checkpoint_path)
- # Fit data to model
- # history = model.fit(trainX_CNN,
- #                     trainY_CNN,
-@@ -460,9 +465,6 @@ if check_point_file.exists() and resuming == "resuming":
- #                     epochs=20,
- #                     callbacks=[clr, cp_callback])
- 
--finished_weights_path = path_adjust + "temp/cp_end.ckpt"
--model.save_weights(finished_weights_path)
--
- # create class weight
- # class_weight = {0: 1e6 / 300.0, 1: 1e6 / 400.0, 2: 1e6 / 300.0}
- train_generator = DataGenerator(checkpoint_path,
-@@ -489,12 +491,14 @@ steps_per_epoch = len(train_generator)
- validation_steps = len(val_generator)
- 
- # example sata training
--model.fit(train_generator,
--          steps_per_epoch=steps_per_epoch,
--          validation_steps=validation_steps,
--          epochs=1000,
--          validation_data=val_generator,
--          callbacks=[cp_callback, WandbCallback()])
-+model.fit(
-+    train_generator,
-+    steps_per_epoch=steps_per_epoch,
-+    validation_steps=validation_steps,
-+    epochs=epochs,
-+    validation_data=val_generator,
-+    callbacks=[cp_callback,
-+               WandbCallback(save_model=True, monitor="loss")])
- 
- model.save(os.path.join(wandb.run.dir, "model.h5"))
- # no class weight
-@@ -603,16 +607,16 @@ model.save(os.path.join(wandb.run.dir, "model.h5"))
- # X_val = X_val.reshape((X_val.shape[0], 40, 10))
- 
- # train one epoch to find the learning rate
--model.fit(
--    X_train,
--    y_train,
--    validation_data=(X_val, y_val),
--    batch_size=256,
--    epochs=1,
--    shuffle=False,
--)  # no class weight
--
--score = model.evaluate(x=X_test, y=y_test, batch_size=256)
-+# model.fit(
-+#     X_train,
-+#     y_train,
-+#     validation_data=(X_val, y_val),
-+#     batch_size=256,
-+#     epochs=1,
-+#     shuffle=False,
-+# )  # no class weight
-+
-+# score = model.evaluate(x=X_test, y=y_test, batch_size=256)
- 
- # Save model to wandb
- 
-diff --git a/temp/cp.ckpt.data-00000-of-00001 b/temp/cp.ckpt.data-00000-of-00001
-index 56c00e4..3d9600c 100644
-Binary files a/temp/cp.ckpt.data-00000-of-00001 and b/temp/cp.ckpt.data-00000-of-00001 differ
-diff --git a/temp/cp.ckpt.index b/temp/cp.ckpt.index
-index 3770713..d6d3b4b 100644
-Binary files a/temp/cp.ckpt.index and b/temp/cp.ckpt.index differ
-diff --git a/temp/cp_end.ckpt b/temp/cp_end.ckpt
-index c1c1d9e..62e0a67 100644
-Binary files a/temp/cp_end.ckpt and b/temp/cp_end.ckpt differ
-diff --git a/temp/cp_end.ckpt.data-00000-of-00001 b/temp/cp_end.ckpt.data-00000-of-00001
-index 1d572d0..12d3157 100644
-Binary files a/temp/cp_end.ckpt.data-00000-of-00001 and b/temp/cp_end.ckpt.data-00000-of-00001 differ
-diff --git a/temp/cp_end.ckpt.index b/temp/cp_end.ckpt.index
-index 15de09a..0ab67e1 100644
-Binary files a/temp/cp_end.ckpt.index and b/temp/cp_end.ckpt.index differ
-diff --git a/temp/is_script_finished.txt b/temp/is_script_finished.txt
-index 40ca0d3..bec7a08 100644
---- a/temp/is_script_finished.txt
-+++ b/temp/is_script_finished.txt
-@@ -1 +1 @@
--full_script_time64.39192986488342
-\ No newline at end of file
-+full_script_time25.707124948501587
-\ No newline at end of file
-Submodule third_party_libraries/TABL contains modified content
-Submodule third_party_libraries/TABL 7f25314..df3e92d:
-diff --git a/third_party_libraries/TABL/Layers.py b/third_party_libraries/TABL/Layers.py
-index 9020002..b299e57 100644
---- a/third_party_libraries/TABL/Layers.py
-+++ b/third_party_libraries/TABL/Layers.py
-@@ -3,8 +3,9 @@
- """
- @author: Dat Tran (dat.tranthanh@tut.fi)
- """
-+import keras
- from keras import backend as K
--from keras.engine.topology import Layer
-+from keras.layers import Layer
- from keras import activations as Activations
- from keras import initializers as Initializers
- 
-@@ -18,25 +19,25 @@ class Constraint(object):
- 
-     def get_config(self):
-         return {}
--    
-+
-+
- class MinMax(Constraint):
-     """
-     Customized min-max constraint for scalar
-     """
--
-     def __init__(self, min_value=0.0, max_value=10.0):
-         self.min_value = min_value
-         self.max_value = max_value
-+
-     def __call__(self, w):
--        
--        return K.clip(w,self.min_value,self.max_value)
-+
-+        return K.clip(w, self.min_value, self.max_value)
- 
-     def get_config(self):
--        return {'min_value': self.min_value,
--                'max_value': self.max_value}
--        
-+        return {'min_value': self.min_value, 'max_value': self.max_value}
- 
--def nmodeproduct(x,w,mode):
-+
-+def nmodeproduct(x, w, mode):
-     """
-     n-mode product for 2D matrices
-     x: NxHxW
-@@ -45,74 +46,83 @@ def nmodeproduct(x,w,mode):
-     
-     output: NxhxW (mode1) or NxHxw (mode2)
-     """
--    if mode==2:
--        x=K.dot(x,w)
-+    if mode == 2:
-+        x = K.dot(x, w)
-     else:
--        x=K.permute_dimensions(x,(0,2,1))
--        x=K.dot(x,w)
--        x=K.permute_dimensions(x,(0,2,1))
-+        x = K.permute_dimensions(x, (0, 2, 1))
-+        x = K.dot(x, w)
-+        x = K.permute_dimensions(x, (0, 2, 1))
-     return x
- 
-+
- class BL(Layer):
-     """
-     Bilinear Layer
-     """
--    def __init__(self, output_dim,
-+    def __init__(self,
-+                 output_dim,
-                  kernel_regularizer=None,
--                 kernel_constraint=None,**kwargs):
-+                 kernel_constraint=None,
-+                 **kwargs):
-         """
-         output_dim : output dimensions of 2D tensor, should be a list of len 2, e.g. [30,20]
-         kernel_regularizer : keras regularizer object
-         kernel_constraint: keras constraint object
-         """
--        
-+
-         self.output_dim = output_dim
--        self.kernel_regularizer=kernel_regularizer
--        self.kernel_constraint=kernel_constraint
--        
-+        self.kernel_regularizer = kernel_regularizer
-+        self.kernel_constraint = kernel_constraint
-+
-         super(BL, self).__init__(**kwargs)
- 
-     def build(self, input_shape):
--        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
--                                      initializer='he_uniform',
--                                      regularizer=self.kernel_regularizer,
--                                      constraint=self.kernel_constraint,
--                                      trainable=True)
--        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
--                                      initializer='he_uniform',
--                                      regularizer=self.kernel_regularizer,
--                                      constraint=self.kernel_constraint,
--                                      trainable=True)
--
--        self.bias=self.add_weight(name='bias',shape=(self.output_dim[0],self.output_dim[1]),
--                              initializer='zeros',trainable=True)
-+        self.W1 = self.add_weight(name='W1',
-+                                  shape=(input_shape[1], self.output_dim[0]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.kernel_regularizer,
-+                                  constraint=self.kernel_constraint,
-+                                  trainable=True)
-+        self.W2 = self.add_weight(name='W2',
-+                                  shape=(input_shape[2], self.output_dim[1]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.kernel_regularizer,
-+                                  constraint=self.kernel_constraint,
-+                                  trainable=True)
-+
-+        self.bias = self.add_weight(name='bias',
-+                                    shape=(self.output_dim[0],
-+                                           self.output_dim[1]),
-+                                    initializer='zeros',
-+                                    trainable=True)
-         super(BL, self).build(input_shape)
- 
-     def call(self, x):
-         print(K.int_shape(x))
--        x = nmodeproduct(x,self.W1,1)
--        x = nmodeproduct(x,self.W2,2)
--        x = K.bias_add(x,self.bias)
--        
--        if self.output_dim[1]==1:
--            x = K.squeeze(x,axis=-1)
-+        x = nmodeproduct(x, self.W1, 1)
-+        x = nmodeproduct(x, self.W2, 2)
-+        x = K.bias_add(x, self.bias)
-+
-+        if self.output_dim[1] == 1:
-+            x = K.squeeze(x, axis=-1)
-         print(K.int_shape(x))
-         return x
- 
-     def compute_output_shape(self, input_shape):
--        if self.output_dim[1]==1:
-+        if self.output_dim[1] == 1:
-             return (input_shape[0], self.output_dim[0])
-         else:
-             return (input_shape[0], self.output_dim[0], self.output_dim[1])
--        
--        
-+
-+
- class TABL(Layer):
-     """
-     Temporal Attention augmented Bilinear Layer
-     https://arxiv.org/abs/1712.00975
-     
-     """
--    def __init__(self, output_dim,
-+    def __init__(self,
-+                 output_dim,
-                  projection_regularizer=None,
-                  projection_constraint=None,
-                  attention_regularizer=None,
-@@ -125,45 +135,52 @@ class TABL(Layer):
-         attention_regularizer: keras regularizer object for attention matrix
-         attention_constraint: keras constraint object for attention matrix
-         """
--        
-+
-         self.output_dim = output_dim
-         self.projection_regularizer = projection_regularizer
-         self.projection_constraint = projection_constraint
-         self.attention_regularizer = attention_regularizer
-         self.attention_constraint = attention_constraint
--        
-+
-         super(TABL, self).__init__(**kwargs)
- 
-     def build(self, input_shape):
--        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
--                                      initializer='he_uniform',
--                                      regularizer=self.projection_regularizer,
--                                      constraint=self.projection_constraint,
--                                      trainable=True)
--        
--        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
--                                      initializer='he_uniform',
--                                      regularizer=self.projection_regularizer,
--                                      constraint=self.projection_constraint,
--                                      trainable=True)
--        
--        self.W = self.add_weight(name='W',shape=(input_shape[2], input_shape[2]),
--                                      initializer=Initializers.Constant(1.0/input_shape[2]),
--                                      regularizer=self.attention_regularizer,
--                                      constraint=self.attention_constraint,
--                                      trainable=True)
--        
--        self.alpha = self.add_weight(name='alpha',shape=(1,),
--                                      initializer=Initializers.Constant(0.5),
--                                      constraint=MinMax(),
--                                      trainable=True)
--
--
--        self.bias=self.add_weight(name='bias',shape=(1, self.output_dim[0],self.output_dim[1]),
--                              initializer='zeros',trainable=True)
--        
-+        self.W1 = self.add_weight(name='W1',
-+                                  shape=(input_shape[1], self.output_dim[0]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.projection_regularizer,
-+                                  constraint=self.projection_constraint,
-+                                  trainable=True)
-+
-+        self.W2 = self.add_weight(name='W2',
-+                                  shape=(input_shape[2], self.output_dim[1]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.projection_regularizer,
-+                                  constraint=self.projection_constraint,
-+                                  trainable=True)
-+
-+        self.W = self.add_weight(name='W',
-+                                 shape=(input_shape[2], input_shape[2]),
-+                                 initializer=Initializers.Constant(
-+                                     1.0 / input_shape[2]),
-+                                 regularizer=self.attention_regularizer,
-+                                 constraint=self.attention_constraint,
-+                                 trainable=True)
-+
-+        self.alpha = self.add_weight(name='alpha',
-+                                     shape=(1, ),
-+                                     initializer=Initializers.Constant(0.5),
-+                                     constraint=MinMax(),
-+                                     trainable=True)
-+
-+        self.bias = self.add_weight(name='bias',
-+                                    shape=(1, self.output_dim[0],
-+                                           self.output_dim[1]),
-+                                    initializer='zeros',
-+                                    trainable=True)
-+
-         self.in_shape = input_shape
--        
-+
-         super(TABL, self).build(input_shape)
- 
-     def call(self, x):
-@@ -174,27 +191,26 @@ class TABL(Layer):
-         W: D2 x D2
-         """
-         # first mode projection
--        x = nmodeproduct(x,self.W1,1) # N x d1 x D2
--        # enforcing constant (1) on the diagonal 
--        W = self.W-self.W*K.eye(self.in_shape[2],dtype='float32')+K.eye(self.in_shape[2],dtype='float32')/self.in_shape[2]
--        # calculate attention 
--        attention = Activations.softmax(nmodeproduct(x,W,2),axis=-1) # N x d1 x D2
-+        x = nmodeproduct(x, self.W1, 1)  # N x d1 x D2
-+        # enforcing constant (1) on the diagonal
-+        W = self.W - self.W * K.eye(self.in_shape[2], dtype='float32') + K.eye(
-+            self.in_shape[2], dtype='float32') / self.in_shape[2]
-+        # calculate attention
-+        attention = Activations.softmax(nmodeproduct(x, W, 2),
-+                                        axis=-1)  # N x d1 x D2
-         # apply attention
--        x = self.alpha*x + (1.0 - self.alpha)*x*attention
-+        x = self.alpha * x + (1.0 - self.alpha) * x * attention
-         # second mode projection
--        x = nmodeproduct(x,self.W2,2)
-+        x = nmodeproduct(x, self.W2, 2)
-         # bias add
-         x = x + self.bias
--        
--        if self.output_dim[1]==1:
--            x = K.squeeze(x,axis=-1)
-+
-+        if self.output_dim[1] == 1:
-+            x = K.squeeze(x, axis=-1)
-         return x
- 
-     def compute_output_shape(self, input_shape):
--        if self.output_dim[1]==1:
-+        if self.output_dim[1] == 1:
-             return (input_shape[0], self.output_dim[0])
-         else:
-             return (input_shape[0], self.output_dim[0], self.output_dim[1])
--
--
--
-diff --git a/third_party_libraries/TABL/Models.py b/third_party_libraries/TABL/Models.py
-index e2bf62c..d5c4c45 100644
---- a/third_party_libraries/TABL/Models.py
-+++ b/third_party_libraries/TABL/Models.py
-@@ -4,7 +4,6 @@
- @author: Dat Tran (dat.tranthanh@tut.fi)
- """
- 
--
- from third_party_libraries.TABL import Layers
- import keras
- 
-@@ -74,7 +73,8 @@ def TABL(
- 
-     x = inputs
-     for k in range(1, len(template) - 1):
--        x = Layers.BL(template[k], projection_regularizer, projection_constraint)(x)
-+        x = Layers.BL(template[k], projection_regularizer,
-+                      projection_constraint)(x)
-         x = keras.layers.Activation("relu")(x)
-         x = keras.layers.Dropout(dropout)(x)
- 
-@@ -94,4 +94,3 @@ def TABL(
-     model.compile(optimizer, "categorical_crossentropy", ["acc"])
- 
-     return model
--
-diff --git a/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc
-new file mode 100644
-index 0000000..ff0a753
-Binary files /dev/null and b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc differ
-diff --git a/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc
-new file mode 100644
-index 0000000..a57f1ba
-Binary files /dev/null and b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc differ
-diff --git a/yaml/tabl.yaml b/yaml/tabl.yaml
-index 2cdc7b2..456e287 100644
---- a/yaml/tabl.yaml
-+++ b/yaml/tabl.yaml
-@@ -4,4 +4,14 @@ epochs:
-   value: 100
- batch_size:
-   desc: Size of each mini-batch
--  value: 32
-+  value: 256
-+window_length:
-+  desc: null
-+  value: 200
-+num_features:
-+  desc: null
-+  value: 40
-+dropout:
-+  desc: null
-+  value: 0.1
-+
diff --git a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/media/graph/graph_summary_6f818560.graph.json b/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/media/graph/graph_summary_6f818560.graph.json
deleted file mode 100644
index fcd5b7c..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/media/graph/graph_summary_6f818560.graph.json
+++ /dev/null
@@ -1 +0,0 @@
-{"format": "keras", "nodes": [{"name": "input_1", "id": "input_1", "class_name": "InputLayer", "output_shape": [null, 40, 200], "num_parameters": 0}, {"name": "bl_1", "id": "bl_1", "class_name": "BL", "output_shape": [null, 60, 10], "num_parameters": 5000}, {"name": "activation_1", "id": "activation_1", "class_name": "Activation", "output_shape": [null, 60, 10], "num_parameters": 0}, {"name": "dropout_1", "id": "dropout_1", "class_name": "Dropout", "output_shape": [null, 60, 10], "num_parameters": 0}, {"name": "bl_2", "id": "bl_2", "class_name": "BL", "output_shape": [null, 120, 5], "num_parameters": 7850}, {"name": "activation_2", "id": "activation_2", "class_name": "Activation", "output_shape": [null, 120, 5], "num_parameters": 0}, {"name": "dropout_2", "id": "dropout_2", "class_name": "Dropout", "output_shape": [null, 120, 5], "num_parameters": 0}, {"name": "tabl_1", "id": "tabl_1", "class_name": "TABL", "output_shape": [null, 3], "num_parameters": 394}, {"name": "activation_3", "id": "activation_3", "class_name": "Activation", "output_shape": [null, 3], "num_parameters": 0}], "edges": [["input_1", "bl_1"], ["bl_1", "activation_1"], ["activation_1", "dropout_1"], ["dropout_1", "bl_2"], ["bl_2", "activation_2"], ["activation_2", "dropout_2"], ["dropout_2", "tabl_1"], ["tabl_1", "activation_3"]]}
\ No newline at end of file
diff --git a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/model-best.h5 b/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/model-best.h5
deleted file mode 100644
index 6d031fc..0000000
Binary files a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/model-best.h5 and /dev/null differ
diff --git a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/requirements.txt b/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/requirements.txt
deleted file mode 100644
index c4244a7..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/requirements.txt
+++ /dev/null
@@ -1,339 +0,0 @@
-absl-py==0.9.0
-alabaster==0.7.12
-anaconda-client==1.7.2
-anaconda-project==0.8.3
-appdirs==1.4.3
-argparse==1.4.0
-asn1crypto==1.3.0
-astor==0.8.1
-astroid==2.3.3
-astropy==4.0
-asyncore-wsgi==0.0.9
-atomicwrites==1.3.0
-attrs==19.3.0
-autobahn==20.2.1
-autoflake==1.3.1
-automat==0.8.0
-babel==2.8.0
-backcall==0.1.0
-backports.os==0.1.1
-backports.shutil-get-terminal-size==1.0.0
-beautifulsoup4==4.8.2
-bitarray==1.2.1
-bkcharts==0.2
-black==19.10b0
-bleach==3.1.0
-blis==0.4.1
-bokeh==1.4.0
-boto3==1.12.37
-boto==2.49.0
-botocore==1.15.37
-bottle==0.12.18
-bottleneck==1.3.1
-cachetools==4.0.0
-catalogue==1.0.0
-cchardet==2.1.6
-certifi==2019.11.28
-cffi==1.13.2
-chardet==3.0.4
-click==7.0
-cloudpickle==1.2.2
-clyent==1.2.2
-colorama==0.4.3
-configparser==5.0.0
-constantly==15.1.0
-contextlib2==0.6.0.post1
-cryptography==2.8
-cvxpy==1.0.31
-cycler==0.10.0
-cymem==2.0.3
-cython==0.29.14
-cytoolz==0.10.1
-dask==2.10.1
-dateparser==0.7.2
-decorator==4.4.1
-defusedxml==0.6.0
-dill==0.3.1.1
-distributed==2.10.0
-docker-pycreds==0.4.0
-docutils==0.15.2
-easyprocess==0.2.10
-ecos==2.0.7.post1
-entrypoints==0.3
-et-xmlfile==1.0.1
-fastai2==0.0.17
-fastcache==1.1.0
-fastcore==0.1.17
-fastprogress==0.2.2
-filelock==3.0.12
-flake8==3.7.9
-flask==1.1.1
-fsspec==0.6.2
-future==0.18.2
-gast==0.2.2
-gevent==1.4.0
-gitdb==4.0.5
-gitpython==3.1.2
-glob2==0.7
-gmpy2==2.0.8
-google-api-core==1.16.0
-google-api-python-client==1.8.0
-google-auth-httplib2==0.0.3
-google-auth-oauthlib==0.4.1
-google-auth==1.13.1
-google-cloud-profiler==1.0.9
-google-pasta==0.1.8
-googleapis-common-protos==1.51.0
-gql==0.2.0
-graphql-core==1.1
-greenlet==0.4.15
-grpcio==1.27.1
-h5py==2.10.0
-hanging-threads==2.0.5
-heapdict==1.0.1
-html5lib==1.0.1
-httplib2==0.17.2
-hyperlink==19.0.0
-hypothesis==5.4.1
-idna==2.8
-imageio==2.6.1
-imagesize==1.2.0
-imbalanced-learn==0.6.1
-imblearn==0.0
-importlib-metadata==1.5.0
-incremental==17.5.0
-inflect==4.1.0
-ipykernel==5.1.4
-ipython-genutils==0.2.0
-ipython==7.12.0
-ipywidgets==7.5.1
-isort==4.3.21
-itsdangerous==1.1.0
-jaraco.itertools==5.0.0
-jdcal==1.4.1
-jedi==0.16.0
-jeepney==0.4.2
-jieba==0.42.1
-jinja2==2.11.1
-jmespath==0.9.5
-joblib==0.14.1
-json5==0.9.0
-jsonschema==3.2.0
-jupyter-client==5.3.4
-jupyter-console==6.1.0
-jupyter-core==4.6.1
-jupyter==1.0.0
-jupyterlab-server==1.0.6
-jupyterlab==1.2.6
-keras-applications==1.0.8
-keras-bert==0.81.0
-keras-embed-sim==0.7.0
-keras-layer-normalization==0.14.0
-keras-multi-head==0.22.0
-keras-pos-embd==0.11.0
-keras-position-wise-feed-forward==0.6.0
-keras-preprocessing==1.1.0
-keras-self-attention==0.41.0
-keras-transformer==0.32.0
-keras==2.3.1
-keyring==21.1.0
-kiwisolver==1.1.0
-ktrain==0.12.3
-langdetect==1.0.8
-lazy-object-proxy==1.4.3
-libarchive-c==2.8
-lief==0.9.0
-llvmlite==0.31.0
-locket==0.2.0
-lxml==4.5.0
-markdown==3.2
-markupsafe==1.1.1
-matplotlib==3.1.3
-mccabe==0.6.1
-mistune==0.8.4
-mkl-fft==1.0.15
-mkl-random==1.1.0
-mkl-service==2.3.0
-mock==3.0.5
-more-itertools==8.2.0
-mpmath==1.1.0
-msgpack==1.0.0
-multipledispatch==0.6.0
-multiprocess==0.70.9
-murmurhash==1.0.2
-nbconvert==5.6.1
-nbformat==5.0.4
-networkx==2.4
-nltk==3.4.5
-nose==1.3.7
-notebook==6.0.3
-numba==0.48.0
-numexpr==2.7.1
-numpy==1.18.1
-numpydoc==0.9.2
-nvidia-ml-py3==7.352.0
-oauthlib==3.1.0
-olefile==0.46
-opencv-python==4.2.0.34
-openpyxl==3.0.3
-opt-einsum==3.1.0
-osqp==0.6.1
-packaging==20.1
-pandas==1.0.3
-pandocfilters==1.4.2
-parse==1.6.6
-parso==0.6.0
-partd==1.1.0
-path==13.1.0
-pathlib2==2.3.5
-pathspec==0.7.0
-pathtools==0.1.2
-patsy==0.5.1
-pep8==1.7.1
-pexpect==4.8.0
-pickleshare==0.7.5
-pillow==7.0.0
-pip==20.0.2
-pkginfo==1.5.0.1
-plac==1.1.3
-pluggy==0.13.1
-ply==3.11
-preshed==3.0.2
-prometheus-client==0.7.1
-promise==2.3
-prompt-toolkit==3.0.3
-protobuf==3.11.3
-psutil==5.6.7
-psycopg2-binary==2.8.4
-ptyprocess==0.6.0
-py==1.8.1
-pyarrow==0.16.0
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycodestyle==2.5.0
-pycosat==0.6.3
-pycparser==2.19
-pycrypto==2.6.1
-pycurl==7.43.0.5
-pydbus==0.6.0
-pyflakes==2.1.1
-pygments==2.5.2
-pyhamcrest==2.0.0
-pylint==2.4.4
-pyodbc==4.0.28
-pyopenssl==19.1.0
-pyparsing==2.4.6
-pyrsistent==0.15.7
-pysocks==1.7.1
-pytest-arraydiff==0.3
-pytest-astropy-header==0.1.2
-pytest-astropy==0.7.0
-pytest-doctestplus==0.5.0
-pytest-openfiles==0.4.0
-pytest-remotedata==0.3.2
-pytest==5.3.5
-python-dateutil==2.8.1
-pyts==0.10.0
-pytz==2019.3
-pyunpack==0.1.2
-pywavelets==1.1.1
-pyyaml==5.3
-pyzmq==18.1.1
-qtawesome==0.6.1
-qtconsole==4.6.0
-qtpy==1.9.0
-regex==2020.1.8
-requests-oauthlib==1.3.0
-requests==2.22.0
-rope==0.16.0
-rsa==4.0
-ruamel-yaml==0.15.87
-s3transfer==0.3.3
-sacremoses==0.0.38
-scikit-image==0.16.2
-scikit-learn==0.22.1
-scipy==1.4.1
-scoop==0.7.1.1
-scs==2.1.2
-seaborn==0.10.0
-secretstorage==3.1.2
-send2trash==1.5.0
-sentencepiece==0.1.85
-sentry-sdk==0.14.3
-seqeval==0.0.12
-service-identity==18.1.0
-setuptools==45.1.0.post20200127
-shortuuid==1.0.1
-simplegeneric==0.8.1
-singledispatch==3.4.0.3
-six==1.14.0
-smmap==3.0.4
-snowballstemmer==2.0.0
-sortedcollections==1.1.2
-sortedcontainers==2.1.0
-soupsieve==1.9.5
-spacy==2.2.3
-sphinx==2.3.1
-sphinxcontrib-applehelp==1.0.1
-sphinxcontrib-devhelp==1.0.1
-sphinxcontrib-htmlhelp==1.0.2
-sphinxcontrib-jsmath==1.0.1
-sphinxcontrib-qthelp==1.0.2
-sphinxcontrib-serializinghtml==1.1.3
-sphinxcontrib-websupport==1.1.2
-spyder-kernels==0.5.2
-spyder==3.3.6
-sqlalchemy==1.3.13
-srsly==1.0.1
-statsmodels==0.11.0
-subprocess32==3.5.4
-sympy==1.5.1
-syntok==1.2.2
-tables==3.6.1
-tblib==1.6.0
-tensorboard==2.1.1
-tensorflow-datasets==2.1.0
-tensorflow-estimator==2.1.0
-tensorflow-metadata==0.21.1
-tensorflow==2.1.0
-termcolor==1.1.0
-terminado==0.8.3
-testpath==0.4.4
-thinc==7.3.1
-tokenizers==0.5.2
-toml==0.10.0
-toolz==0.10.0
-torch==1.4.0
-torchvision==0.5.0
-tornado==6.0.3
-tqdm==4.42.0
-traitlets==4.3.3
-transformers==2.8.0
-twisted==19.10.0
-txaio==20.1.1
-typed-ast==1.4.1
-tzlocal==2.0.0
-unicodecsv==0.14.1
-uritemplate==3.0.1
-urllib3==1.25.8
-wandb==0.8.35
-wasabi==0.6.0
-watchdog==0.10.2
-wcwidth==0.1.8
-web-pdb==1.5.1
-webencodings==0.5.1
-websocket-client==0.57.0
-weresync==1.1.5
-werkzeug==0.16.1
-wheel==0.34.2
-widgetsnbextension==3.5.1
-wrapt==1.11.2
-wurlitzer==2.0.0
-xlrd==1.2.0
-xlsxwriter==1.2.7
-xlwt==1.3.0
-yapf==0.29.0
-yapsy==1.11.223
-zict==1.0.0
-zipp==2.1.0
-zope.interface==4.7.1
\ No newline at end of file
diff --git a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/wandb-events.jsonl b/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/wandb-events.jsonl
deleted file mode 100644
index 7544bcd..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/wandb-events.jsonl
+++ /dev/null
@@ -1 +0,0 @@
-{"system.cpu": 34.33, "system.memory": 11.95, "system.disk": 23.5, "system.proc.memory.availableMB": 34448.08, "system.proc.memory.rssMB": 688.8, "system.proc.memory.percent": 1.76, "system.proc.cpu.threads": 30.17, "system.network.sent": 397050, "system.network.recv": 142308, "_wandb": true, "_timestamp": 1589165826, "_runtime": 9}
diff --git a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/wandb-history.jsonl b/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/wandb-history.jsonl
deleted file mode 100644
index 0d803c6..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/wandb-history.jsonl
+++ /dev/null
@@ -1,36 +0,0 @@
-{"epoch": 0, "val_loss": NaN, "val_accuracy": 0.19140625, "loss": 2.3455004692077637, "accuracy": 0.3125, "_runtime": 4.393453359603882, "_timestamp": 1589165821.4874327, "_step": 0}
-{"epoch": 1, "val_loss": 0.929656982421875, "val_accuracy": 0.19921875, "loss": 1.9181803464889526, "accuracy": 0.31640625, "_runtime": 4.571531534194946, "_timestamp": 1589165821.665511, "_step": 1}
-{"epoch": 2, "val_loss": 0.8063456416130066, "val_accuracy": 0.1953125, "loss": 1.622144103050232, "accuracy": 0.337890625, "_runtime": 4.7628014087677, "_timestamp": 1589165821.8567808, "_step": 2}
-{"epoch": 3, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.365234375, "_runtime": 4.922028303146362, "_timestamp": 1589165822.0160077, "_step": 3}
-{"epoch": 4, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.0817883014678955, "_timestamp": 1589165822.1757677, "_step": 4}
-{"epoch": 5, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.2040815353393555, "_timestamp": 1589165822.298061, "_step": 5}
-{"epoch": 6, "val_loss": NaN, "val_accuracy": 0.7265625, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.383246898651123, "_timestamp": 1589165822.4772263, "_step": 6}
-{"epoch": 7, "val_loss": NaN, "val_accuracy": 0.5859375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.521531105041504, "_timestamp": 1589165822.6155105, "_step": 7}
-{"epoch": 8, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.680713176727295, "_timestamp": 1589165822.7746925, "_step": 8}
-{"epoch": 9, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 5.811899900436401, "_timestamp": 1589165822.9058793, "_step": 9}
-{"epoch": 10, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.928986549377441, "_timestamp": 1589165823.022966, "_step": 10}
-{"epoch": 11, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 6.0942652225494385, "_timestamp": 1589165823.1882446, "_step": 11}
-{"epoch": 12, "val_loss": NaN, "val_accuracy": 0.7265625, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.231035470962524, "_timestamp": 1589165823.3250148, "_step": 12}
-{"epoch": 13, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.358723878860474, "_timestamp": 1589165823.4527032, "_step": 13}
-{"epoch": 14, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.519897699356079, "_timestamp": 1589165823.613877, "_step": 14}
-{"epoch": 15, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.3671875, "_runtime": 6.63865065574646, "_timestamp": 1589165823.73263, "_step": 15}
-{"epoch": 16, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.80715012550354, "_timestamp": 1589165823.9011295, "_step": 16}
-{"epoch": 17, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.3671875, "_runtime": 6.945959568023682, "_timestamp": 1589165824.039939, "_step": 17}
-{"epoch": 18, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.063310384750366, "_timestamp": 1589165824.1572897, "_step": 18}
-{"epoch": 19, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.233299493789673, "_timestamp": 1589165824.3272789, "_step": 19}
-{"epoch": 20, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.342777967453003, "_timestamp": 1589165824.4367573, "_step": 20}
-{"epoch": 21, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.5305657386779785, "_timestamp": 1589165824.624545, "_step": 21}
-{"epoch": 22, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.643781423568726, "_timestamp": 1589165824.7377608, "_step": 22}
-{"epoch": 23, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.3671875, "_runtime": 7.76062798500061, "_timestamp": 1589165824.8546073, "_step": 23}
-{"epoch": 24, "val_loss": NaN, "val_accuracy": 0.80078125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.9320595264434814, "_timestamp": 1589165825.026039, "_step": 24}
-{"epoch": 25, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.057352781295776, "_timestamp": 1589165825.1513321, "_step": 25}
-{"epoch": 26, "val_loss": NaN, "val_accuracy": 0.80078125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.174784660339355, "_timestamp": 1589165825.268764, "_step": 26}
-{"epoch": 27, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.325506448745728, "_timestamp": 1589165825.4194858, "_step": 27}
-{"epoch": 28, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 8.446620225906372, "_timestamp": 1589165825.5405996, "_step": 28}
-{"epoch": 29, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.567692995071411, "_timestamp": 1589165825.6616724, "_step": 29}
-{"epoch": 30, "val_loss": NaN, "val_accuracy": 0.80078125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.678091526031494, "_timestamp": 1589165825.772071, "_step": 30}
-{"epoch": 31, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.855355739593506, "_timestamp": 1589165825.949335, "_step": 31}
-{"epoch": 32, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.98594856262207, "_timestamp": 1589165826.079928, "_step": 32}
-{"epoch": 33, "val_loss": NaN, "val_accuracy": 0.7265625, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.106799840927124, "_timestamp": 1589165826.2007792, "_step": 33}
-{"epoch": 34, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.3671875, "_runtime": 9.262706756591797, "_timestamp": 1589165826.356686, "_step": 34}
-{"epoch": 35, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.39004135131836, "_timestamp": 1589165826.4840207, "_step": 35}
diff --git a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/wandb-metadata.json b/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/wandb-metadata.json
deleted file mode 100644
index b00768f..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/wandb-metadata.json
+++ /dev/null
@@ -1,23 +0,0 @@
-{
-    "root": "/home/garthtrickett/ProdigyAI",
-    "program": "tabl.py",
-    "git": {
-        "remote": "https://github.com/garthtrickett/ProdigyAI",
-        "commit": "7667fefed2018a0664418c25c97e3c08866a6f71"
-    },
-    "email": "garthtrickett@gmail.com",
-    "startedAt": "2020-05-11T02:56:57.354397",
-    "host": "preemp-cpu-1",
-    "username": "garthtrickett",
-    "executable": "/home/garthtrickett/miniconda3/envs/ProdigyAI/bin/python",
-    "os": "Linux-5.3.0-1018-gcp-x86_64-with-debian-buster-sid",
-    "python": "3.7.6",
-    "cpu_count": 6,
-    "args": [],
-    "state": "killed",
-    "jobType": null,
-    "mode": "run",
-    "project": "prodigyai",
-    "heartbeatAt": "2020-05-11T02:57:07.113983",
-    "exitcode": 255
-}
diff --git a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/wandb-summary.json b/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/wandb-summary.json
deleted file mode 100644
index 8a57edb..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025657-33cb2k5u/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"graph": {"_type": "graph-file", "path": "media/graph/graph_summary_6f818560.graph.json", "sha256": "6f81856020e576034c23528bd5c8cc156872ed948db82ecb89f721130ae37682", "size": 1318}, "_runtime": 9.39004135131836, "val_accuracy": 0.8984375, "loss": NaN, "_timestamp": 1589165826.4840207, "_step": 35, "epoch": 35, "val_loss": NaN, "accuracy": 0.37109375, "best_loss": 1.622144103050232, "best_epoch": 2}
diff --git a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/config.yaml b/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/config.yaml
deleted file mode 100644
index 8c1a8a9..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/config.yaml
+++ /dev/null
@@ -1,34 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.8.35
-    framework: keras
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.7.6
-dataset:
-  desc: null
-  value: esugj36b.h5
-params:
-  desc: null
-  value:
-    batch_size:
-      desc: Size of each mini-batch
-      value: 256
-    dropout:
-      desc: null
-      value: 0.1
-    epochs:
-      desc: Number of epochs to train over
-      value: 100
-    num_features:
-      desc: null
-      value: 40
-    window_length:
-      desc: null
-      value: 200
-yaml:
-  desc: null
-  value: yaml/tabl.yaml
diff --git a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/cp.ckpt b/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/cp.ckpt
deleted file mode 100644
index 4f7f2f2..0000000
Binary files a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/cp.ckpt and /dev/null differ
diff --git a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/diff.patch b/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/diff.patch
deleted file mode 100644
index 214be23..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/diff.patch
+++ /dev/null
@@ -1,1270 +0,0 @@
-diff --git a/pipeline/deeplob.py b/pipeline/deeplob.py
-index 2d6c076..fef42d4 100644
---- a/pipeline/deeplob.py
-+++ b/pipeline/deeplob.py
-@@ -2,6 +2,7 @@
- import pandas as pd
- import pickle
- import numpy as np
-+import tensorflow
- import tensorflow as tf
- from tensorflow.keras import layers
- 
-@@ -25,21 +26,17 @@ from keras.utils import np_utils
- import matplotlib.pyplot as plt
- import math
- from numba import njit, prange
--
--# Init wandb
--import wandb
--from wandb.keras import WandbCallback
--wandb.init(project="prodigyai")
-+import yaml
- 
- # set random seeds
- np.random.seed(1)
- tf.random.set_seed(2)
- 
- import keras
--from keras.callbacks import ModelCheckpoint
--from keras import backend as K
-+from tensorflow.keras.callbacks import ModelCheckpoint
- import h5py
--
-+import wandb
-+from wandb.keras import WandbCallback
- # check if using gpu
- gpus = tf.config.list_physical_devices()
- any_gpus = [s for s in gpus if "GPU" in s[0]]
-@@ -105,6 +102,23 @@ if cwd == home + "/":
-     cwd = cwd + "/ProdigyAI"
-     path_adjust = cwd
- 
-+# Init wandb
-+yaml_path = path_adjust + "yaml/deeplob.yaml"
-+with open(yaml_path) as file:
-+    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
-+
-+config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
-+wandb.init(dir="~/ProdigyAI/",
-+           project="prodigyai",
-+           config=config_dictionary,
-+           resume=True)
-+
-+window_length = wandb.config['params']['window_length']['value']
-+num_features = wandb.config['params']['num_features']['value']
-+epochs = wandb.config['params']['epochs']['value']
-+batch_size = wandb.config['params']['batch_size']['value']
-+number_of_lstm = wandb.config['params']['number_of_lstm']['value']
-+
- # limit gpu usage for keras
- gpu_devices = tf.config.experimental.list_physical_devices("GPU")
- for device in gpu_devices:
-@@ -216,7 +230,8 @@ import h5py
- 
- home = str(Path.home())
- 
--file_name = "arch=DLOB&name=two_model&WL=200&pt=1&sl=1&min_ret=4.347826086956522e-05&vbs=0.5&head=0&skip=0&vol_max=4.3498260869565224e-05&vol_min=4.348826086956522e-05&filter=none&cm_vol_mod=0&sw=on&fd=off&input=obook&ntb=True&tslbc=True.h5"
-+file_name = "esugj36b.h5"
-+wandb.config.update({'dataset': file_name})
- path = home + "/ProdigyAI/data/preprocessed/" + file_name
- h5f = h5py.File(path, "r")
- prices_for_window_index_array_train = h5f[
-@@ -268,6 +283,7 @@ def generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data):
- 
- class DataGenerator(tf.compat.v2.keras.utils.Sequence):
-     def __init__(self,
-+                 checkpoint_path,
-                  prices_for_window_index_array,
-                  input_features_normalized,
-                  y_data,
-@@ -277,6 +293,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
-                  to_fit,
-                  shuffle=True):
-         self.batch_size = batch_size
-+        self.checkpoint_path = checkpoint_path
-         self.prices_for_window_index_array = prices_for_window_index_array
-         self.input_features_normalized = input_features_normalized
-         self.labels = y_data
-@@ -323,6 +340,8 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
-     def on_epoch_end(self):
- 
-+        wandb.save(self.checkpoint_path)
-+
-         self.indexes = np.arange(len(self.prices_for_window_index_array))
- 
-         if self.shuffle:
-@@ -351,8 +370,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
- 
- n_classes = 3
--dim = (200, 40)
--batch_size = 64
-+dim = (window_length, num_features)
- to_fit = True
- 
- # for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,
-@@ -371,70 +389,6 @@ to_fit = True
- 
- ## PRODIGY AI HOCKUS POCKUS END
- 
--
--### Model Architecture
--def create_deeplob(T, NF, number_of_lstm):
--    input_lmd = Input(shape=(T, NF, 1))
--
--    # build the convolutional block
--    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    conv_first1 = Conv2D(32, (1, 10))(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    # build the inception module
--    convsecond_1 = Conv2D(64, (1, 1), padding="same")(conv_first1)
--    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
--    convsecond_1 = Conv2D(64, (3, 1), padding="same")(convsecond_1)
--    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
--
--    convsecond_2 = Conv2D(64, (1, 1), padding="same")(conv_first1)
--    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
--    convsecond_2 = Conv2D(64, (5, 1), padding="same")(convsecond_2)
--    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
--
--    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),
--                                padding="same")(conv_first1)
--    convsecond_3 = Conv2D(64, (1, 1), padding="same")(convsecond_3)
--    convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)
--
--    convsecond_output = concatenate([convsecond_1, convsecond_2, convsecond_3],
--                                    axis=3)
--
--    # use the MC dropout here
--    conv_reshape = Reshape(
--        (int(convsecond_output.shape[1]),
--         int(convsecond_output.shape[3])))(convsecond_output)
--
--    conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)
--
--    # build the output layer
--    out = Dense(3, activation="softmax")(conv_lstm)
--    model = Model(inputs=input_lmd, outputs=out)
--    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
--    model.compile(optimizer=adam,
--                  loss="categorical_crossentropy",
--                  metrics=["accuracy"])
--
--    return model
--
--
- checkpoint_path = path_adjust + "temp/cp.ckpt"
- # Create a callback that saves the model's weights
- cp_callback = ModelCheckpoint(filepath=checkpoint_path,
-@@ -446,9 +400,8 @@ if check_point_file.exists() and resuming == "resuming":
-     print("weights loaded")
-     model.load_weights(checkpoint_path)
- 
--batch_size = 64
--
--train_generator = DataGenerator(prices_for_window_index_array_train,
-+train_generator = DataGenerator(checkpoint_path,
-+                                prices_for_window_index_array_train,
-                                 input_features_normalized_train,
-                                 y_train,
-                                 batch_size,
-@@ -457,7 +410,8 @@ train_generator = DataGenerator(prices_for_window_index_array_train,
-                                 to_fit,
-                                 shuffle=True)
- 
--val_generator = DataGenerator(prices_for_window_index_array_val,
-+val_generator = DataGenerator(checkpoint_path,
-+                              prices_for_window_index_array_val,
-                               input_features_normalized_val,
-                               y_val,
-                               batch_size,
-@@ -469,14 +423,87 @@ val_generator = DataGenerator(prices_for_window_index_array_val,
- steps_per_epoch = len(train_generator)
- validation_steps = len(val_generator)
- 
--deeplob = create_deeplob(200, 40, 64)
--deeplob.fit(train_generator,
--            steps_per_epoch=steps_per_epoch,
--            validation_steps=validation_steps,
--            epochs=100,
--            verbose=2,
--            validation_data=val_generator,
--            callbacks=[cp_callback, WandbCallback()])
-+if wandb.run.resumed:
-+    wandb.restore("model-best.h5",run_path="garthtrickett/prodigyai/" + wandb.run.id)
-+    # restore the best model
-+    deeplob = tensorflow.keras.models.load_model(wandb.restore("model-best.h5").name)
-+    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
-+    deeplob.compile(optimizer=adam,
-+                    loss="categorical_crossentropy",
-+                    metrics=["accuracy"])
-+else:
-+    ### Model Architecture
-+    def create_deeplob(T, NF, number_of_lstm):
-+        input_lmd = Input(shape=(T, NF, 1))
-+
-+        # build the convolutional block
-+        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        conv_first1 = Conv2D(32, (1, 10))(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        # build the inception module
-+        convsecond_1 = Conv2D(64, (1, 1), padding="same")(conv_first1)
-+        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
-+        convsecond_1 = Conv2D(64, (3, 1), padding="same")(convsecond_1)
-+        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
-+
-+        convsecond_2 = Conv2D(64, (1, 1), padding="same")(conv_first1)
-+        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
-+        convsecond_2 = Conv2D(64, (5, 1), padding="same")(convsecond_2)
-+        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
-+
-+        convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),
-+                                    padding="same")(conv_first1)
-+        convsecond_3 = Conv2D(64, (1, 1), padding="same")(convsecond_3)
-+        convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)
-+
-+        convsecond_output = concatenate(
-+            [convsecond_1, convsecond_2, convsecond_3], axis=3)
-+
-+        # use the MC dropout here
-+        conv_reshape = Reshape(
-+            (int(convsecond_output.shape[1]),
-+             int(convsecond_output.shape[3])))(convsecond_output)
-+
-+        conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)
-+
-+        # build the output layer
-+        out = Dense(3, activation="softmax")(conv_lstm)
-+        model = Model(inputs=input_lmd, outputs=out)
-+        adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
-+        model.compile(optimizer=adam,
-+                      loss="categorical_crossentropy",
-+                      metrics=["accuracy"])
-+
-+        return model
-+
-+    deeplob = create_deeplob(window_length, num_features, number_of_lstm)
-+deeplob.fit(
-+    train_generator,
-+    steps_per_epoch=steps_per_epoch,
-+    validation_steps=validation_steps,
-+    epochs=100,
-+    verbose=2,
-+    validation_data=val_generator,
-+    callbacks=[cp_callback,
-+               WandbCallback(save_model=True, monitor="loss")])
- 
- finished_weights_path = path_adjust + "temp/cp_end.ckpt"
- deeplob.save_weights(finished_weights_path)
-diff --git a/pipeline/preprocessing.py b/pipeline/preprocessing.py
-index 932ae0b..714d325 100644
---- a/pipeline/preprocessing.py
-+++ b/pipeline/preprocessing.py
-@@ -136,54 +136,67 @@ if stage == 2:
-     side = X_for_all_labels["predicted_bins"]
-     # Could use the probabilities (instead of [1,0,0] use [0.2,0.55,0.25]
- 
-+import yaml
-+import wandb
-+
-+yaml_path = path_adjust + "yaml/preprocessing.yaml"
-+with open(yaml_path) as file:
-+    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
-+
-+config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
-+
-+wandb.init(
-+    dir="~/ProdigyAI/",
-+    project="prodigyai",
-+    config=config_dictionary,
-+)
-+
-+minimum_return = eval(wandb.config['params']['minimum_return']['value'])
-+vertical_barrier_seconds = eval(
-+    wandb.config['params']['vertical_barrier_seconds']['value'])
-+
- # Parameters
- parameters = dict()
--model_arch = "TABL"
--parameters["arch"] = model_arch
--parameters["name"] = model
--parameters["WL"] = 200  # WINDOW LONG
--parameters["pt"] = 1
--parameters["sl"] = 1
--parameters["min_ret"] = 0.001 * 1 / 23
--parameters["vbs"] = round(1 / 2,
--                          3)  # Increasing this decreases vertical touches
--parameters["head"] = 1000  # take only first x number of rows 0 means of
--parameters["skip"] = 0  # sample every n'th row if skip > 0
--# get even classes at fraction = 0.8 so the training set is balanced then set to 1
--# 3 million rows is about the limit of before it starts taking ages / memory maxing
--parameters["vol_max"] = (
--    parameters["min_ret"] + 0.00000002
-+wandb.config['params']['head'][
-+    'value'] = 1000  # take only first x number of rows 0 means of
-+volume_max = (
-+    minimum_return + wandb.config['params']['vol_max_modifier']['value']
- )  # The higher this is the more an increase in volatility requries an increase
- # in return to be considered buy/sell (Increasing this increases end barrier vertical touches)
--parameters["vol_min"] = parameters["min_ret"] + 0.00000001
-+volume_min = minimum_return + wandb.config['params']['vol_min_modifier'][
-+    'value']
- 
--parameters["filter"] = "none"
-+filter_type = wandb.config['params']['filter_type']['value']
- 
--if parameters["filter"] == "cm":
--    parameters["cm_vol_mod"] = 500
-+if filter_type == "cm":
-+    cusum_filter_vol_modifier = wandb.config['params'][
-+        'cusum_filter_volume_modifier']['value']
- else:
--    parameters["cm_vol_mod"] = 0
-+    cusum_filter_vol_modifier = 0
- 
--parameters["sw"] = "on"  # sample weights
--parameters["fd"] = "off"
-+use_sample_weights = wandb.config['params']['use_sample_weights']['value']
-+use_sample_weights = wandb.config['params']['use_fractional_differentiation'][
-+    'value']
- 
--parameters["input"] = "obook"
-+input_type = wandb.config['params']['input_type']['value']
- 
--parameters["ntb"] = True  # non time bars
-+# parameters["ntb"] = True  # non time bars
- 
--if parameters["ntb"] == True:
--    # Pick whether you want to add in the time since last bar input feature
--    # time since last bar column
--    parameters["tslbc"] = True  # time since last bar column
--else:
--    # Pick whether you want to add in the volume input feature
--    parameters["vbc"] = True  # volume bar column
-+# if parameters["ntb"] == True:
-+#     # Pick whether you want to add in the time since last bar input feature
-+#     # time since last bar column
-+#     parameters["tslbc"] = True  # time since last bar column
-+# else:
-+#     # Pick whether you want to add in the volume input feature
-+#     parameters["vbc"] = True  # volume bar column
- 
- # Create the txt file string
--parameter_string = "&".join("{}{}{}".format(key, "=", val)
--                            for key, val in parameters.items())
-+parameter_string = wandb.run.id
- 
--pt_sl = [parameters["pt"], parameters["sl"]]
-+pt_sl = [
-+    wandb.config['params']['profit_taking_multiplier']['value'],
-+    wandb.config['params']['stop_loss_multiplier']['value']
-+]
- cpus = cpu_count() - 1
- 
- regenerate_features_and_labels = True
-@@ -192,10 +205,10 @@ if regenerate_features_and_labels == True:
-     if stage == 1:
-         # Side
-         print("starting data load")
--        head = parameters["head"]
-+        head = wandb.config['params']['head']['value']
- 
-         # # read parquet file of dollar bars
--        if parameters["input"] == "bars":
-+        if input_type == "bars":
-             # Mlfinlab bars
-             data = pq.read_pandas(
-                 path_adjust + "data/bars/"
-@@ -216,7 +229,7 @@ if regenerate_features_and_labels == True:
-             data.index = pd.to_datetime(data.index, infer_datetime_format=True)
- 
-         # read parquet file of raw ticks
--        if parameters["input"] == "ticks":
-+        if input_type == "ticks":
-             data = pq.read_pandas(
-                 path_adjust + "data/bars/" +
-                 "btcusdt_agg_trades_raw_tick_data.parquet").to_pandas()
-@@ -238,11 +251,7 @@ if regenerate_features_and_labels == True:
-             # Should do something else than drop the duplicates (maybe trades doesnt have duplicate indexes rather than aggtrades)
-             data = data.loc[~data.index.duplicated(keep="first")]
- 
--            # skip most rows if this is > 1
--            if parameters["skip"] > 0:
--                data = data.iloc[::parameters["skip"], :]
--
--        if parameters["input"] == "obook":
-+        if input_type == "orderbook":
-             with open(path_adjust + "temp/orderbook_data_name.txt",
-                       "r") as text_file:
-                 orderbook_preprocessed_file_name = text_file.read()
-@@ -275,7 +284,7 @@ if regenerate_features_and_labels == True:
-         # duplicate_fast_search(data.index.duplicated())
- 
-         # Fractional differentiation
--        if parameters["fd"] == "on":
-+        if use_sample_weights == "on":
-             data_series = data["close"].to_frame()
-             # # generate 100 points
-             # nsample = 1000
-@@ -313,16 +322,16 @@ if regenerate_features_and_labels == True:
- 
-         start = time.time()
-         volatility_level_array = volatility_levels_numba(
--            np.ascontiguousarray(data.close.values), parameters["WL"])
-+            np.ascontiguousarray(data.close.values),
-+            wandb.config['params']['window_length']['value'])
-         data["window_volatility_level"] = volatility_level_array
- 
-         # Should adjust the max value
-         # To get more vertical touches we can
-         # either increase vol_max or
-         # decrease the window seconds
--        scaler = MinMaxScaler(
--            feature_range=(parameters["vol_min"],
--                           parameters["vol_max"]))  # normalization
-+        scaler = MinMaxScaler(feature_range=(volume_min,
-+                                             volume_max))  # normalization
- 
-         normed_window_volatility_level = scaler.fit_transform(
-             data[["window_volatility_level"]])
-@@ -338,7 +347,7 @@ if regenerate_features_and_labels == True:
-         close_np_array, close_index_np_array = pandas_series_to_numba_ready_np_arrays(
-             close_copy)
- 
--        volatility_threshold = volatility_threshold * parameters["cm_vol_mod"]
-+        volatility_threshold = volatility_threshold * cusum_filter_vol_modifier
-         print("data_len = " + str(len(data)))
-         start = time.time()
-         sampled_idx = filter_events(
-@@ -346,7 +355,7 @@ if regenerate_features_and_labels == True:
-             close_np_array,
-             close_index_np_array,
-             volatility_threshold,
--            parameters["filter"],
-+            filter_type,
-         )
-         print("sampled_idx_len = " + str(len(sampled_idx)))
-         end = time.time()
-@@ -356,7 +365,8 @@ if regenerate_features_and_labels == True:
-         # size
-         start = time.time()
-         volatility_level_array = volatility_levels_numba(
--            data.close.values, parameters["WL"])
-+            data.close.values,
-+            wandb.config['params']['window_length']['value'])
-         data["window_volatility_level"] = volatility_level_array
- 
-     # This code runs for both first and second stage preprocessing
-@@ -364,7 +374,7 @@ if regenerate_features_and_labels == True:
-     vertical_barrier_timestamps = ml.labeling.add_vertical_barrier(
-         t_events=sampled_idx,
-         close=data["close"],
--        num_seconds=parameters["vbs"])
-+        num_seconds=vertical_barrier_seconds)
-     end = time.time()
-     print("vertical barrier" + str(end - start))
- 
-@@ -376,7 +386,7 @@ if regenerate_features_and_labels == True:
-         t_events=sampled_idx,
-         pt_sl=pt_sl,
-         target=data["window_volatility_level"],
--        min_ret=parameters["min_ret"],
-+        min_ret=minimum_return,
-         num_threads=cpus * 2,
-         vertical_barrier_times=vertical_barrier_timestamps,
-         side_prediction=side,
-@@ -533,26 +543,27 @@ if stage == 1:
-     ### ORDERBOOK VOLUME DATA
-     volumes_for_all_labels = volumes.loc[data.close.index]
- 
--    ## TRADE DATA
--    input_features_trade = []
--    close_array = data.close.values
--    input_features_trade.append(close_array)
-+    # ## TRADE DATA
-+    # input_features_trade = []
-+    # close_array = data.close.values
-+    # input_features_trade.append(close_array)
- 
--    if parameters["ntb"] == False and parameters["vbc"] == True:
--        volume_array = data.volume.values
--        input_features_trade.append(volume_array)
--    if parameters["ntb"] == True and parameters["tslbc"] == True:
--        time_since_last_bar_array = data.time_since_last_bar.values
--        input_features_trade.append(time_since_last_bar_array)
-+    # if parameters["ntb"] == False and parameters["vbc"] == True:
-+    #     volume_array = data.volume.values
-+    #     input_features_trade.append(volume_array)
-+    # if parameters["ntb"] == True and parameters["tslbc"] == True:
-+    #     time_since_last_bar_array = data.time_since_last_bar.values
-+    #     input_features_trade.append(time_since_last_bar_array)
- 
-     end_time = time.time()
-     print(end_time - start_time)
- 
--    # min max limits
-+    # Type of scaling to apply
-+    scaling_type = wandb.config['params']['scaling_type']['value']
- 
--    minimum = -1
--    maximum = 1
--    scaling_type = "z_score"
-+    # min max limits
-+    minimum = wandb.config['params']['scaling_maximum']['value']
-+    maximum = wandb.config['params']['scaling_minimum']['value']
- 
-     ### Split intothe training/validation/test sets
-     print("splitting into train/va/test sets start")
-@@ -573,11 +584,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_train = volumes_for_all_labels.iloc[
-         train_close_array_integer_index[0] -
--        parameters["WL"]:train_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        train_close_array_integer_index[-1] + 2]
- 
-     close_index_array_train = close_index_array[
-         train_close_array_integer_index[0] -
--        parameters["WL"]:train_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        train_close_array_integer_index[-1] + 2]
- 
-     end_time = time.time()
- 
-@@ -623,7 +636,7 @@ if stage == 1:
-     # print("Make window started")
-     # start_time = time.time()
- 
--    # padding = parameters["WL"] * 2
-+    # padding = wandb.config['params']['window_length']['value'] * 2
- 
-     # split_by = 100000
- 
-@@ -652,7 +665,7 @@ if stage == 1:
-     #         len(prices_for_window_index_array_train[start_index:end_index]),
-     #         input_features_normalized_train[:, close_and_input_start_index:
-     #                                         close_and_input_end_index],
--    #         parameters["WL"],
-+    #         wandb.config['params']['window_length']['value'],
-     #         model_arch,
-     #     )
- 
-@@ -697,11 +710,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_val = volumes_for_all_labels.iloc[
-         val_close_array_integer_index[0] -
--        parameters["WL"]:val_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        val_close_array_integer_index[-1] + 2]
- 
-     close_index_array_val = close_index_array[
-         val_close_array_integer_index[0] -
--        parameters["WL"]:val_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        val_close_array_integer_index[-1] + 2]
- 
-     input_features_val = make_input_features_from_orderbook_data(
-         volumes_for_all_labels_val)
-@@ -721,7 +736,7 @@ if stage == 1:
-     #     #     prices_for_window_index_array_val,
-     #     #     close_index_array_val,
-     #     #     input_features_normalized_val,
--    #     #     parameters["WL"],
-+    #     #     wandb.config['params']['window_length']['value'],
-     #     #     model_arch,
-     #     # )
- 
-@@ -736,11 +751,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_test = volumes_for_all_labels.iloc[
-         test_close_array_integer_index[0] -
--        parameters["WL"]:test_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        test_close_array_integer_index[-1] + 2]
- 
-     close_index_array_test = close_index_array[
-         test_close_array_integer_index[0] -
--        parameters["WL"]:test_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        test_close_array_integer_index[-1] + 2]
- 
-     input_features_test = make_input_features_from_orderbook_data(
-         volumes_for_all_labels_test)
-@@ -761,7 +778,7 @@ if stage == 1:
- #     #     prices_for_window_index_array_test,
- #     #     close_index_array_test,
- #     #     input_features_normalized_test,
--#     #     parameters["WL"],
-+#     #     wandb.config['params']['window_length']['value'],
- #     #     model_arch,
- #     # )
- 
-@@ -782,8 +799,9 @@ ax = plt.gca()
- 
- data.iloc[window_index - 200:window_index + 200].plot(y="close",
-                                                       use_index=True)
--plot_window_and_touch_and_label(window_index, parameters["WL"], data,
--                                triple_barrier_events, labels)
-+plot_window_and_touch_and_label(
-+    window_index, wandb.config['params']['window_length']['value'], data,
-+    triple_barrier_events, labels)
- 
- data.iloc[window_index - 10:window_index + 30]
- 
-@@ -837,9 +855,9 @@ print("writing train/val/test to .h5 files finished taking " +
- #     h5f.create_dataset("P", data=P)
- # h5f.create_dataset("y", data=y)
- # h5f.create_dataset("sampled_idx_epoch", data=sampled_idx_epoch)
--# if parameters["sw"] == "on":
-+# if use_sample_weights == "on":
- #     h5f.create_dataset("sample_weights", data=sample_weights)
--# elif parameters["sw"] == "off":
-+# elif use_sample_weights == "off":
- #     h5f.create_dataset("sample_weights", data=np.zeros(1))
- # h5f.close()
- 
-diff --git a/pipeline/tabl.py b/pipeline/tabl.py
-index 87d51f8..f0244ec 100644
---- a/pipeline/tabl.py
-+++ b/pipeline/tabl.py
-@@ -6,7 +6,7 @@ import sys
- 
- import imblearn
- from imblearn.over_sampling import SMOTE
--
-+import tensorflow
- import matplotlib.pyplot as plt
- 
- sys.path.append("..")
-@@ -17,29 +17,26 @@ home = str(Path.home())
- sys.path.append(home + "/ProdigyAI")
- 
- from numba import njit, prange
--import tensorflow as tf
-+
- import math
- 
--import keras
- import numpy as np
- from third_party_libraries.TABL import Models
- from third_party_libraries.keras_lr_finder.lr_finder import LRFinder
- from third_party_libraries.CLR.clr_callbacks import CyclicLR
- 
--from tensorflow.keras.losses import categorical_crossentropy
--from tensorflow.keras.optimizers import SGD, Adam
-+import keras
-+from keras.losses import categorical_crossentropy
-+from keras.optimizers import SGD, Adam
- from keras.callbacks import ModelCheckpoint
- 
-+import tensorflow as tf
- # Init wandb
- import wandb
- from wandb.keras import WandbCallback
- 
- import yaml
- 
--# import tensorflow.keras as keras
--# import tensorflow.keras.layers as layers
--# import tensorflow.keras.backend as K
--
- # Sorting out whether we are using the ipython kernel or not
- try:
-     get_ipython()
-@@ -95,25 +92,24 @@ with open(yaml_path) as file:
-     yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
- 
- config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
--wandb.init(
--    dir="~/ProdigyAI/",
--    project="prodigyai",
--    config=config_dictionary,
--)
--
--# try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes
--
--# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1
--template = [[40, 200], [60, 10], [120, 5], [3, 1]]
--
--# example data
--example_x = np.random.rand(1000, 40, 10)
--np.min(example_x)
--np.max(example_x)
--np.mean(example_x)
--example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)
--template = [[40, 200], [60, 10], [120, 5], [3, 1]]
--# 200(WINDOW LENGTH)
-+wandb.init(dir="~/ProdigyAI/",
-+           project="prodigyai",
-+           config=config_dictionary,
-+           resume=True)
-+
-+window_length = wandb.config['params']['window_length']['value']
-+num_features = wandb.config['params']['num_features']['value']
-+epochs = wandb.config['params']['epochs']['value']
-+batch_size = wandb.config['params']['batch_size']['value']
-+dropout = wandb.config['params']['dropout']['value']
-+
-+# # try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes
-+# # example data
-+# example_x = np.random.rand(1000, 40, 10)
-+# np.min(example_x)
-+# np.max(example_x)
-+# np.mean(example_x)
-+# example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)
- 
- ## PRODIGY AI HOCKUS POCKUS START
- from pathlib import Path
-@@ -121,7 +117,7 @@ import h5py
- 
- home = str(Path.home())
- 
--file_name = "arch=TABL&name=two_model&WL=200&pt=1&sl=1&min_ret=4.347826086956522e-05&vbs=0.5&head=1000&skip=0&vol_max=4.3498260869565224e-05&vol_min=4.348826086956522e-05&filter=none&cm_vol_mod=0&sw=on&fd=off&input=obook&ntb=True&tslbc=True.h5"
-+file_name = "esugj36b.h5"
- wandb.config.update({'dataset': file_name})
- path = home + "/ProdigyAI/data/preprocessed/" + file_name
- h5f = h5py.File(path, "r")
-@@ -265,8 +261,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
- 
- n_classes = 3
--dim = (200, 40)
--batch_size = 64
-+dim = (window_length, num_features)
- to_fit = True
- 
- # for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,
-@@ -280,22 +275,15 @@ to_fit = True
- #     import pdb
- #     pdb.set_trace()
- 
-+# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1
-+
-+template = [[num_features, window_length], [60, 10], [120, 5], [3, 1]]
-+
- # get Bilinear model
- projection_regularizer = None
- projection_constraint = keras.constraints.max_norm(3.0, axis=0)
- attention_regularizer = None
- attention_constraint = keras.constraints.max_norm(5.0, axis=1)
--dropout = 0.1
--
--model = Models.TABL(
--    template,
--    dropout,
--    projection_regularizer,
--    projection_constraint,
--    attention_regularizer,
--    attention_constraint,
--)
--model.summary()
- 
- # path = path_adjust + "data/lob_2010/train_and_test_tabl.h5"
- # h5f = h5py.File(path, "r")
-@@ -442,6 +430,18 @@ clr = CyclicLR(base_lr=base_lr,
- 
- optimizer = keras.optimizers.Adam()
- 
-+model = Models.TABL(
-+    template,
-+    dropout,
-+    projection_regularizer,
-+    projection_constraint,
-+    attention_regularizer,
-+    attention_constraint,
-+)
-+model.summary()
-+
-+
-+
- # Compile the model
- model.compile(loss=loss_function, optimizer=optimizer, metrics=["accuracy"])
- # Create a callback that saves the model's weights
-@@ -449,10 +449,15 @@ cp_callback = ModelCheckpoint(filepath=checkpoint_path,
-                               save_weights_only=True,
-                               verbose=1)
- 
--check_point_file = Path(checkpoint_path)
--if check_point_file.exists() and resuming == "resuming":
--    print("weights loaded")
--    model.load_weights(checkpoint_path)
-+if wandb.run.resumed:
-+    weights = wandb.restore("cp.ckpt",
-+                            run_path="garthtrickett/prodigyai/" + wandb.run.id)
-+    model.load_weights(weights.name)
-+
-+# check_point_file = Path(checkpoint_path)
-+# if check_point_file.exists() and resuming == "resuming":
-+#     print("weights loaded")
-+#     model.load_weights(checkpoint_path)
- # Fit data to model
- # history = model.fit(trainX_CNN,
- #                     trainY_CNN,
-@@ -460,9 +465,6 @@ if check_point_file.exists() and resuming == "resuming":
- #                     epochs=20,
- #                     callbacks=[clr, cp_callback])
- 
--finished_weights_path = path_adjust + "temp/cp_end.ckpt"
--model.save_weights(finished_weights_path)
--
- # create class weight
- # class_weight = {0: 1e6 / 300.0, 1: 1e6 / 400.0, 2: 1e6 / 300.0}
- train_generator = DataGenerator(checkpoint_path,
-@@ -489,12 +491,14 @@ steps_per_epoch = len(train_generator)
- validation_steps = len(val_generator)
- 
- # example sata training
--model.fit(train_generator,
--          steps_per_epoch=steps_per_epoch,
--          validation_steps=validation_steps,
--          epochs=1000,
--          validation_data=val_generator,
--          callbacks=[cp_callback, WandbCallback()])
-+model.fit(
-+    train_generator,
-+    steps_per_epoch=steps_per_epoch,
-+    validation_steps=validation_steps,
-+    epochs=epochs,
-+    validation_data=val_generator,
-+    callbacks=[cp_callback,
-+               WandbCallback(save_model=True, monitor="loss")])
- 
- model.save(os.path.join(wandb.run.dir, "model.h5"))
- # no class weight
-@@ -603,16 +607,16 @@ model.save(os.path.join(wandb.run.dir, "model.h5"))
- # X_val = X_val.reshape((X_val.shape[0], 40, 10))
- 
- # train one epoch to find the learning rate
--model.fit(
--    X_train,
--    y_train,
--    validation_data=(X_val, y_val),
--    batch_size=256,
--    epochs=1,
--    shuffle=False,
--)  # no class weight
--
--score = model.evaluate(x=X_test, y=y_test, batch_size=256)
-+# model.fit(
-+#     X_train,
-+#     y_train,
-+#     validation_data=(X_val, y_val),
-+#     batch_size=256,
-+#     epochs=1,
-+#     shuffle=False,
-+# )  # no class weight
-+
-+# score = model.evaluate(x=X_test, y=y_test, batch_size=256)
- 
- # Save model to wandb
- 
-diff --git a/temp/cp.ckpt.data-00000-of-00001 b/temp/cp.ckpt.data-00000-of-00001
-index 56c00e4..3d9600c 100644
-Binary files a/temp/cp.ckpt.data-00000-of-00001 and b/temp/cp.ckpt.data-00000-of-00001 differ
-diff --git a/temp/cp.ckpt.index b/temp/cp.ckpt.index
-index 3770713..d6d3b4b 100644
-Binary files a/temp/cp.ckpt.index and b/temp/cp.ckpt.index differ
-diff --git a/temp/cp_end.ckpt b/temp/cp_end.ckpt
-index c1c1d9e..62e0a67 100644
-Binary files a/temp/cp_end.ckpt and b/temp/cp_end.ckpt differ
-diff --git a/temp/cp_end.ckpt.data-00000-of-00001 b/temp/cp_end.ckpt.data-00000-of-00001
-index 1d572d0..12d3157 100644
-Binary files a/temp/cp_end.ckpt.data-00000-of-00001 and b/temp/cp_end.ckpt.data-00000-of-00001 differ
-diff --git a/temp/cp_end.ckpt.index b/temp/cp_end.ckpt.index
-index 15de09a..0ab67e1 100644
-Binary files a/temp/cp_end.ckpt.index and b/temp/cp_end.ckpt.index differ
-diff --git a/temp/is_script_finished.txt b/temp/is_script_finished.txt
-index 40ca0d3..bec7a08 100644
---- a/temp/is_script_finished.txt
-+++ b/temp/is_script_finished.txt
-@@ -1 +1 @@
--full_script_time64.39192986488342
-\ No newline at end of file
-+full_script_time25.707124948501587
-\ No newline at end of file
-Submodule third_party_libraries/TABL contains modified content
-Submodule third_party_libraries/TABL 7f25314..df3e92d:
-diff --git a/third_party_libraries/TABL/Layers.py b/third_party_libraries/TABL/Layers.py
-index 9020002..b299e57 100644
---- a/third_party_libraries/TABL/Layers.py
-+++ b/third_party_libraries/TABL/Layers.py
-@@ -3,8 +3,9 @@
- """
- @author: Dat Tran (dat.tranthanh@tut.fi)
- """
-+import keras
- from keras import backend as K
--from keras.engine.topology import Layer
-+from keras.layers import Layer
- from keras import activations as Activations
- from keras import initializers as Initializers
- 
-@@ -18,25 +19,25 @@ class Constraint(object):
- 
-     def get_config(self):
-         return {}
--    
-+
-+
- class MinMax(Constraint):
-     """
-     Customized min-max constraint for scalar
-     """
--
-     def __init__(self, min_value=0.0, max_value=10.0):
-         self.min_value = min_value
-         self.max_value = max_value
-+
-     def __call__(self, w):
--        
--        return K.clip(w,self.min_value,self.max_value)
-+
-+        return K.clip(w, self.min_value, self.max_value)
- 
-     def get_config(self):
--        return {'min_value': self.min_value,
--                'max_value': self.max_value}
--        
-+        return {'min_value': self.min_value, 'max_value': self.max_value}
- 
--def nmodeproduct(x,w,mode):
-+
-+def nmodeproduct(x, w, mode):
-     """
-     n-mode product for 2D matrices
-     x: NxHxW
-@@ -45,74 +46,83 @@ def nmodeproduct(x,w,mode):
-     
-     output: NxhxW (mode1) or NxHxw (mode2)
-     """
--    if mode==2:
--        x=K.dot(x,w)
-+    if mode == 2:
-+        x = K.dot(x, w)
-     else:
--        x=K.permute_dimensions(x,(0,2,1))
--        x=K.dot(x,w)
--        x=K.permute_dimensions(x,(0,2,1))
-+        x = K.permute_dimensions(x, (0, 2, 1))
-+        x = K.dot(x, w)
-+        x = K.permute_dimensions(x, (0, 2, 1))
-     return x
- 
-+
- class BL(Layer):
-     """
-     Bilinear Layer
-     """
--    def __init__(self, output_dim,
-+    def __init__(self,
-+                 output_dim,
-                  kernel_regularizer=None,
--                 kernel_constraint=None,**kwargs):
-+                 kernel_constraint=None,
-+                 **kwargs):
-         """
-         output_dim : output dimensions of 2D tensor, should be a list of len 2, e.g. [30,20]
-         kernel_regularizer : keras regularizer object
-         kernel_constraint: keras constraint object
-         """
--        
-+
-         self.output_dim = output_dim
--        self.kernel_regularizer=kernel_regularizer
--        self.kernel_constraint=kernel_constraint
--        
-+        self.kernel_regularizer = kernel_regularizer
-+        self.kernel_constraint = kernel_constraint
-+
-         super(BL, self).__init__(**kwargs)
- 
-     def build(self, input_shape):
--        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
--                                      initializer='he_uniform',
--                                      regularizer=self.kernel_regularizer,
--                                      constraint=self.kernel_constraint,
--                                      trainable=True)
--        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
--                                      initializer='he_uniform',
--                                      regularizer=self.kernel_regularizer,
--                                      constraint=self.kernel_constraint,
--                                      trainable=True)
--
--        self.bias=self.add_weight(name='bias',shape=(self.output_dim[0],self.output_dim[1]),
--                              initializer='zeros',trainable=True)
-+        self.W1 = self.add_weight(name='W1',
-+                                  shape=(input_shape[1], self.output_dim[0]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.kernel_regularizer,
-+                                  constraint=self.kernel_constraint,
-+                                  trainable=True)
-+        self.W2 = self.add_weight(name='W2',
-+                                  shape=(input_shape[2], self.output_dim[1]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.kernel_regularizer,
-+                                  constraint=self.kernel_constraint,
-+                                  trainable=True)
-+
-+        self.bias = self.add_weight(name='bias',
-+                                    shape=(self.output_dim[0],
-+                                           self.output_dim[1]),
-+                                    initializer='zeros',
-+                                    trainable=True)
-         super(BL, self).build(input_shape)
- 
-     def call(self, x):
-         print(K.int_shape(x))
--        x = nmodeproduct(x,self.W1,1)
--        x = nmodeproduct(x,self.W2,2)
--        x = K.bias_add(x,self.bias)
--        
--        if self.output_dim[1]==1:
--            x = K.squeeze(x,axis=-1)
-+        x = nmodeproduct(x, self.W1, 1)
-+        x = nmodeproduct(x, self.W2, 2)
-+        x = K.bias_add(x, self.bias)
-+
-+        if self.output_dim[1] == 1:
-+            x = K.squeeze(x, axis=-1)
-         print(K.int_shape(x))
-         return x
- 
-     def compute_output_shape(self, input_shape):
--        if self.output_dim[1]==1:
-+        if self.output_dim[1] == 1:
-             return (input_shape[0], self.output_dim[0])
-         else:
-             return (input_shape[0], self.output_dim[0], self.output_dim[1])
--        
--        
-+
-+
- class TABL(Layer):
-     """
-     Temporal Attention augmented Bilinear Layer
-     https://arxiv.org/abs/1712.00975
-     
-     """
--    def __init__(self, output_dim,
-+    def __init__(self,
-+                 output_dim,
-                  projection_regularizer=None,
-                  projection_constraint=None,
-                  attention_regularizer=None,
-@@ -125,45 +135,52 @@ class TABL(Layer):
-         attention_regularizer: keras regularizer object for attention matrix
-         attention_constraint: keras constraint object for attention matrix
-         """
--        
-+
-         self.output_dim = output_dim
-         self.projection_regularizer = projection_regularizer
-         self.projection_constraint = projection_constraint
-         self.attention_regularizer = attention_regularizer
-         self.attention_constraint = attention_constraint
--        
-+
-         super(TABL, self).__init__(**kwargs)
- 
-     def build(self, input_shape):
--        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
--                                      initializer='he_uniform',
--                                      regularizer=self.projection_regularizer,
--                                      constraint=self.projection_constraint,
--                                      trainable=True)
--        
--        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
--                                      initializer='he_uniform',
--                                      regularizer=self.projection_regularizer,
--                                      constraint=self.projection_constraint,
--                                      trainable=True)
--        
--        self.W = self.add_weight(name='W',shape=(input_shape[2], input_shape[2]),
--                                      initializer=Initializers.Constant(1.0/input_shape[2]),
--                                      regularizer=self.attention_regularizer,
--                                      constraint=self.attention_constraint,
--                                      trainable=True)
--        
--        self.alpha = self.add_weight(name='alpha',shape=(1,),
--                                      initializer=Initializers.Constant(0.5),
--                                      constraint=MinMax(),
--                                      trainable=True)
--
--
--        self.bias=self.add_weight(name='bias',shape=(1, self.output_dim[0],self.output_dim[1]),
--                              initializer='zeros',trainable=True)
--        
-+        self.W1 = self.add_weight(name='W1',
-+                                  shape=(input_shape[1], self.output_dim[0]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.projection_regularizer,
-+                                  constraint=self.projection_constraint,
-+                                  trainable=True)
-+
-+        self.W2 = self.add_weight(name='W2',
-+                                  shape=(input_shape[2], self.output_dim[1]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.projection_regularizer,
-+                                  constraint=self.projection_constraint,
-+                                  trainable=True)
-+
-+        self.W = self.add_weight(name='W',
-+                                 shape=(input_shape[2], input_shape[2]),
-+                                 initializer=Initializers.Constant(
-+                                     1.0 / input_shape[2]),
-+                                 regularizer=self.attention_regularizer,
-+                                 constraint=self.attention_constraint,
-+                                 trainable=True)
-+
-+        self.alpha = self.add_weight(name='alpha',
-+                                     shape=(1, ),
-+                                     initializer=Initializers.Constant(0.5),
-+                                     constraint=MinMax(),
-+                                     trainable=True)
-+
-+        self.bias = self.add_weight(name='bias',
-+                                    shape=(1, self.output_dim[0],
-+                                           self.output_dim[1]),
-+                                    initializer='zeros',
-+                                    trainable=True)
-+
-         self.in_shape = input_shape
--        
-+
-         super(TABL, self).build(input_shape)
- 
-     def call(self, x):
-@@ -174,27 +191,26 @@ class TABL(Layer):
-         W: D2 x D2
-         """
-         # first mode projection
--        x = nmodeproduct(x,self.W1,1) # N x d1 x D2
--        # enforcing constant (1) on the diagonal 
--        W = self.W-self.W*K.eye(self.in_shape[2],dtype='float32')+K.eye(self.in_shape[2],dtype='float32')/self.in_shape[2]
--        # calculate attention 
--        attention = Activations.softmax(nmodeproduct(x,W,2),axis=-1) # N x d1 x D2
-+        x = nmodeproduct(x, self.W1, 1)  # N x d1 x D2
-+        # enforcing constant (1) on the diagonal
-+        W = self.W - self.W * K.eye(self.in_shape[2], dtype='float32') + K.eye(
-+            self.in_shape[2], dtype='float32') / self.in_shape[2]
-+        # calculate attention
-+        attention = Activations.softmax(nmodeproduct(x, W, 2),
-+                                        axis=-1)  # N x d1 x D2
-         # apply attention
--        x = self.alpha*x + (1.0 - self.alpha)*x*attention
-+        x = self.alpha * x + (1.0 - self.alpha) * x * attention
-         # second mode projection
--        x = nmodeproduct(x,self.W2,2)
-+        x = nmodeproduct(x, self.W2, 2)
-         # bias add
-         x = x + self.bias
--        
--        if self.output_dim[1]==1:
--            x = K.squeeze(x,axis=-1)
-+
-+        if self.output_dim[1] == 1:
-+            x = K.squeeze(x, axis=-1)
-         return x
- 
-     def compute_output_shape(self, input_shape):
--        if self.output_dim[1]==1:
-+        if self.output_dim[1] == 1:
-             return (input_shape[0], self.output_dim[0])
-         else:
-             return (input_shape[0], self.output_dim[0], self.output_dim[1])
--
--
--
-diff --git a/third_party_libraries/TABL/Models.py b/third_party_libraries/TABL/Models.py
-index e2bf62c..d5c4c45 100644
---- a/third_party_libraries/TABL/Models.py
-+++ b/third_party_libraries/TABL/Models.py
-@@ -4,7 +4,6 @@
- @author: Dat Tran (dat.tranthanh@tut.fi)
- """
- 
--
- from third_party_libraries.TABL import Layers
- import keras
- 
-@@ -74,7 +73,8 @@ def TABL(
- 
-     x = inputs
-     for k in range(1, len(template) - 1):
--        x = Layers.BL(template[k], projection_regularizer, projection_constraint)(x)
-+        x = Layers.BL(template[k], projection_regularizer,
-+                      projection_constraint)(x)
-         x = keras.layers.Activation("relu")(x)
-         x = keras.layers.Dropout(dropout)(x)
- 
-@@ -94,4 +94,3 @@ def TABL(
-     model.compile(optimizer, "categorical_crossentropy", ["acc"])
- 
-     return model
--
-diff --git a/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc
-new file mode 100644
-index 0000000..ff0a753
-Binary files /dev/null and b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc differ
-diff --git a/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc
-new file mode 100644
-index 0000000..a57f1ba
-Binary files /dev/null and b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc differ
-diff --git a/yaml/tabl.yaml b/yaml/tabl.yaml
-index 2cdc7b2..456e287 100644
---- a/yaml/tabl.yaml
-+++ b/yaml/tabl.yaml
-@@ -4,4 +4,14 @@ epochs:
-   value: 100
- batch_size:
-   desc: Size of each mini-batch
--  value: 32
-+  value: 256
-+window_length:
-+  desc: null
-+  value: 200
-+num_features:
-+  desc: null
-+  value: 40
-+dropout:
-+  desc: null
-+  value: 0.1
-+
diff --git a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/media/graph/graph_summary_6f818560.graph.json b/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/media/graph/graph_summary_6f818560.graph.json
deleted file mode 100644
index fcd5b7c..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/media/graph/graph_summary_6f818560.graph.json
+++ /dev/null
@@ -1 +0,0 @@
-{"format": "keras", "nodes": [{"name": "input_1", "id": "input_1", "class_name": "InputLayer", "output_shape": [null, 40, 200], "num_parameters": 0}, {"name": "bl_1", "id": "bl_1", "class_name": "BL", "output_shape": [null, 60, 10], "num_parameters": 5000}, {"name": "activation_1", "id": "activation_1", "class_name": "Activation", "output_shape": [null, 60, 10], "num_parameters": 0}, {"name": "dropout_1", "id": "dropout_1", "class_name": "Dropout", "output_shape": [null, 60, 10], "num_parameters": 0}, {"name": "bl_2", "id": "bl_2", "class_name": "BL", "output_shape": [null, 120, 5], "num_parameters": 7850}, {"name": "activation_2", "id": "activation_2", "class_name": "Activation", "output_shape": [null, 120, 5], "num_parameters": 0}, {"name": "dropout_2", "id": "dropout_2", "class_name": "Dropout", "output_shape": [null, 120, 5], "num_parameters": 0}, {"name": "tabl_1", "id": "tabl_1", "class_name": "TABL", "output_shape": [null, 3], "num_parameters": 394}, {"name": "activation_3", "id": "activation_3", "class_name": "Activation", "output_shape": [null, 3], "num_parameters": 0}], "edges": [["input_1", "bl_1"], ["bl_1", "activation_1"], ["activation_1", "dropout_1"], ["dropout_1", "bl_2"], ["bl_2", "activation_2"], ["activation_2", "dropout_2"], ["dropout_2", "tabl_1"], ["tabl_1", "activation_3"]]}
\ No newline at end of file
diff --git a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/requirements.txt b/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/requirements.txt
deleted file mode 100644
index c4244a7..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/requirements.txt
+++ /dev/null
@@ -1,339 +0,0 @@
-absl-py==0.9.0
-alabaster==0.7.12
-anaconda-client==1.7.2
-anaconda-project==0.8.3
-appdirs==1.4.3
-argparse==1.4.0
-asn1crypto==1.3.0
-astor==0.8.1
-astroid==2.3.3
-astropy==4.0
-asyncore-wsgi==0.0.9
-atomicwrites==1.3.0
-attrs==19.3.0
-autobahn==20.2.1
-autoflake==1.3.1
-automat==0.8.0
-babel==2.8.0
-backcall==0.1.0
-backports.os==0.1.1
-backports.shutil-get-terminal-size==1.0.0
-beautifulsoup4==4.8.2
-bitarray==1.2.1
-bkcharts==0.2
-black==19.10b0
-bleach==3.1.0
-blis==0.4.1
-bokeh==1.4.0
-boto3==1.12.37
-boto==2.49.0
-botocore==1.15.37
-bottle==0.12.18
-bottleneck==1.3.1
-cachetools==4.0.0
-catalogue==1.0.0
-cchardet==2.1.6
-certifi==2019.11.28
-cffi==1.13.2
-chardet==3.0.4
-click==7.0
-cloudpickle==1.2.2
-clyent==1.2.2
-colorama==0.4.3
-configparser==5.0.0
-constantly==15.1.0
-contextlib2==0.6.0.post1
-cryptography==2.8
-cvxpy==1.0.31
-cycler==0.10.0
-cymem==2.0.3
-cython==0.29.14
-cytoolz==0.10.1
-dask==2.10.1
-dateparser==0.7.2
-decorator==4.4.1
-defusedxml==0.6.0
-dill==0.3.1.1
-distributed==2.10.0
-docker-pycreds==0.4.0
-docutils==0.15.2
-easyprocess==0.2.10
-ecos==2.0.7.post1
-entrypoints==0.3
-et-xmlfile==1.0.1
-fastai2==0.0.17
-fastcache==1.1.0
-fastcore==0.1.17
-fastprogress==0.2.2
-filelock==3.0.12
-flake8==3.7.9
-flask==1.1.1
-fsspec==0.6.2
-future==0.18.2
-gast==0.2.2
-gevent==1.4.0
-gitdb==4.0.5
-gitpython==3.1.2
-glob2==0.7
-gmpy2==2.0.8
-google-api-core==1.16.0
-google-api-python-client==1.8.0
-google-auth-httplib2==0.0.3
-google-auth-oauthlib==0.4.1
-google-auth==1.13.1
-google-cloud-profiler==1.0.9
-google-pasta==0.1.8
-googleapis-common-protos==1.51.0
-gql==0.2.0
-graphql-core==1.1
-greenlet==0.4.15
-grpcio==1.27.1
-h5py==2.10.0
-hanging-threads==2.0.5
-heapdict==1.0.1
-html5lib==1.0.1
-httplib2==0.17.2
-hyperlink==19.0.0
-hypothesis==5.4.1
-idna==2.8
-imageio==2.6.1
-imagesize==1.2.0
-imbalanced-learn==0.6.1
-imblearn==0.0
-importlib-metadata==1.5.0
-incremental==17.5.0
-inflect==4.1.0
-ipykernel==5.1.4
-ipython-genutils==0.2.0
-ipython==7.12.0
-ipywidgets==7.5.1
-isort==4.3.21
-itsdangerous==1.1.0
-jaraco.itertools==5.0.0
-jdcal==1.4.1
-jedi==0.16.0
-jeepney==0.4.2
-jieba==0.42.1
-jinja2==2.11.1
-jmespath==0.9.5
-joblib==0.14.1
-json5==0.9.0
-jsonschema==3.2.0
-jupyter-client==5.3.4
-jupyter-console==6.1.0
-jupyter-core==4.6.1
-jupyter==1.0.0
-jupyterlab-server==1.0.6
-jupyterlab==1.2.6
-keras-applications==1.0.8
-keras-bert==0.81.0
-keras-embed-sim==0.7.0
-keras-layer-normalization==0.14.0
-keras-multi-head==0.22.0
-keras-pos-embd==0.11.0
-keras-position-wise-feed-forward==0.6.0
-keras-preprocessing==1.1.0
-keras-self-attention==0.41.0
-keras-transformer==0.32.0
-keras==2.3.1
-keyring==21.1.0
-kiwisolver==1.1.0
-ktrain==0.12.3
-langdetect==1.0.8
-lazy-object-proxy==1.4.3
-libarchive-c==2.8
-lief==0.9.0
-llvmlite==0.31.0
-locket==0.2.0
-lxml==4.5.0
-markdown==3.2
-markupsafe==1.1.1
-matplotlib==3.1.3
-mccabe==0.6.1
-mistune==0.8.4
-mkl-fft==1.0.15
-mkl-random==1.1.0
-mkl-service==2.3.0
-mock==3.0.5
-more-itertools==8.2.0
-mpmath==1.1.0
-msgpack==1.0.0
-multipledispatch==0.6.0
-multiprocess==0.70.9
-murmurhash==1.0.2
-nbconvert==5.6.1
-nbformat==5.0.4
-networkx==2.4
-nltk==3.4.5
-nose==1.3.7
-notebook==6.0.3
-numba==0.48.0
-numexpr==2.7.1
-numpy==1.18.1
-numpydoc==0.9.2
-nvidia-ml-py3==7.352.0
-oauthlib==3.1.0
-olefile==0.46
-opencv-python==4.2.0.34
-openpyxl==3.0.3
-opt-einsum==3.1.0
-osqp==0.6.1
-packaging==20.1
-pandas==1.0.3
-pandocfilters==1.4.2
-parse==1.6.6
-parso==0.6.0
-partd==1.1.0
-path==13.1.0
-pathlib2==2.3.5
-pathspec==0.7.0
-pathtools==0.1.2
-patsy==0.5.1
-pep8==1.7.1
-pexpect==4.8.0
-pickleshare==0.7.5
-pillow==7.0.0
-pip==20.0.2
-pkginfo==1.5.0.1
-plac==1.1.3
-pluggy==0.13.1
-ply==3.11
-preshed==3.0.2
-prometheus-client==0.7.1
-promise==2.3
-prompt-toolkit==3.0.3
-protobuf==3.11.3
-psutil==5.6.7
-psycopg2-binary==2.8.4
-ptyprocess==0.6.0
-py==1.8.1
-pyarrow==0.16.0
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycodestyle==2.5.0
-pycosat==0.6.3
-pycparser==2.19
-pycrypto==2.6.1
-pycurl==7.43.0.5
-pydbus==0.6.0
-pyflakes==2.1.1
-pygments==2.5.2
-pyhamcrest==2.0.0
-pylint==2.4.4
-pyodbc==4.0.28
-pyopenssl==19.1.0
-pyparsing==2.4.6
-pyrsistent==0.15.7
-pysocks==1.7.1
-pytest-arraydiff==0.3
-pytest-astropy-header==0.1.2
-pytest-astropy==0.7.0
-pytest-doctestplus==0.5.0
-pytest-openfiles==0.4.0
-pytest-remotedata==0.3.2
-pytest==5.3.5
-python-dateutil==2.8.1
-pyts==0.10.0
-pytz==2019.3
-pyunpack==0.1.2
-pywavelets==1.1.1
-pyyaml==5.3
-pyzmq==18.1.1
-qtawesome==0.6.1
-qtconsole==4.6.0
-qtpy==1.9.0
-regex==2020.1.8
-requests-oauthlib==1.3.0
-requests==2.22.0
-rope==0.16.0
-rsa==4.0
-ruamel-yaml==0.15.87
-s3transfer==0.3.3
-sacremoses==0.0.38
-scikit-image==0.16.2
-scikit-learn==0.22.1
-scipy==1.4.1
-scoop==0.7.1.1
-scs==2.1.2
-seaborn==0.10.0
-secretstorage==3.1.2
-send2trash==1.5.0
-sentencepiece==0.1.85
-sentry-sdk==0.14.3
-seqeval==0.0.12
-service-identity==18.1.0
-setuptools==45.1.0.post20200127
-shortuuid==1.0.1
-simplegeneric==0.8.1
-singledispatch==3.4.0.3
-six==1.14.0
-smmap==3.0.4
-snowballstemmer==2.0.0
-sortedcollections==1.1.2
-sortedcontainers==2.1.0
-soupsieve==1.9.5
-spacy==2.2.3
-sphinx==2.3.1
-sphinxcontrib-applehelp==1.0.1
-sphinxcontrib-devhelp==1.0.1
-sphinxcontrib-htmlhelp==1.0.2
-sphinxcontrib-jsmath==1.0.1
-sphinxcontrib-qthelp==1.0.2
-sphinxcontrib-serializinghtml==1.1.3
-sphinxcontrib-websupport==1.1.2
-spyder-kernels==0.5.2
-spyder==3.3.6
-sqlalchemy==1.3.13
-srsly==1.0.1
-statsmodels==0.11.0
-subprocess32==3.5.4
-sympy==1.5.1
-syntok==1.2.2
-tables==3.6.1
-tblib==1.6.0
-tensorboard==2.1.1
-tensorflow-datasets==2.1.0
-tensorflow-estimator==2.1.0
-tensorflow-metadata==0.21.1
-tensorflow==2.1.0
-termcolor==1.1.0
-terminado==0.8.3
-testpath==0.4.4
-thinc==7.3.1
-tokenizers==0.5.2
-toml==0.10.0
-toolz==0.10.0
-torch==1.4.0
-torchvision==0.5.0
-tornado==6.0.3
-tqdm==4.42.0
-traitlets==4.3.3
-transformers==2.8.0
-twisted==19.10.0
-txaio==20.1.1
-typed-ast==1.4.1
-tzlocal==2.0.0
-unicodecsv==0.14.1
-uritemplate==3.0.1
-urllib3==1.25.8
-wandb==0.8.35
-wasabi==0.6.0
-watchdog==0.10.2
-wcwidth==0.1.8
-web-pdb==1.5.1
-webencodings==0.5.1
-websocket-client==0.57.0
-weresync==1.1.5
-werkzeug==0.16.1
-wheel==0.34.2
-widgetsnbextension==3.5.1
-wrapt==1.11.2
-wurlitzer==2.0.0
-xlrd==1.2.0
-xlsxwriter==1.2.7
-xlwt==1.3.0
-yapf==0.29.0
-yapsy==1.11.223
-zict==1.0.0
-zipp==2.1.0
-zope.interface==4.7.1
\ No newline at end of file
diff --git a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/wandb-events.jsonl b/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/wandb-events.jsonl
deleted file mode 100644
index 3841fe3..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/wandb-events.jsonl
+++ /dev/null
@@ -1 +0,0 @@
-{"system.cpu": 24.56, "system.memory": 11.94, "system.disk": 23.5, "system.proc.memory.availableMB": 34462.5, "system.proc.memory.rssMB": 673.84, "system.proc.memory.percent": 1.72, "system.proc.cpu.threads": 30.6, "system.network.sent": 91674, "system.network.recv": 207675, "_wandb": true, "_timestamp": 1589165841, "_runtime": 6}
diff --git a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/wandb-history.jsonl b/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/wandb-history.jsonl
deleted file mode 100644
index 476b647..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/wandb-history.jsonl
+++ /dev/null
@@ -1,30 +0,0 @@
-{"_step":22,"loss":"NaN","_runtime":7.643781423568726,"accuracy":0.37109375,"epoch":22,"val_accuracy":0.8984375,"_timestamp":1.5891658247377608e+09,"val_loss":"NaN"}
-{"_step":23,"loss":"NaN","_runtime":7.76062798500061,"accuracy":0.3671875,"epoch":23,"val_accuracy":0.8984375,"_timestamp":1.5891658248546073e+09,"val_loss":"NaN"}
-{"_step":24,"loss":"NaN","_runtime":7.9320595264434814,"accuracy":0.37109375,"epoch":24,"val_accuracy":0.80078125,"_timestamp":1.589165825026039e+09,"val_loss":"NaN"}
-{"_step":25,"loss":"NaN","_runtime":8.057352781295776,"accuracy":0.37109375,"epoch":25,"val_accuracy":0.8984375,"_timestamp":1.5891658251513321e+09,"val_loss":"NaN"}
-{"_step":26,"loss":"NaN","_runtime":8.174784660339355,"accuracy":0.37109375,"epoch":26,"val_accuracy":0.80078125,"_timestamp":1.589165825268764e+09,"val_loss":"NaN"}
-{"_step":27,"loss":"NaN","_runtime":8.325506448745728,"accuracy":0.37109375,"epoch":27,"val_accuracy":0.8984375,"_timestamp":1.5891658254194858e+09,"val_loss":"NaN"}
-{"_step":28,"loss":"NaN","_runtime":8.446620225906372,"accuracy":0.369140625,"epoch":28,"val_accuracy":0.8984375,"_timestamp":1.5891658255405996e+09,"val_loss":"NaN"}
-{"_step":29,"loss":"NaN","_runtime":8.567692995071411,"accuracy":0.37109375,"epoch":29,"val_accuracy":0.8984375,"_timestamp":1.5891658256616724e+09,"val_loss":"NaN"}
-{"_step":30,"loss":"NaN","_runtime":8.678091526031494,"accuracy":0.37109375,"epoch":30,"val_accuracy":0.80078125,"_timestamp":1.589165825772071e+09,"val_loss":"NaN"}
-{"_step":31,"loss":"NaN","_runtime":8.855355739593506,"accuracy":0.37109375,"epoch":31,"val_accuracy":0.8984375,"_timestamp":1.589165825949335e+09,"val_loss":"NaN"}
-{"epoch": 0, "val_loss": NaN, "val_accuracy": 0.86328125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 13.265033483505249, "_timestamp": 1589165838.6728885, "_step": 32}
-{"epoch": 1, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 13.400344371795654, "_timestamp": 1589165838.8081994, "_step": 33}
-{"epoch": 2, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 13.516113042831421, "_timestamp": 1589165838.923968, "_step": 34}
-{"epoch": 3, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 13.623668193817139, "_timestamp": 1589165839.0315232, "_step": 35}
-{"epoch": 4, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 13.8060884475708, "_timestamp": 1589165839.2139435, "_step": 36}
-{"epoch": 5, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 13.926376581192017, "_timestamp": 1589165839.3342316, "_step": 37}
-{"epoch": 6, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 14.035024166107178, "_timestamp": 1589165839.4428792, "_step": 38}
-{"epoch": 7, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 14.20172119140625, "_timestamp": 1589165839.6095762, "_step": 39}
-{"epoch": 8, "val_loss": NaN, "val_accuracy": 0.875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 14.328731298446655, "_timestamp": 1589165839.7365863, "_step": 40}
-{"epoch": 9, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 14.441466808319092, "_timestamp": 1589165839.8493218, "_step": 41}
-{"epoch": 10, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 14.627290964126587, "_timestamp": 1589165840.035146, "_step": 42}
-{"epoch": 11, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 14.754547834396362, "_timestamp": 1589165840.1624029, "_step": 43}
-{"epoch": 12, "val_loss": NaN, "val_accuracy": 0.59375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 14.854538679122925, "_timestamp": 1589165840.2623937, "_step": 44}
-{"epoch": 13, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 15.026014566421509, "_timestamp": 1589165840.4338696, "_step": 45}
-{"epoch": 14, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 15.143637657165527, "_timestamp": 1589165840.5514927, "_step": 46}
-{"epoch": 15, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 15.255104780197144, "_timestamp": 1589165840.6629598, "_step": 47}
-{"epoch": 16, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 15.35708737373352, "_timestamp": 1589165840.7649424, "_step": 48}
-{"epoch": 17, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 15.542102813720703, "_timestamp": 1589165840.9499578, "_step": 49}
-{"epoch": 18, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 15.665008783340454, "_timestamp": 1589165841.0728638, "_step": 50}
-{"epoch": 19, "val_loss": NaN, "val_accuracy": 0.83984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 15.77570128440857, "_timestamp": 1589165841.1835563, "_step": 51}
diff --git a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/wandb-metadata.json b/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/wandb-metadata.json
deleted file mode 100644
index 8847b4c..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/wandb-metadata.json
+++ /dev/null
@@ -1,22 +0,0 @@
-{
-    "root": "/home/garthtrickett/ProdigyAI",
-    "program": "tabl.py",
-    "git": {
-        "remote": "https://github.com/garthtrickett/ProdigyAI",
-        "commit": "7667fefed2018a0664418c25c97e3c08866a6f71"
-    },
-    "email": "garthtrickett@gmail.com",
-    "startedAt": "2020-05-11T02:57:14.525010",
-    "host": "preemp-cpu-1",
-    "username": "garthtrickett",
-    "executable": "/home/garthtrickett/miniconda3/envs/ProdigyAI/bin/python",
-    "os": "Linux-5.3.0-1018-gcp-x86_64-with-debian-buster-sid",
-    "python": "3.7.6",
-    "cpu_count": 6,
-    "args": [],
-    "state": "running",
-    "jobType": null,
-    "mode": "run",
-    "project": "prodigyai",
-    "heartbeatAt": "2020-05-11T02:57:15.191357"
-}
diff --git a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/wandb-summary.json b/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/wandb-summary.json
deleted file mode 100644
index 6349c30..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025714-33cb2k5u/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"loss": NaN, "_step": 51, "epoch": 19, "graph": {"_type": "graph-file", "path": "media/graph/graph_summary_6f818560.graph.json", "sha256": "6f81856020e576034c23528bd5c8cc156872ed948db82ecb89f721130ae37682", "size": 1318}, "_runtime": 15.77570128440857, "accuracy": 0.37109375, "val_loss": NaN, "best_loss": 1.622144103050232, "_timestamp": 1589165841.1835563, "best_epoch": 2, "val_accuracy": 0.83984375}
diff --git a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/config.yaml b/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/config.yaml
deleted file mode 100644
index 8c1a8a9..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/config.yaml
+++ /dev/null
@@ -1,34 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.8.35
-    framework: keras
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.7.6
-dataset:
-  desc: null
-  value: esugj36b.h5
-params:
-  desc: null
-  value:
-    batch_size:
-      desc: Size of each mini-batch
-      value: 256
-    dropout:
-      desc: null
-      value: 0.1
-    epochs:
-      desc: Number of epochs to train over
-      value: 100
-    num_features:
-      desc: null
-      value: 40
-    window_length:
-      desc: null
-      value: 200
-yaml:
-  desc: null
-  value: yaml/tabl.yaml
diff --git a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/cp.ckpt b/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/cp.ckpt
deleted file mode 100644
index 4dc2765..0000000
Binary files a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/cp.ckpt and /dev/null differ
diff --git a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/diff.patch b/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/diff.patch
deleted file mode 100644
index 214be23..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/diff.patch
+++ /dev/null
@@ -1,1270 +0,0 @@
-diff --git a/pipeline/deeplob.py b/pipeline/deeplob.py
-index 2d6c076..fef42d4 100644
---- a/pipeline/deeplob.py
-+++ b/pipeline/deeplob.py
-@@ -2,6 +2,7 @@
- import pandas as pd
- import pickle
- import numpy as np
-+import tensorflow
- import tensorflow as tf
- from tensorflow.keras import layers
- 
-@@ -25,21 +26,17 @@ from keras.utils import np_utils
- import matplotlib.pyplot as plt
- import math
- from numba import njit, prange
--
--# Init wandb
--import wandb
--from wandb.keras import WandbCallback
--wandb.init(project="prodigyai")
-+import yaml
- 
- # set random seeds
- np.random.seed(1)
- tf.random.set_seed(2)
- 
- import keras
--from keras.callbacks import ModelCheckpoint
--from keras import backend as K
-+from tensorflow.keras.callbacks import ModelCheckpoint
- import h5py
--
-+import wandb
-+from wandb.keras import WandbCallback
- # check if using gpu
- gpus = tf.config.list_physical_devices()
- any_gpus = [s for s in gpus if "GPU" in s[0]]
-@@ -105,6 +102,23 @@ if cwd == home + "/":
-     cwd = cwd + "/ProdigyAI"
-     path_adjust = cwd
- 
-+# Init wandb
-+yaml_path = path_adjust + "yaml/deeplob.yaml"
-+with open(yaml_path) as file:
-+    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
-+
-+config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
-+wandb.init(dir="~/ProdigyAI/",
-+           project="prodigyai",
-+           config=config_dictionary,
-+           resume=True)
-+
-+window_length = wandb.config['params']['window_length']['value']
-+num_features = wandb.config['params']['num_features']['value']
-+epochs = wandb.config['params']['epochs']['value']
-+batch_size = wandb.config['params']['batch_size']['value']
-+number_of_lstm = wandb.config['params']['number_of_lstm']['value']
-+
- # limit gpu usage for keras
- gpu_devices = tf.config.experimental.list_physical_devices("GPU")
- for device in gpu_devices:
-@@ -216,7 +230,8 @@ import h5py
- 
- home = str(Path.home())
- 
--file_name = "arch=DLOB&name=two_model&WL=200&pt=1&sl=1&min_ret=4.347826086956522e-05&vbs=0.5&head=0&skip=0&vol_max=4.3498260869565224e-05&vol_min=4.348826086956522e-05&filter=none&cm_vol_mod=0&sw=on&fd=off&input=obook&ntb=True&tslbc=True.h5"
-+file_name = "esugj36b.h5"
-+wandb.config.update({'dataset': file_name})
- path = home + "/ProdigyAI/data/preprocessed/" + file_name
- h5f = h5py.File(path, "r")
- prices_for_window_index_array_train = h5f[
-@@ -268,6 +283,7 @@ def generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data):
- 
- class DataGenerator(tf.compat.v2.keras.utils.Sequence):
-     def __init__(self,
-+                 checkpoint_path,
-                  prices_for_window_index_array,
-                  input_features_normalized,
-                  y_data,
-@@ -277,6 +293,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
-                  to_fit,
-                  shuffle=True):
-         self.batch_size = batch_size
-+        self.checkpoint_path = checkpoint_path
-         self.prices_for_window_index_array = prices_for_window_index_array
-         self.input_features_normalized = input_features_normalized
-         self.labels = y_data
-@@ -323,6 +340,8 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
-     def on_epoch_end(self):
- 
-+        wandb.save(self.checkpoint_path)
-+
-         self.indexes = np.arange(len(self.prices_for_window_index_array))
- 
-         if self.shuffle:
-@@ -351,8 +370,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
- 
- n_classes = 3
--dim = (200, 40)
--batch_size = 64
-+dim = (window_length, num_features)
- to_fit = True
- 
- # for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,
-@@ -371,70 +389,6 @@ to_fit = True
- 
- ## PRODIGY AI HOCKUS POCKUS END
- 
--
--### Model Architecture
--def create_deeplob(T, NF, number_of_lstm):
--    input_lmd = Input(shape=(T, NF, 1))
--
--    # build the convolutional block
--    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    conv_first1 = Conv2D(32, (1, 10))(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    # build the inception module
--    convsecond_1 = Conv2D(64, (1, 1), padding="same")(conv_first1)
--    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
--    convsecond_1 = Conv2D(64, (3, 1), padding="same")(convsecond_1)
--    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
--
--    convsecond_2 = Conv2D(64, (1, 1), padding="same")(conv_first1)
--    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
--    convsecond_2 = Conv2D(64, (5, 1), padding="same")(convsecond_2)
--    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
--
--    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),
--                                padding="same")(conv_first1)
--    convsecond_3 = Conv2D(64, (1, 1), padding="same")(convsecond_3)
--    convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)
--
--    convsecond_output = concatenate([convsecond_1, convsecond_2, convsecond_3],
--                                    axis=3)
--
--    # use the MC dropout here
--    conv_reshape = Reshape(
--        (int(convsecond_output.shape[1]),
--         int(convsecond_output.shape[3])))(convsecond_output)
--
--    conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)
--
--    # build the output layer
--    out = Dense(3, activation="softmax")(conv_lstm)
--    model = Model(inputs=input_lmd, outputs=out)
--    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
--    model.compile(optimizer=adam,
--                  loss="categorical_crossentropy",
--                  metrics=["accuracy"])
--
--    return model
--
--
- checkpoint_path = path_adjust + "temp/cp.ckpt"
- # Create a callback that saves the model's weights
- cp_callback = ModelCheckpoint(filepath=checkpoint_path,
-@@ -446,9 +400,8 @@ if check_point_file.exists() and resuming == "resuming":
-     print("weights loaded")
-     model.load_weights(checkpoint_path)
- 
--batch_size = 64
--
--train_generator = DataGenerator(prices_for_window_index_array_train,
-+train_generator = DataGenerator(checkpoint_path,
-+                                prices_for_window_index_array_train,
-                                 input_features_normalized_train,
-                                 y_train,
-                                 batch_size,
-@@ -457,7 +410,8 @@ train_generator = DataGenerator(prices_for_window_index_array_train,
-                                 to_fit,
-                                 shuffle=True)
- 
--val_generator = DataGenerator(prices_for_window_index_array_val,
-+val_generator = DataGenerator(checkpoint_path,
-+                              prices_for_window_index_array_val,
-                               input_features_normalized_val,
-                               y_val,
-                               batch_size,
-@@ -469,14 +423,87 @@ val_generator = DataGenerator(prices_for_window_index_array_val,
- steps_per_epoch = len(train_generator)
- validation_steps = len(val_generator)
- 
--deeplob = create_deeplob(200, 40, 64)
--deeplob.fit(train_generator,
--            steps_per_epoch=steps_per_epoch,
--            validation_steps=validation_steps,
--            epochs=100,
--            verbose=2,
--            validation_data=val_generator,
--            callbacks=[cp_callback, WandbCallback()])
-+if wandb.run.resumed:
-+    wandb.restore("model-best.h5",run_path="garthtrickett/prodigyai/" + wandb.run.id)
-+    # restore the best model
-+    deeplob = tensorflow.keras.models.load_model(wandb.restore("model-best.h5").name)
-+    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
-+    deeplob.compile(optimizer=adam,
-+                    loss="categorical_crossentropy",
-+                    metrics=["accuracy"])
-+else:
-+    ### Model Architecture
-+    def create_deeplob(T, NF, number_of_lstm):
-+        input_lmd = Input(shape=(T, NF, 1))
-+
-+        # build the convolutional block
-+        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        conv_first1 = Conv2D(32, (1, 10))(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        # build the inception module
-+        convsecond_1 = Conv2D(64, (1, 1), padding="same")(conv_first1)
-+        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
-+        convsecond_1 = Conv2D(64, (3, 1), padding="same")(convsecond_1)
-+        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
-+
-+        convsecond_2 = Conv2D(64, (1, 1), padding="same")(conv_first1)
-+        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
-+        convsecond_2 = Conv2D(64, (5, 1), padding="same")(convsecond_2)
-+        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
-+
-+        convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),
-+                                    padding="same")(conv_first1)
-+        convsecond_3 = Conv2D(64, (1, 1), padding="same")(convsecond_3)
-+        convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)
-+
-+        convsecond_output = concatenate(
-+            [convsecond_1, convsecond_2, convsecond_3], axis=3)
-+
-+        # use the MC dropout here
-+        conv_reshape = Reshape(
-+            (int(convsecond_output.shape[1]),
-+             int(convsecond_output.shape[3])))(convsecond_output)
-+
-+        conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)
-+
-+        # build the output layer
-+        out = Dense(3, activation="softmax")(conv_lstm)
-+        model = Model(inputs=input_lmd, outputs=out)
-+        adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
-+        model.compile(optimizer=adam,
-+                      loss="categorical_crossentropy",
-+                      metrics=["accuracy"])
-+
-+        return model
-+
-+    deeplob = create_deeplob(window_length, num_features, number_of_lstm)
-+deeplob.fit(
-+    train_generator,
-+    steps_per_epoch=steps_per_epoch,
-+    validation_steps=validation_steps,
-+    epochs=100,
-+    verbose=2,
-+    validation_data=val_generator,
-+    callbacks=[cp_callback,
-+               WandbCallback(save_model=True, monitor="loss")])
- 
- finished_weights_path = path_adjust + "temp/cp_end.ckpt"
- deeplob.save_weights(finished_weights_path)
-diff --git a/pipeline/preprocessing.py b/pipeline/preprocessing.py
-index 932ae0b..714d325 100644
---- a/pipeline/preprocessing.py
-+++ b/pipeline/preprocessing.py
-@@ -136,54 +136,67 @@ if stage == 2:
-     side = X_for_all_labels["predicted_bins"]
-     # Could use the probabilities (instead of [1,0,0] use [0.2,0.55,0.25]
- 
-+import yaml
-+import wandb
-+
-+yaml_path = path_adjust + "yaml/preprocessing.yaml"
-+with open(yaml_path) as file:
-+    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
-+
-+config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
-+
-+wandb.init(
-+    dir="~/ProdigyAI/",
-+    project="prodigyai",
-+    config=config_dictionary,
-+)
-+
-+minimum_return = eval(wandb.config['params']['minimum_return']['value'])
-+vertical_barrier_seconds = eval(
-+    wandb.config['params']['vertical_barrier_seconds']['value'])
-+
- # Parameters
- parameters = dict()
--model_arch = "TABL"
--parameters["arch"] = model_arch
--parameters["name"] = model
--parameters["WL"] = 200  # WINDOW LONG
--parameters["pt"] = 1
--parameters["sl"] = 1
--parameters["min_ret"] = 0.001 * 1 / 23
--parameters["vbs"] = round(1 / 2,
--                          3)  # Increasing this decreases vertical touches
--parameters["head"] = 1000  # take only first x number of rows 0 means of
--parameters["skip"] = 0  # sample every n'th row if skip > 0
--# get even classes at fraction = 0.8 so the training set is balanced then set to 1
--# 3 million rows is about the limit of before it starts taking ages / memory maxing
--parameters["vol_max"] = (
--    parameters["min_ret"] + 0.00000002
-+wandb.config['params']['head'][
-+    'value'] = 1000  # take only first x number of rows 0 means of
-+volume_max = (
-+    minimum_return + wandb.config['params']['vol_max_modifier']['value']
- )  # The higher this is the more an increase in volatility requries an increase
- # in return to be considered buy/sell (Increasing this increases end barrier vertical touches)
--parameters["vol_min"] = parameters["min_ret"] + 0.00000001
-+volume_min = minimum_return + wandb.config['params']['vol_min_modifier'][
-+    'value']
- 
--parameters["filter"] = "none"
-+filter_type = wandb.config['params']['filter_type']['value']
- 
--if parameters["filter"] == "cm":
--    parameters["cm_vol_mod"] = 500
-+if filter_type == "cm":
-+    cusum_filter_vol_modifier = wandb.config['params'][
-+        'cusum_filter_volume_modifier']['value']
- else:
--    parameters["cm_vol_mod"] = 0
-+    cusum_filter_vol_modifier = 0
- 
--parameters["sw"] = "on"  # sample weights
--parameters["fd"] = "off"
-+use_sample_weights = wandb.config['params']['use_sample_weights']['value']
-+use_sample_weights = wandb.config['params']['use_fractional_differentiation'][
-+    'value']
- 
--parameters["input"] = "obook"
-+input_type = wandb.config['params']['input_type']['value']
- 
--parameters["ntb"] = True  # non time bars
-+# parameters["ntb"] = True  # non time bars
- 
--if parameters["ntb"] == True:
--    # Pick whether you want to add in the time since last bar input feature
--    # time since last bar column
--    parameters["tslbc"] = True  # time since last bar column
--else:
--    # Pick whether you want to add in the volume input feature
--    parameters["vbc"] = True  # volume bar column
-+# if parameters["ntb"] == True:
-+#     # Pick whether you want to add in the time since last bar input feature
-+#     # time since last bar column
-+#     parameters["tslbc"] = True  # time since last bar column
-+# else:
-+#     # Pick whether you want to add in the volume input feature
-+#     parameters["vbc"] = True  # volume bar column
- 
- # Create the txt file string
--parameter_string = "&".join("{}{}{}".format(key, "=", val)
--                            for key, val in parameters.items())
-+parameter_string = wandb.run.id
- 
--pt_sl = [parameters["pt"], parameters["sl"]]
-+pt_sl = [
-+    wandb.config['params']['profit_taking_multiplier']['value'],
-+    wandb.config['params']['stop_loss_multiplier']['value']
-+]
- cpus = cpu_count() - 1
- 
- regenerate_features_and_labels = True
-@@ -192,10 +205,10 @@ if regenerate_features_and_labels == True:
-     if stage == 1:
-         # Side
-         print("starting data load")
--        head = parameters["head"]
-+        head = wandb.config['params']['head']['value']
- 
-         # # read parquet file of dollar bars
--        if parameters["input"] == "bars":
-+        if input_type == "bars":
-             # Mlfinlab bars
-             data = pq.read_pandas(
-                 path_adjust + "data/bars/"
-@@ -216,7 +229,7 @@ if regenerate_features_and_labels == True:
-             data.index = pd.to_datetime(data.index, infer_datetime_format=True)
- 
-         # read parquet file of raw ticks
--        if parameters["input"] == "ticks":
-+        if input_type == "ticks":
-             data = pq.read_pandas(
-                 path_adjust + "data/bars/" +
-                 "btcusdt_agg_trades_raw_tick_data.parquet").to_pandas()
-@@ -238,11 +251,7 @@ if regenerate_features_and_labels == True:
-             # Should do something else than drop the duplicates (maybe trades doesnt have duplicate indexes rather than aggtrades)
-             data = data.loc[~data.index.duplicated(keep="first")]
- 
--            # skip most rows if this is > 1
--            if parameters["skip"] > 0:
--                data = data.iloc[::parameters["skip"], :]
--
--        if parameters["input"] == "obook":
-+        if input_type == "orderbook":
-             with open(path_adjust + "temp/orderbook_data_name.txt",
-                       "r") as text_file:
-                 orderbook_preprocessed_file_name = text_file.read()
-@@ -275,7 +284,7 @@ if regenerate_features_and_labels == True:
-         # duplicate_fast_search(data.index.duplicated())
- 
-         # Fractional differentiation
--        if parameters["fd"] == "on":
-+        if use_sample_weights == "on":
-             data_series = data["close"].to_frame()
-             # # generate 100 points
-             # nsample = 1000
-@@ -313,16 +322,16 @@ if regenerate_features_and_labels == True:
- 
-         start = time.time()
-         volatility_level_array = volatility_levels_numba(
--            np.ascontiguousarray(data.close.values), parameters["WL"])
-+            np.ascontiguousarray(data.close.values),
-+            wandb.config['params']['window_length']['value'])
-         data["window_volatility_level"] = volatility_level_array
- 
-         # Should adjust the max value
-         # To get more vertical touches we can
-         # either increase vol_max or
-         # decrease the window seconds
--        scaler = MinMaxScaler(
--            feature_range=(parameters["vol_min"],
--                           parameters["vol_max"]))  # normalization
-+        scaler = MinMaxScaler(feature_range=(volume_min,
-+                                             volume_max))  # normalization
- 
-         normed_window_volatility_level = scaler.fit_transform(
-             data[["window_volatility_level"]])
-@@ -338,7 +347,7 @@ if regenerate_features_and_labels == True:
-         close_np_array, close_index_np_array = pandas_series_to_numba_ready_np_arrays(
-             close_copy)
- 
--        volatility_threshold = volatility_threshold * parameters["cm_vol_mod"]
-+        volatility_threshold = volatility_threshold * cusum_filter_vol_modifier
-         print("data_len = " + str(len(data)))
-         start = time.time()
-         sampled_idx = filter_events(
-@@ -346,7 +355,7 @@ if regenerate_features_and_labels == True:
-             close_np_array,
-             close_index_np_array,
-             volatility_threshold,
--            parameters["filter"],
-+            filter_type,
-         )
-         print("sampled_idx_len = " + str(len(sampled_idx)))
-         end = time.time()
-@@ -356,7 +365,8 @@ if regenerate_features_and_labels == True:
-         # size
-         start = time.time()
-         volatility_level_array = volatility_levels_numba(
--            data.close.values, parameters["WL"])
-+            data.close.values,
-+            wandb.config['params']['window_length']['value'])
-         data["window_volatility_level"] = volatility_level_array
- 
-     # This code runs for both first and second stage preprocessing
-@@ -364,7 +374,7 @@ if regenerate_features_and_labels == True:
-     vertical_barrier_timestamps = ml.labeling.add_vertical_barrier(
-         t_events=sampled_idx,
-         close=data["close"],
--        num_seconds=parameters["vbs"])
-+        num_seconds=vertical_barrier_seconds)
-     end = time.time()
-     print("vertical barrier" + str(end - start))
- 
-@@ -376,7 +386,7 @@ if regenerate_features_and_labels == True:
-         t_events=sampled_idx,
-         pt_sl=pt_sl,
-         target=data["window_volatility_level"],
--        min_ret=parameters["min_ret"],
-+        min_ret=minimum_return,
-         num_threads=cpus * 2,
-         vertical_barrier_times=vertical_barrier_timestamps,
-         side_prediction=side,
-@@ -533,26 +543,27 @@ if stage == 1:
-     ### ORDERBOOK VOLUME DATA
-     volumes_for_all_labels = volumes.loc[data.close.index]
- 
--    ## TRADE DATA
--    input_features_trade = []
--    close_array = data.close.values
--    input_features_trade.append(close_array)
-+    # ## TRADE DATA
-+    # input_features_trade = []
-+    # close_array = data.close.values
-+    # input_features_trade.append(close_array)
- 
--    if parameters["ntb"] == False and parameters["vbc"] == True:
--        volume_array = data.volume.values
--        input_features_trade.append(volume_array)
--    if parameters["ntb"] == True and parameters["tslbc"] == True:
--        time_since_last_bar_array = data.time_since_last_bar.values
--        input_features_trade.append(time_since_last_bar_array)
-+    # if parameters["ntb"] == False and parameters["vbc"] == True:
-+    #     volume_array = data.volume.values
-+    #     input_features_trade.append(volume_array)
-+    # if parameters["ntb"] == True and parameters["tslbc"] == True:
-+    #     time_since_last_bar_array = data.time_since_last_bar.values
-+    #     input_features_trade.append(time_since_last_bar_array)
- 
-     end_time = time.time()
-     print(end_time - start_time)
- 
--    # min max limits
-+    # Type of scaling to apply
-+    scaling_type = wandb.config['params']['scaling_type']['value']
- 
--    minimum = -1
--    maximum = 1
--    scaling_type = "z_score"
-+    # min max limits
-+    minimum = wandb.config['params']['scaling_maximum']['value']
-+    maximum = wandb.config['params']['scaling_minimum']['value']
- 
-     ### Split intothe training/validation/test sets
-     print("splitting into train/va/test sets start")
-@@ -573,11 +584,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_train = volumes_for_all_labels.iloc[
-         train_close_array_integer_index[0] -
--        parameters["WL"]:train_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        train_close_array_integer_index[-1] + 2]
- 
-     close_index_array_train = close_index_array[
-         train_close_array_integer_index[0] -
--        parameters["WL"]:train_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        train_close_array_integer_index[-1] + 2]
- 
-     end_time = time.time()
- 
-@@ -623,7 +636,7 @@ if stage == 1:
-     # print("Make window started")
-     # start_time = time.time()
- 
--    # padding = parameters["WL"] * 2
-+    # padding = wandb.config['params']['window_length']['value'] * 2
- 
-     # split_by = 100000
- 
-@@ -652,7 +665,7 @@ if stage == 1:
-     #         len(prices_for_window_index_array_train[start_index:end_index]),
-     #         input_features_normalized_train[:, close_and_input_start_index:
-     #                                         close_and_input_end_index],
--    #         parameters["WL"],
-+    #         wandb.config['params']['window_length']['value'],
-     #         model_arch,
-     #     )
- 
-@@ -697,11 +710,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_val = volumes_for_all_labels.iloc[
-         val_close_array_integer_index[0] -
--        parameters["WL"]:val_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        val_close_array_integer_index[-1] + 2]
- 
-     close_index_array_val = close_index_array[
-         val_close_array_integer_index[0] -
--        parameters["WL"]:val_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        val_close_array_integer_index[-1] + 2]
- 
-     input_features_val = make_input_features_from_orderbook_data(
-         volumes_for_all_labels_val)
-@@ -721,7 +736,7 @@ if stage == 1:
-     #     #     prices_for_window_index_array_val,
-     #     #     close_index_array_val,
-     #     #     input_features_normalized_val,
--    #     #     parameters["WL"],
-+    #     #     wandb.config['params']['window_length']['value'],
-     #     #     model_arch,
-     #     # )
- 
-@@ -736,11 +751,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_test = volumes_for_all_labels.iloc[
-         test_close_array_integer_index[0] -
--        parameters["WL"]:test_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        test_close_array_integer_index[-1] + 2]
- 
-     close_index_array_test = close_index_array[
-         test_close_array_integer_index[0] -
--        parameters["WL"]:test_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        test_close_array_integer_index[-1] + 2]
- 
-     input_features_test = make_input_features_from_orderbook_data(
-         volumes_for_all_labels_test)
-@@ -761,7 +778,7 @@ if stage == 1:
- #     #     prices_for_window_index_array_test,
- #     #     close_index_array_test,
- #     #     input_features_normalized_test,
--#     #     parameters["WL"],
-+#     #     wandb.config['params']['window_length']['value'],
- #     #     model_arch,
- #     # )
- 
-@@ -782,8 +799,9 @@ ax = plt.gca()
- 
- data.iloc[window_index - 200:window_index + 200].plot(y="close",
-                                                       use_index=True)
--plot_window_and_touch_and_label(window_index, parameters["WL"], data,
--                                triple_barrier_events, labels)
-+plot_window_and_touch_and_label(
-+    window_index, wandb.config['params']['window_length']['value'], data,
-+    triple_barrier_events, labels)
- 
- data.iloc[window_index - 10:window_index + 30]
- 
-@@ -837,9 +855,9 @@ print("writing train/val/test to .h5 files finished taking " +
- #     h5f.create_dataset("P", data=P)
- # h5f.create_dataset("y", data=y)
- # h5f.create_dataset("sampled_idx_epoch", data=sampled_idx_epoch)
--# if parameters["sw"] == "on":
-+# if use_sample_weights == "on":
- #     h5f.create_dataset("sample_weights", data=sample_weights)
--# elif parameters["sw"] == "off":
-+# elif use_sample_weights == "off":
- #     h5f.create_dataset("sample_weights", data=np.zeros(1))
- # h5f.close()
- 
-diff --git a/pipeline/tabl.py b/pipeline/tabl.py
-index 87d51f8..f0244ec 100644
---- a/pipeline/tabl.py
-+++ b/pipeline/tabl.py
-@@ -6,7 +6,7 @@ import sys
- 
- import imblearn
- from imblearn.over_sampling import SMOTE
--
-+import tensorflow
- import matplotlib.pyplot as plt
- 
- sys.path.append("..")
-@@ -17,29 +17,26 @@ home = str(Path.home())
- sys.path.append(home + "/ProdigyAI")
- 
- from numba import njit, prange
--import tensorflow as tf
-+
- import math
- 
--import keras
- import numpy as np
- from third_party_libraries.TABL import Models
- from third_party_libraries.keras_lr_finder.lr_finder import LRFinder
- from third_party_libraries.CLR.clr_callbacks import CyclicLR
- 
--from tensorflow.keras.losses import categorical_crossentropy
--from tensorflow.keras.optimizers import SGD, Adam
-+import keras
-+from keras.losses import categorical_crossentropy
-+from keras.optimizers import SGD, Adam
- from keras.callbacks import ModelCheckpoint
- 
-+import tensorflow as tf
- # Init wandb
- import wandb
- from wandb.keras import WandbCallback
- 
- import yaml
- 
--# import tensorflow.keras as keras
--# import tensorflow.keras.layers as layers
--# import tensorflow.keras.backend as K
--
- # Sorting out whether we are using the ipython kernel or not
- try:
-     get_ipython()
-@@ -95,25 +92,24 @@ with open(yaml_path) as file:
-     yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
- 
- config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
--wandb.init(
--    dir="~/ProdigyAI/",
--    project="prodigyai",
--    config=config_dictionary,
--)
--
--# try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes
--
--# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1
--template = [[40, 200], [60, 10], [120, 5], [3, 1]]
--
--# example data
--example_x = np.random.rand(1000, 40, 10)
--np.min(example_x)
--np.max(example_x)
--np.mean(example_x)
--example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)
--template = [[40, 200], [60, 10], [120, 5], [3, 1]]
--# 200(WINDOW LENGTH)
-+wandb.init(dir="~/ProdigyAI/",
-+           project="prodigyai",
-+           config=config_dictionary,
-+           resume=True)
-+
-+window_length = wandb.config['params']['window_length']['value']
-+num_features = wandb.config['params']['num_features']['value']
-+epochs = wandb.config['params']['epochs']['value']
-+batch_size = wandb.config['params']['batch_size']['value']
-+dropout = wandb.config['params']['dropout']['value']
-+
-+# # try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes
-+# # example data
-+# example_x = np.random.rand(1000, 40, 10)
-+# np.min(example_x)
-+# np.max(example_x)
-+# np.mean(example_x)
-+# example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)
- 
- ## PRODIGY AI HOCKUS POCKUS START
- from pathlib import Path
-@@ -121,7 +117,7 @@ import h5py
- 
- home = str(Path.home())
- 
--file_name = "arch=TABL&name=two_model&WL=200&pt=1&sl=1&min_ret=4.347826086956522e-05&vbs=0.5&head=1000&skip=0&vol_max=4.3498260869565224e-05&vol_min=4.348826086956522e-05&filter=none&cm_vol_mod=0&sw=on&fd=off&input=obook&ntb=True&tslbc=True.h5"
-+file_name = "esugj36b.h5"
- wandb.config.update({'dataset': file_name})
- path = home + "/ProdigyAI/data/preprocessed/" + file_name
- h5f = h5py.File(path, "r")
-@@ -265,8 +261,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
- 
- n_classes = 3
--dim = (200, 40)
--batch_size = 64
-+dim = (window_length, num_features)
- to_fit = True
- 
- # for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,
-@@ -280,22 +275,15 @@ to_fit = True
- #     import pdb
- #     pdb.set_trace()
- 
-+# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1
-+
-+template = [[num_features, window_length], [60, 10], [120, 5], [3, 1]]
-+
- # get Bilinear model
- projection_regularizer = None
- projection_constraint = keras.constraints.max_norm(3.0, axis=0)
- attention_regularizer = None
- attention_constraint = keras.constraints.max_norm(5.0, axis=1)
--dropout = 0.1
--
--model = Models.TABL(
--    template,
--    dropout,
--    projection_regularizer,
--    projection_constraint,
--    attention_regularizer,
--    attention_constraint,
--)
--model.summary()
- 
- # path = path_adjust + "data/lob_2010/train_and_test_tabl.h5"
- # h5f = h5py.File(path, "r")
-@@ -442,6 +430,18 @@ clr = CyclicLR(base_lr=base_lr,
- 
- optimizer = keras.optimizers.Adam()
- 
-+model = Models.TABL(
-+    template,
-+    dropout,
-+    projection_regularizer,
-+    projection_constraint,
-+    attention_regularizer,
-+    attention_constraint,
-+)
-+model.summary()
-+
-+
-+
- # Compile the model
- model.compile(loss=loss_function, optimizer=optimizer, metrics=["accuracy"])
- # Create a callback that saves the model's weights
-@@ -449,10 +449,15 @@ cp_callback = ModelCheckpoint(filepath=checkpoint_path,
-                               save_weights_only=True,
-                               verbose=1)
- 
--check_point_file = Path(checkpoint_path)
--if check_point_file.exists() and resuming == "resuming":
--    print("weights loaded")
--    model.load_weights(checkpoint_path)
-+if wandb.run.resumed:
-+    weights = wandb.restore("cp.ckpt",
-+                            run_path="garthtrickett/prodigyai/" + wandb.run.id)
-+    model.load_weights(weights.name)
-+
-+# check_point_file = Path(checkpoint_path)
-+# if check_point_file.exists() and resuming == "resuming":
-+#     print("weights loaded")
-+#     model.load_weights(checkpoint_path)
- # Fit data to model
- # history = model.fit(trainX_CNN,
- #                     trainY_CNN,
-@@ -460,9 +465,6 @@ if check_point_file.exists() and resuming == "resuming":
- #                     epochs=20,
- #                     callbacks=[clr, cp_callback])
- 
--finished_weights_path = path_adjust + "temp/cp_end.ckpt"
--model.save_weights(finished_weights_path)
--
- # create class weight
- # class_weight = {0: 1e6 / 300.0, 1: 1e6 / 400.0, 2: 1e6 / 300.0}
- train_generator = DataGenerator(checkpoint_path,
-@@ -489,12 +491,14 @@ steps_per_epoch = len(train_generator)
- validation_steps = len(val_generator)
- 
- # example sata training
--model.fit(train_generator,
--          steps_per_epoch=steps_per_epoch,
--          validation_steps=validation_steps,
--          epochs=1000,
--          validation_data=val_generator,
--          callbacks=[cp_callback, WandbCallback()])
-+model.fit(
-+    train_generator,
-+    steps_per_epoch=steps_per_epoch,
-+    validation_steps=validation_steps,
-+    epochs=epochs,
-+    validation_data=val_generator,
-+    callbacks=[cp_callback,
-+               WandbCallback(save_model=True, monitor="loss")])
- 
- model.save(os.path.join(wandb.run.dir, "model.h5"))
- # no class weight
-@@ -603,16 +607,16 @@ model.save(os.path.join(wandb.run.dir, "model.h5"))
- # X_val = X_val.reshape((X_val.shape[0], 40, 10))
- 
- # train one epoch to find the learning rate
--model.fit(
--    X_train,
--    y_train,
--    validation_data=(X_val, y_val),
--    batch_size=256,
--    epochs=1,
--    shuffle=False,
--)  # no class weight
--
--score = model.evaluate(x=X_test, y=y_test, batch_size=256)
-+# model.fit(
-+#     X_train,
-+#     y_train,
-+#     validation_data=(X_val, y_val),
-+#     batch_size=256,
-+#     epochs=1,
-+#     shuffle=False,
-+# )  # no class weight
-+
-+# score = model.evaluate(x=X_test, y=y_test, batch_size=256)
- 
- # Save model to wandb
- 
-diff --git a/temp/cp.ckpt.data-00000-of-00001 b/temp/cp.ckpt.data-00000-of-00001
-index 56c00e4..3d9600c 100644
-Binary files a/temp/cp.ckpt.data-00000-of-00001 and b/temp/cp.ckpt.data-00000-of-00001 differ
-diff --git a/temp/cp.ckpt.index b/temp/cp.ckpt.index
-index 3770713..d6d3b4b 100644
-Binary files a/temp/cp.ckpt.index and b/temp/cp.ckpt.index differ
-diff --git a/temp/cp_end.ckpt b/temp/cp_end.ckpt
-index c1c1d9e..62e0a67 100644
-Binary files a/temp/cp_end.ckpt and b/temp/cp_end.ckpt differ
-diff --git a/temp/cp_end.ckpt.data-00000-of-00001 b/temp/cp_end.ckpt.data-00000-of-00001
-index 1d572d0..12d3157 100644
-Binary files a/temp/cp_end.ckpt.data-00000-of-00001 and b/temp/cp_end.ckpt.data-00000-of-00001 differ
-diff --git a/temp/cp_end.ckpt.index b/temp/cp_end.ckpt.index
-index 15de09a..0ab67e1 100644
-Binary files a/temp/cp_end.ckpt.index and b/temp/cp_end.ckpt.index differ
-diff --git a/temp/is_script_finished.txt b/temp/is_script_finished.txt
-index 40ca0d3..bec7a08 100644
---- a/temp/is_script_finished.txt
-+++ b/temp/is_script_finished.txt
-@@ -1 +1 @@
--full_script_time64.39192986488342
-\ No newline at end of file
-+full_script_time25.707124948501587
-\ No newline at end of file
-Submodule third_party_libraries/TABL contains modified content
-Submodule third_party_libraries/TABL 7f25314..df3e92d:
-diff --git a/third_party_libraries/TABL/Layers.py b/third_party_libraries/TABL/Layers.py
-index 9020002..b299e57 100644
---- a/third_party_libraries/TABL/Layers.py
-+++ b/third_party_libraries/TABL/Layers.py
-@@ -3,8 +3,9 @@
- """
- @author: Dat Tran (dat.tranthanh@tut.fi)
- """
-+import keras
- from keras import backend as K
--from keras.engine.topology import Layer
-+from keras.layers import Layer
- from keras import activations as Activations
- from keras import initializers as Initializers
- 
-@@ -18,25 +19,25 @@ class Constraint(object):
- 
-     def get_config(self):
-         return {}
--    
-+
-+
- class MinMax(Constraint):
-     """
-     Customized min-max constraint for scalar
-     """
--
-     def __init__(self, min_value=0.0, max_value=10.0):
-         self.min_value = min_value
-         self.max_value = max_value
-+
-     def __call__(self, w):
--        
--        return K.clip(w,self.min_value,self.max_value)
-+
-+        return K.clip(w, self.min_value, self.max_value)
- 
-     def get_config(self):
--        return {'min_value': self.min_value,
--                'max_value': self.max_value}
--        
-+        return {'min_value': self.min_value, 'max_value': self.max_value}
- 
--def nmodeproduct(x,w,mode):
-+
-+def nmodeproduct(x, w, mode):
-     """
-     n-mode product for 2D matrices
-     x: NxHxW
-@@ -45,74 +46,83 @@ def nmodeproduct(x,w,mode):
-     
-     output: NxhxW (mode1) or NxHxw (mode2)
-     """
--    if mode==2:
--        x=K.dot(x,w)
-+    if mode == 2:
-+        x = K.dot(x, w)
-     else:
--        x=K.permute_dimensions(x,(0,2,1))
--        x=K.dot(x,w)
--        x=K.permute_dimensions(x,(0,2,1))
-+        x = K.permute_dimensions(x, (0, 2, 1))
-+        x = K.dot(x, w)
-+        x = K.permute_dimensions(x, (0, 2, 1))
-     return x
- 
-+
- class BL(Layer):
-     """
-     Bilinear Layer
-     """
--    def __init__(self, output_dim,
-+    def __init__(self,
-+                 output_dim,
-                  kernel_regularizer=None,
--                 kernel_constraint=None,**kwargs):
-+                 kernel_constraint=None,
-+                 **kwargs):
-         """
-         output_dim : output dimensions of 2D tensor, should be a list of len 2, e.g. [30,20]
-         kernel_regularizer : keras regularizer object
-         kernel_constraint: keras constraint object
-         """
--        
-+
-         self.output_dim = output_dim
--        self.kernel_regularizer=kernel_regularizer
--        self.kernel_constraint=kernel_constraint
--        
-+        self.kernel_regularizer = kernel_regularizer
-+        self.kernel_constraint = kernel_constraint
-+
-         super(BL, self).__init__(**kwargs)
- 
-     def build(self, input_shape):
--        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
--                                      initializer='he_uniform',
--                                      regularizer=self.kernel_regularizer,
--                                      constraint=self.kernel_constraint,
--                                      trainable=True)
--        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
--                                      initializer='he_uniform',
--                                      regularizer=self.kernel_regularizer,
--                                      constraint=self.kernel_constraint,
--                                      trainable=True)
--
--        self.bias=self.add_weight(name='bias',shape=(self.output_dim[0],self.output_dim[1]),
--                              initializer='zeros',trainable=True)
-+        self.W1 = self.add_weight(name='W1',
-+                                  shape=(input_shape[1], self.output_dim[0]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.kernel_regularizer,
-+                                  constraint=self.kernel_constraint,
-+                                  trainable=True)
-+        self.W2 = self.add_weight(name='W2',
-+                                  shape=(input_shape[2], self.output_dim[1]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.kernel_regularizer,
-+                                  constraint=self.kernel_constraint,
-+                                  trainable=True)
-+
-+        self.bias = self.add_weight(name='bias',
-+                                    shape=(self.output_dim[0],
-+                                           self.output_dim[1]),
-+                                    initializer='zeros',
-+                                    trainable=True)
-         super(BL, self).build(input_shape)
- 
-     def call(self, x):
-         print(K.int_shape(x))
--        x = nmodeproduct(x,self.W1,1)
--        x = nmodeproduct(x,self.W2,2)
--        x = K.bias_add(x,self.bias)
--        
--        if self.output_dim[1]==1:
--            x = K.squeeze(x,axis=-1)
-+        x = nmodeproduct(x, self.W1, 1)
-+        x = nmodeproduct(x, self.W2, 2)
-+        x = K.bias_add(x, self.bias)
-+
-+        if self.output_dim[1] == 1:
-+            x = K.squeeze(x, axis=-1)
-         print(K.int_shape(x))
-         return x
- 
-     def compute_output_shape(self, input_shape):
--        if self.output_dim[1]==1:
-+        if self.output_dim[1] == 1:
-             return (input_shape[0], self.output_dim[0])
-         else:
-             return (input_shape[0], self.output_dim[0], self.output_dim[1])
--        
--        
-+
-+
- class TABL(Layer):
-     """
-     Temporal Attention augmented Bilinear Layer
-     https://arxiv.org/abs/1712.00975
-     
-     """
--    def __init__(self, output_dim,
-+    def __init__(self,
-+                 output_dim,
-                  projection_regularizer=None,
-                  projection_constraint=None,
-                  attention_regularizer=None,
-@@ -125,45 +135,52 @@ class TABL(Layer):
-         attention_regularizer: keras regularizer object for attention matrix
-         attention_constraint: keras constraint object for attention matrix
-         """
--        
-+
-         self.output_dim = output_dim
-         self.projection_regularizer = projection_regularizer
-         self.projection_constraint = projection_constraint
-         self.attention_regularizer = attention_regularizer
-         self.attention_constraint = attention_constraint
--        
-+
-         super(TABL, self).__init__(**kwargs)
- 
-     def build(self, input_shape):
--        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
--                                      initializer='he_uniform',
--                                      regularizer=self.projection_regularizer,
--                                      constraint=self.projection_constraint,
--                                      trainable=True)
--        
--        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
--                                      initializer='he_uniform',
--                                      regularizer=self.projection_regularizer,
--                                      constraint=self.projection_constraint,
--                                      trainable=True)
--        
--        self.W = self.add_weight(name='W',shape=(input_shape[2], input_shape[2]),
--                                      initializer=Initializers.Constant(1.0/input_shape[2]),
--                                      regularizer=self.attention_regularizer,
--                                      constraint=self.attention_constraint,
--                                      trainable=True)
--        
--        self.alpha = self.add_weight(name='alpha',shape=(1,),
--                                      initializer=Initializers.Constant(0.5),
--                                      constraint=MinMax(),
--                                      trainable=True)
--
--
--        self.bias=self.add_weight(name='bias',shape=(1, self.output_dim[0],self.output_dim[1]),
--                              initializer='zeros',trainable=True)
--        
-+        self.W1 = self.add_weight(name='W1',
-+                                  shape=(input_shape[1], self.output_dim[0]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.projection_regularizer,
-+                                  constraint=self.projection_constraint,
-+                                  trainable=True)
-+
-+        self.W2 = self.add_weight(name='W2',
-+                                  shape=(input_shape[2], self.output_dim[1]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.projection_regularizer,
-+                                  constraint=self.projection_constraint,
-+                                  trainable=True)
-+
-+        self.W = self.add_weight(name='W',
-+                                 shape=(input_shape[2], input_shape[2]),
-+                                 initializer=Initializers.Constant(
-+                                     1.0 / input_shape[2]),
-+                                 regularizer=self.attention_regularizer,
-+                                 constraint=self.attention_constraint,
-+                                 trainable=True)
-+
-+        self.alpha = self.add_weight(name='alpha',
-+                                     shape=(1, ),
-+                                     initializer=Initializers.Constant(0.5),
-+                                     constraint=MinMax(),
-+                                     trainable=True)
-+
-+        self.bias = self.add_weight(name='bias',
-+                                    shape=(1, self.output_dim[0],
-+                                           self.output_dim[1]),
-+                                    initializer='zeros',
-+                                    trainable=True)
-+
-         self.in_shape = input_shape
--        
-+
-         super(TABL, self).build(input_shape)
- 
-     def call(self, x):
-@@ -174,27 +191,26 @@ class TABL(Layer):
-         W: D2 x D2
-         """
-         # first mode projection
--        x = nmodeproduct(x,self.W1,1) # N x d1 x D2
--        # enforcing constant (1) on the diagonal 
--        W = self.W-self.W*K.eye(self.in_shape[2],dtype='float32')+K.eye(self.in_shape[2],dtype='float32')/self.in_shape[2]
--        # calculate attention 
--        attention = Activations.softmax(nmodeproduct(x,W,2),axis=-1) # N x d1 x D2
-+        x = nmodeproduct(x, self.W1, 1)  # N x d1 x D2
-+        # enforcing constant (1) on the diagonal
-+        W = self.W - self.W * K.eye(self.in_shape[2], dtype='float32') + K.eye(
-+            self.in_shape[2], dtype='float32') / self.in_shape[2]
-+        # calculate attention
-+        attention = Activations.softmax(nmodeproduct(x, W, 2),
-+                                        axis=-1)  # N x d1 x D2
-         # apply attention
--        x = self.alpha*x + (1.0 - self.alpha)*x*attention
-+        x = self.alpha * x + (1.0 - self.alpha) * x * attention
-         # second mode projection
--        x = nmodeproduct(x,self.W2,2)
-+        x = nmodeproduct(x, self.W2, 2)
-         # bias add
-         x = x + self.bias
--        
--        if self.output_dim[1]==1:
--            x = K.squeeze(x,axis=-1)
-+
-+        if self.output_dim[1] == 1:
-+            x = K.squeeze(x, axis=-1)
-         return x
- 
-     def compute_output_shape(self, input_shape):
--        if self.output_dim[1]==1:
-+        if self.output_dim[1] == 1:
-             return (input_shape[0], self.output_dim[0])
-         else:
-             return (input_shape[0], self.output_dim[0], self.output_dim[1])
--
--
--
-diff --git a/third_party_libraries/TABL/Models.py b/third_party_libraries/TABL/Models.py
-index e2bf62c..d5c4c45 100644
---- a/third_party_libraries/TABL/Models.py
-+++ b/third_party_libraries/TABL/Models.py
-@@ -4,7 +4,6 @@
- @author: Dat Tran (dat.tranthanh@tut.fi)
- """
- 
--
- from third_party_libraries.TABL import Layers
- import keras
- 
-@@ -74,7 +73,8 @@ def TABL(
- 
-     x = inputs
-     for k in range(1, len(template) - 1):
--        x = Layers.BL(template[k], projection_regularizer, projection_constraint)(x)
-+        x = Layers.BL(template[k], projection_regularizer,
-+                      projection_constraint)(x)
-         x = keras.layers.Activation("relu")(x)
-         x = keras.layers.Dropout(dropout)(x)
- 
-@@ -94,4 +94,3 @@ def TABL(
-     model.compile(optimizer, "categorical_crossentropy", ["acc"])
- 
-     return model
--
-diff --git a/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc
-new file mode 100644
-index 0000000..ff0a753
-Binary files /dev/null and b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc differ
-diff --git a/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc
-new file mode 100644
-index 0000000..a57f1ba
-Binary files /dev/null and b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc differ
-diff --git a/yaml/tabl.yaml b/yaml/tabl.yaml
-index 2cdc7b2..456e287 100644
---- a/yaml/tabl.yaml
-+++ b/yaml/tabl.yaml
-@@ -4,4 +4,14 @@ epochs:
-   value: 100
- batch_size:
-   desc: Size of each mini-batch
--  value: 32
-+  value: 256
-+window_length:
-+  desc: null
-+  value: 200
-+num_features:
-+  desc: null
-+  value: 40
-+dropout:
-+  desc: null
-+  value: 0.1
-+
diff --git a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/media/graph/graph_summary_6f818560.graph.json b/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/media/graph/graph_summary_6f818560.graph.json
deleted file mode 100644
index fcd5b7c..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/media/graph/graph_summary_6f818560.graph.json
+++ /dev/null
@@ -1 +0,0 @@
-{"format": "keras", "nodes": [{"name": "input_1", "id": "input_1", "class_name": "InputLayer", "output_shape": [null, 40, 200], "num_parameters": 0}, {"name": "bl_1", "id": "bl_1", "class_name": "BL", "output_shape": [null, 60, 10], "num_parameters": 5000}, {"name": "activation_1", "id": "activation_1", "class_name": "Activation", "output_shape": [null, 60, 10], "num_parameters": 0}, {"name": "dropout_1", "id": "dropout_1", "class_name": "Dropout", "output_shape": [null, 60, 10], "num_parameters": 0}, {"name": "bl_2", "id": "bl_2", "class_name": "BL", "output_shape": [null, 120, 5], "num_parameters": 7850}, {"name": "activation_2", "id": "activation_2", "class_name": "Activation", "output_shape": [null, 120, 5], "num_parameters": 0}, {"name": "dropout_2", "id": "dropout_2", "class_name": "Dropout", "output_shape": [null, 120, 5], "num_parameters": 0}, {"name": "tabl_1", "id": "tabl_1", "class_name": "TABL", "output_shape": [null, 3], "num_parameters": 394}, {"name": "activation_3", "id": "activation_3", "class_name": "Activation", "output_shape": [null, 3], "num_parameters": 0}], "edges": [["input_1", "bl_1"], ["bl_1", "activation_1"], ["activation_1", "dropout_1"], ["dropout_1", "bl_2"], ["bl_2", "activation_2"], ["activation_2", "dropout_2"], ["dropout_2", "tabl_1"], ["tabl_1", "activation_3"]]}
\ No newline at end of file
diff --git a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/model.h5 b/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/model.h5
deleted file mode 100644
index be2c01d..0000000
Binary files a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/model.h5 and /dev/null differ
diff --git a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/requirements.txt b/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/requirements.txt
deleted file mode 100644
index c4244a7..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/requirements.txt
+++ /dev/null
@@ -1,339 +0,0 @@
-absl-py==0.9.0
-alabaster==0.7.12
-anaconda-client==1.7.2
-anaconda-project==0.8.3
-appdirs==1.4.3
-argparse==1.4.0
-asn1crypto==1.3.0
-astor==0.8.1
-astroid==2.3.3
-astropy==4.0
-asyncore-wsgi==0.0.9
-atomicwrites==1.3.0
-attrs==19.3.0
-autobahn==20.2.1
-autoflake==1.3.1
-automat==0.8.0
-babel==2.8.0
-backcall==0.1.0
-backports.os==0.1.1
-backports.shutil-get-terminal-size==1.0.0
-beautifulsoup4==4.8.2
-bitarray==1.2.1
-bkcharts==0.2
-black==19.10b0
-bleach==3.1.0
-blis==0.4.1
-bokeh==1.4.0
-boto3==1.12.37
-boto==2.49.0
-botocore==1.15.37
-bottle==0.12.18
-bottleneck==1.3.1
-cachetools==4.0.0
-catalogue==1.0.0
-cchardet==2.1.6
-certifi==2019.11.28
-cffi==1.13.2
-chardet==3.0.4
-click==7.0
-cloudpickle==1.2.2
-clyent==1.2.2
-colorama==0.4.3
-configparser==5.0.0
-constantly==15.1.0
-contextlib2==0.6.0.post1
-cryptography==2.8
-cvxpy==1.0.31
-cycler==0.10.0
-cymem==2.0.3
-cython==0.29.14
-cytoolz==0.10.1
-dask==2.10.1
-dateparser==0.7.2
-decorator==4.4.1
-defusedxml==0.6.0
-dill==0.3.1.1
-distributed==2.10.0
-docker-pycreds==0.4.0
-docutils==0.15.2
-easyprocess==0.2.10
-ecos==2.0.7.post1
-entrypoints==0.3
-et-xmlfile==1.0.1
-fastai2==0.0.17
-fastcache==1.1.0
-fastcore==0.1.17
-fastprogress==0.2.2
-filelock==3.0.12
-flake8==3.7.9
-flask==1.1.1
-fsspec==0.6.2
-future==0.18.2
-gast==0.2.2
-gevent==1.4.0
-gitdb==4.0.5
-gitpython==3.1.2
-glob2==0.7
-gmpy2==2.0.8
-google-api-core==1.16.0
-google-api-python-client==1.8.0
-google-auth-httplib2==0.0.3
-google-auth-oauthlib==0.4.1
-google-auth==1.13.1
-google-cloud-profiler==1.0.9
-google-pasta==0.1.8
-googleapis-common-protos==1.51.0
-gql==0.2.0
-graphql-core==1.1
-greenlet==0.4.15
-grpcio==1.27.1
-h5py==2.10.0
-hanging-threads==2.0.5
-heapdict==1.0.1
-html5lib==1.0.1
-httplib2==0.17.2
-hyperlink==19.0.0
-hypothesis==5.4.1
-idna==2.8
-imageio==2.6.1
-imagesize==1.2.0
-imbalanced-learn==0.6.1
-imblearn==0.0
-importlib-metadata==1.5.0
-incremental==17.5.0
-inflect==4.1.0
-ipykernel==5.1.4
-ipython-genutils==0.2.0
-ipython==7.12.0
-ipywidgets==7.5.1
-isort==4.3.21
-itsdangerous==1.1.0
-jaraco.itertools==5.0.0
-jdcal==1.4.1
-jedi==0.16.0
-jeepney==0.4.2
-jieba==0.42.1
-jinja2==2.11.1
-jmespath==0.9.5
-joblib==0.14.1
-json5==0.9.0
-jsonschema==3.2.0
-jupyter-client==5.3.4
-jupyter-console==6.1.0
-jupyter-core==4.6.1
-jupyter==1.0.0
-jupyterlab-server==1.0.6
-jupyterlab==1.2.6
-keras-applications==1.0.8
-keras-bert==0.81.0
-keras-embed-sim==0.7.0
-keras-layer-normalization==0.14.0
-keras-multi-head==0.22.0
-keras-pos-embd==0.11.0
-keras-position-wise-feed-forward==0.6.0
-keras-preprocessing==1.1.0
-keras-self-attention==0.41.0
-keras-transformer==0.32.0
-keras==2.3.1
-keyring==21.1.0
-kiwisolver==1.1.0
-ktrain==0.12.3
-langdetect==1.0.8
-lazy-object-proxy==1.4.3
-libarchive-c==2.8
-lief==0.9.0
-llvmlite==0.31.0
-locket==0.2.0
-lxml==4.5.0
-markdown==3.2
-markupsafe==1.1.1
-matplotlib==3.1.3
-mccabe==0.6.1
-mistune==0.8.4
-mkl-fft==1.0.15
-mkl-random==1.1.0
-mkl-service==2.3.0
-mock==3.0.5
-more-itertools==8.2.0
-mpmath==1.1.0
-msgpack==1.0.0
-multipledispatch==0.6.0
-multiprocess==0.70.9
-murmurhash==1.0.2
-nbconvert==5.6.1
-nbformat==5.0.4
-networkx==2.4
-nltk==3.4.5
-nose==1.3.7
-notebook==6.0.3
-numba==0.48.0
-numexpr==2.7.1
-numpy==1.18.1
-numpydoc==0.9.2
-nvidia-ml-py3==7.352.0
-oauthlib==3.1.0
-olefile==0.46
-opencv-python==4.2.0.34
-openpyxl==3.0.3
-opt-einsum==3.1.0
-osqp==0.6.1
-packaging==20.1
-pandas==1.0.3
-pandocfilters==1.4.2
-parse==1.6.6
-parso==0.6.0
-partd==1.1.0
-path==13.1.0
-pathlib2==2.3.5
-pathspec==0.7.0
-pathtools==0.1.2
-patsy==0.5.1
-pep8==1.7.1
-pexpect==4.8.0
-pickleshare==0.7.5
-pillow==7.0.0
-pip==20.0.2
-pkginfo==1.5.0.1
-plac==1.1.3
-pluggy==0.13.1
-ply==3.11
-preshed==3.0.2
-prometheus-client==0.7.1
-promise==2.3
-prompt-toolkit==3.0.3
-protobuf==3.11.3
-psutil==5.6.7
-psycopg2-binary==2.8.4
-ptyprocess==0.6.0
-py==1.8.1
-pyarrow==0.16.0
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycodestyle==2.5.0
-pycosat==0.6.3
-pycparser==2.19
-pycrypto==2.6.1
-pycurl==7.43.0.5
-pydbus==0.6.0
-pyflakes==2.1.1
-pygments==2.5.2
-pyhamcrest==2.0.0
-pylint==2.4.4
-pyodbc==4.0.28
-pyopenssl==19.1.0
-pyparsing==2.4.6
-pyrsistent==0.15.7
-pysocks==1.7.1
-pytest-arraydiff==0.3
-pytest-astropy-header==0.1.2
-pytest-astropy==0.7.0
-pytest-doctestplus==0.5.0
-pytest-openfiles==0.4.0
-pytest-remotedata==0.3.2
-pytest==5.3.5
-python-dateutil==2.8.1
-pyts==0.10.0
-pytz==2019.3
-pyunpack==0.1.2
-pywavelets==1.1.1
-pyyaml==5.3
-pyzmq==18.1.1
-qtawesome==0.6.1
-qtconsole==4.6.0
-qtpy==1.9.0
-regex==2020.1.8
-requests-oauthlib==1.3.0
-requests==2.22.0
-rope==0.16.0
-rsa==4.0
-ruamel-yaml==0.15.87
-s3transfer==0.3.3
-sacremoses==0.0.38
-scikit-image==0.16.2
-scikit-learn==0.22.1
-scipy==1.4.1
-scoop==0.7.1.1
-scs==2.1.2
-seaborn==0.10.0
-secretstorage==3.1.2
-send2trash==1.5.0
-sentencepiece==0.1.85
-sentry-sdk==0.14.3
-seqeval==0.0.12
-service-identity==18.1.0
-setuptools==45.1.0.post20200127
-shortuuid==1.0.1
-simplegeneric==0.8.1
-singledispatch==3.4.0.3
-six==1.14.0
-smmap==3.0.4
-snowballstemmer==2.0.0
-sortedcollections==1.1.2
-sortedcontainers==2.1.0
-soupsieve==1.9.5
-spacy==2.2.3
-sphinx==2.3.1
-sphinxcontrib-applehelp==1.0.1
-sphinxcontrib-devhelp==1.0.1
-sphinxcontrib-htmlhelp==1.0.2
-sphinxcontrib-jsmath==1.0.1
-sphinxcontrib-qthelp==1.0.2
-sphinxcontrib-serializinghtml==1.1.3
-sphinxcontrib-websupport==1.1.2
-spyder-kernels==0.5.2
-spyder==3.3.6
-sqlalchemy==1.3.13
-srsly==1.0.1
-statsmodels==0.11.0
-subprocess32==3.5.4
-sympy==1.5.1
-syntok==1.2.2
-tables==3.6.1
-tblib==1.6.0
-tensorboard==2.1.1
-tensorflow-datasets==2.1.0
-tensorflow-estimator==2.1.0
-tensorflow-metadata==0.21.1
-tensorflow==2.1.0
-termcolor==1.1.0
-terminado==0.8.3
-testpath==0.4.4
-thinc==7.3.1
-tokenizers==0.5.2
-toml==0.10.0
-toolz==0.10.0
-torch==1.4.0
-torchvision==0.5.0
-tornado==6.0.3
-tqdm==4.42.0
-traitlets==4.3.3
-transformers==2.8.0
-twisted==19.10.0
-txaio==20.1.1
-typed-ast==1.4.1
-tzlocal==2.0.0
-unicodecsv==0.14.1
-uritemplate==3.0.1
-urllib3==1.25.8
-wandb==0.8.35
-wasabi==0.6.0
-watchdog==0.10.2
-wcwidth==0.1.8
-web-pdb==1.5.1
-webencodings==0.5.1
-websocket-client==0.57.0
-weresync==1.1.5
-werkzeug==0.16.1
-wheel==0.34.2
-widgetsnbextension==3.5.1
-wrapt==1.11.2
-wurlitzer==2.0.0
-xlrd==1.2.0
-xlsxwriter==1.2.7
-xlwt==1.3.0
-yapf==0.29.0
-yapsy==1.11.223
-zict==1.0.0
-zipp==2.1.0
-zope.interface==4.7.1
\ No newline at end of file
diff --git a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/wandb-events.jsonl b/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/wandb-events.jsonl
deleted file mode 100644
index c065067..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/wandb-events.jsonl
+++ /dev/null
@@ -1 +0,0 @@
-{"system.cpu": 40.11, "system.memory": 12.08, "system.disk": 23.5, "system.proc.memory.availableMB": 34401.85, "system.proc.memory.rssMB": 737.59, "system.proc.memory.percent": 1.89, "system.proc.cpu.threads": 32.8, "system.network.sent": 215080, "system.network.recv": 245296, "_wandb": true, "_timestamp": 1589165866, "_runtime": 18}
diff --git a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/wandb-history.jsonl b/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/wandb-history.jsonl
deleted file mode 100644
index c9b3d2c..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/wandb-history.jsonl
+++ /dev/null
@@ -1,110 +0,0 @@
-{"_step":28,"loss":"NaN","_runtime":8.446620225906372,"accuracy":0.369140625,"epoch":28,"val_accuracy":0.8984375,"_timestamp":1.5891658255405996e+09,"val_loss":"NaN"}
-{"_step":29,"loss":"NaN","_runtime":8.567692995071411,"accuracy":0.37109375,"epoch":29,"val_accuracy":0.8984375,"_timestamp":1.5891658256616724e+09,"val_loss":"NaN"}
-{"_step":30,"loss":"NaN","_runtime":8.678091526031494,"accuracy":0.37109375,"epoch":30,"val_accuracy":0.80078125,"_timestamp":1.589165825772071e+09,"val_loss":"NaN"}
-{"_step":31,"loss":"NaN","_runtime":8.855355739593506,"accuracy":0.37109375,"epoch":31,"val_accuracy":0.8984375,"_timestamp":1.589165825949335e+09,"val_loss":"NaN"}
-{"_step":32,"loss":"NaN","_runtime":13.265033483505249,"accuracy":0.37109375,"epoch":0,"val_accuracy":0.86328125,"_timestamp":1.5891658386728885e+09,"val_loss":"NaN"}
-{"_step":33,"loss":"NaN","_runtime":13.400344371795654,"accuracy":0.37109375,"epoch":1,"val_accuracy":0.8984375,"_timestamp":1.5891658388081994e+09,"val_loss":"NaN"}
-{"_step":34,"loss":"NaN","_runtime":13.516113042831421,"accuracy":0.37109375,"epoch":2,"val_accuracy":0.8984375,"_timestamp":1.589165838923968e+09,"val_loss":"NaN"}
-{"_step":35,"loss":"NaN","_runtime":13.623668193817139,"accuracy":0.37109375,"epoch":3,"val_accuracy":0.8984375,"_timestamp":1.5891658390315232e+09,"val_loss":"NaN"}
-{"_step":36,"loss":"NaN","_runtime":13.8060884475708,"accuracy":0.37109375,"epoch":4,"val_accuracy":0.8984375,"_timestamp":1.5891658392139435e+09,"val_loss":"NaN"}
-{"_step":37,"loss":"NaN","_runtime":13.926376581192017,"accuracy":0.37109375,"epoch":5,"val_accuracy":0.8984375,"_timestamp":1.5891658393342316e+09,"val_loss":"NaN"}
-{"epoch": 0, "val_loss": NaN, "val_accuracy": 0.85546875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 18.56596040725708, "_timestamp": 1589165852.1848388, "_step": 38}
-{"epoch": 1, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 18.709592819213867, "_timestamp": 1589165852.3284712, "_step": 39}
-{"epoch": 2, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 18.838046073913574, "_timestamp": 1589165852.4569244, "_step": 40}
-{"epoch": 3, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 18.957538604736328, "_timestamp": 1589165852.576417, "_step": 41}
-{"epoch": 4, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 19.13364291191101, "_timestamp": 1589165852.7525213, "_step": 42}
-{"epoch": 5, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 19.259729146957397, "_timestamp": 1589165852.8786075, "_step": 43}
-{"epoch": 6, "val_loss": NaN, "val_accuracy": 0.890625, "loss": NaN, "accuracy": 0.37109375, "_runtime": 19.423646926879883, "_timestamp": 1589165853.0425253, "_step": 44}
-{"epoch": 7, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 19.55221152305603, "_timestamp": 1589165853.17109, "_step": 45}
-{"epoch": 8, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 19.671522855758667, "_timestamp": 1589165853.2904012, "_step": 46}
-{"epoch": 9, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 19.851807594299316, "_timestamp": 1589165853.470686, "_step": 47}
-{"epoch": 10, "val_loss": NaN, "val_accuracy": 0.890625, "loss": NaN, "accuracy": 0.37109375, "_runtime": 19.96437096595764, "_timestamp": 1589165853.5832493, "_step": 48}
-{"epoch": 11, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 20.14822769165039, "_timestamp": 1589165853.767106, "_step": 49}
-{"epoch": 12, "val_loss": NaN, "val_accuracy": 0.66015625, "loss": NaN, "accuracy": 0.37109375, "_runtime": 20.27223801612854, "_timestamp": 1589165853.8911164, "_step": 50}
-{"epoch": 13, "val_loss": NaN, "val_accuracy": 0.890625, "loss": NaN, "accuracy": 0.37109375, "_runtime": 20.381492137908936, "_timestamp": 1589165854.0003705, "_step": 51}
-{"epoch": 14, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 20.568725109100342, "_timestamp": 1589165854.1876035, "_step": 52}
-{"epoch": 15, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 20.681457042694092, "_timestamp": 1589165854.3003354, "_step": 53}
-{"epoch": 16, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 20.856090307235718, "_timestamp": 1589165854.4749687, "_step": 54}
-{"epoch": 17, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 20.983222484588623, "_timestamp": 1589165854.6021008, "_step": 55}
-{"epoch": 18, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 21.099692344665527, "_timestamp": 1589165854.7185707, "_step": 56}
-{"epoch": 19, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 21.277376651763916, "_timestamp": 1589165854.896255, "_step": 57}
-{"epoch": 20, "val_loss": NaN, "val_accuracy": 0.68359375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 21.39777445793152, "_timestamp": 1589165855.0166528, "_step": 58}
-{"epoch": 21, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 21.557739973068237, "_timestamp": 1589165855.1766183, "_step": 59}
-{"epoch": 22, "val_loss": NaN, "val_accuracy": 0.75, "loss": NaN, "accuracy": 0.37109375, "_runtime": 21.698766708374023, "_timestamp": 1589165855.317645, "_step": 60}
-{"epoch": 23, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 21.82015061378479, "_timestamp": 1589165855.439029, "_step": 61}
-{"epoch": 24, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 21.97032332420349, "_timestamp": 1589165855.5892017, "_step": 62}
-{"epoch": 25, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 22.100918769836426, "_timestamp": 1589165855.7197971, "_step": 63}
-{"epoch": 26, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 22.212316751480103, "_timestamp": 1589165855.831195, "_step": 64}
-{"epoch": 27, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 22.378940105438232, "_timestamp": 1589165855.9978185, "_step": 65}
-{"epoch": 28, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 22.50608777999878, "_timestamp": 1589165856.1249661, "_step": 66}
-{"epoch": 29, "val_loss": NaN, "val_accuracy": 0.86328125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 22.612366437911987, "_timestamp": 1589165856.2312448, "_step": 67}
-{"epoch": 30, "val_loss": NaN, "val_accuracy": 0.85546875, "loss": NaN, "accuracy": 0.3671875, "_runtime": 22.784456253051758, "_timestamp": 1589165856.4033346, "_step": 68}
-{"epoch": 31, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 22.912349224090576, "_timestamp": 1589165856.5312276, "_step": 69}
-{"epoch": 32, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 23.03222918510437, "_timestamp": 1589165856.6511075, "_step": 70}
-{"epoch": 33, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 23.21252131462097, "_timestamp": 1589165856.8313997, "_step": 71}
-{"epoch": 34, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 23.32722496986389, "_timestamp": 1589165856.9461033, "_step": 72}
-{"epoch": 35, "val_loss": NaN, "val_accuracy": 0.86328125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 23.50238013267517, "_timestamp": 1589165857.1212585, "_step": 73}
-{"epoch": 36, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 23.654874324798584, "_timestamp": 1589165857.2737527, "_step": 74}
-{"epoch": 37, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 23.77554726600647, "_timestamp": 1589165857.3944256, "_step": 75}
-{"epoch": 38, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 23.93993592262268, "_timestamp": 1589165857.5588143, "_step": 76}
-{"epoch": 39, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 24.049217462539673, "_timestamp": 1589165857.6680958, "_step": 77}
-{"epoch": 40, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 24.22059154510498, "_timestamp": 1589165857.83947, "_step": 78}
-{"epoch": 41, "val_loss": NaN, "val_accuracy": 0.87890625, "loss": NaN, "accuracy": 0.37109375, "_runtime": 24.347931385040283, "_timestamp": 1589165857.9668097, "_step": 79}
-{"epoch": 42, "val_loss": NaN, "val_accuracy": 0.82421875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 24.45352005958557, "_timestamp": 1589165858.0723984, "_step": 80}
-{"epoch": 43, "val_loss": NaN, "val_accuracy": 0.87890625, "loss": NaN, "accuracy": 0.3671875, "_runtime": 24.62743854522705, "_timestamp": 1589165858.246317, "_step": 81}
-{"epoch": 44, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 24.75753617286682, "_timestamp": 1589165858.3764145, "_step": 82}
-{"epoch": 45, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 24.873403072357178, "_timestamp": 1589165858.4922814, "_step": 83}
-{"epoch": 46, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 25.052979707717896, "_timestamp": 1589165858.671858, "_step": 84}
-{"epoch": 47, "val_loss": NaN, "val_accuracy": 0.86328125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 25.1734676361084, "_timestamp": 1589165858.792346, "_step": 85}
-{"epoch": 48, "val_loss": NaN, "val_accuracy": 0.82421875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 25.363243341445923, "_timestamp": 1589165858.9821217, "_step": 86}
-{"epoch": 49, "val_loss": NaN, "val_accuracy": 0.58984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 25.476428031921387, "_timestamp": 1589165859.0953064, "_step": 87}
-{"epoch": 50, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 25.660258054733276, "_timestamp": 1589165859.2791364, "_step": 88}
-{"epoch": 51, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 25.7879900932312, "_timestamp": 1589165859.4068685, "_step": 89}
-{"epoch": 52, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 25.899511575698853, "_timestamp": 1589165859.51839, "_step": 90}
-{"epoch": 53, "val_loss": NaN, "val_accuracy": 0.78125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 26.06508493423462, "_timestamp": 1589165859.6839633, "_step": 91}
-{"epoch": 54, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 26.200456857681274, "_timestamp": 1589165859.8193352, "_step": 92}
-{"epoch": 55, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 26.35785436630249, "_timestamp": 1589165859.9767327, "_step": 93}
-{"epoch": 56, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 26.480684757232666, "_timestamp": 1589165860.0995631, "_step": 94}
-{"epoch": 57, "val_loss": NaN, "val_accuracy": 0.890625, "loss": NaN, "accuracy": 0.369140625, "_runtime": 26.60002112388611, "_timestamp": 1589165860.2188995, "_step": 95}
-{"epoch": 58, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 26.79327917098999, "_timestamp": 1589165860.4121575, "_step": 96}
-{"epoch": 59, "val_loss": NaN, "val_accuracy": 0.89453125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 26.91104507446289, "_timestamp": 1589165860.5299234, "_step": 97}
-{"epoch": 60, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 27.077130794525146, "_timestamp": 1589165860.6960092, "_step": 98}
-{"epoch": 61, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 27.210389614105225, "_timestamp": 1589165860.829268, "_step": 99}
-{"epoch": 62, "val_loss": NaN, "val_accuracy": 0.73828125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 27.32486128807068, "_timestamp": 1589165860.9437397, "_step": 100}
-{"epoch": 63, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.3671875, "_runtime": 27.49658465385437, "_timestamp": 1589165861.115463, "_step": 101}
-{"epoch": 64, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 27.613683223724365, "_timestamp": 1589165861.2325616, "_step": 102}
-{"epoch": 65, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.3671875, "_runtime": 27.733582258224487, "_timestamp": 1589165861.3524606, "_step": 103}
-{"epoch": 66, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 27.92933940887451, "_timestamp": 1589165861.5482178, "_step": 104}
-{"epoch": 67, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 28.047664165496826, "_timestamp": 1589165861.6665425, "_step": 105}
-{"epoch": 68, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 28.21988606452942, "_timestamp": 1589165861.8387644, "_step": 106}
-{"epoch": 69, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 28.34940505027771, "_timestamp": 1589165861.9682834, "_step": 107}
-{"epoch": 70, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 28.46527123451233, "_timestamp": 1589165862.0841496, "_step": 108}
-{"epoch": 71, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 28.639803171157837, "_timestamp": 1589165862.2586815, "_step": 109}
-{"epoch": 72, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 28.75293755531311, "_timestamp": 1589165862.371816, "_step": 110}
-{"epoch": 73, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 28.87182354927063, "_timestamp": 1589165862.490702, "_step": 111}
-{"epoch": 74, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 29.02807307243347, "_timestamp": 1589165862.6469514, "_step": 112}
-{"epoch": 75, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.3671875, "_runtime": 29.156247854232788, "_timestamp": 1589165862.7751262, "_step": 113}
-{"epoch": 76, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 29.267652988433838, "_timestamp": 1589165862.8865314, "_step": 114}
-{"epoch": 77, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 29.43940281867981, "_timestamp": 1589165863.0582812, "_step": 115}
-{"epoch": 78, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 29.558661460876465, "_timestamp": 1589165863.1775398, "_step": 116}
-{"epoch": 79, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 29.69406270980835, "_timestamp": 1589165863.312941, "_step": 117}
-{"epoch": 80, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 29.805070400238037, "_timestamp": 1589165863.4239488, "_step": 118}
-{"epoch": 81, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 29.978322982788086, "_timestamp": 1589165863.5972013, "_step": 119}
-{"epoch": 82, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 30.10282325744629, "_timestamp": 1589165863.7217016, "_step": 120}
-{"epoch": 83, "val_loss": NaN, "val_accuracy": 0.89453125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 30.282686233520508, "_timestamp": 1589165863.9015646, "_step": 121}
-{"epoch": 84, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 30.398473262786865, "_timestamp": 1589165864.0173516, "_step": 122}
-{"epoch": 85, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 30.601835250854492, "_timestamp": 1589165864.2207136, "_step": 123}
-{"epoch": 86, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 30.722200870513916, "_timestamp": 1589165864.3410792, "_step": 124}
-{"epoch": 87, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.3671875, "_runtime": 30.88550591468811, "_timestamp": 1589165864.5043843, "_step": 125}
-{"epoch": 88, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 31.01525568962097, "_timestamp": 1589165864.634134, "_step": 126}
-{"epoch": 89, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 31.135818481445312, "_timestamp": 1589165864.7546968, "_step": 127}
-{"epoch": 90, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 31.317578315734863, "_timestamp": 1589165864.9364567, "_step": 128}
-{"epoch": 91, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 31.435701608657837, "_timestamp": 1589165865.05458, "_step": 129}
-{"epoch": 92, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 31.609537363052368, "_timestamp": 1589165865.2284157, "_step": 130}
-{"epoch": 93, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 31.76369023323059, "_timestamp": 1589165865.3825686, "_step": 131}
-{"epoch": 94, "val_loss": NaN, "val_accuracy": 0.83203125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 31.910706520080566, "_timestamp": 1589165865.529585, "_step": 132}
-{"epoch": 95, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 32.033817529678345, "_timestamp": 1589165865.652696, "_step": 133}
-{"epoch": 96, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 32.1514048576355, "_timestamp": 1589165865.7702832, "_step": 134}
-{"epoch": 97, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 32.32401466369629, "_timestamp": 1589165865.942893, "_step": 135}
-{"epoch": 98, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 32.44492220878601, "_timestamp": 1589165866.0638006, "_step": 136}
-{"epoch": 99, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 32.56118988990784, "_timestamp": 1589165866.1800683, "_step": 137}
diff --git a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/wandb-metadata.json b/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/wandb-metadata.json
deleted file mode 100644
index 76bad7d..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/wandb-metadata.json
+++ /dev/null
@@ -1,23 +0,0 @@
-{
-    "root": "/home/garthtrickett/ProdigyAI",
-    "program": "tabl.py",
-    "git": {
-        "remote": "https://github.com/garthtrickett/ProdigyAI",
-        "commit": "7667fefed2018a0664418c25c97e3c08866a6f71"
-    },
-    "email": "garthtrickett@gmail.com",
-    "startedAt": "2020-05-11T02:57:27.819248",
-    "host": "preemp-cpu-1",
-    "username": "garthtrickett",
-    "executable": "/home/garthtrickett/miniconda3/envs/ProdigyAI/bin/python",
-    "os": "Linux-5.3.0-1018-gcp-x86_64-with-debian-buster-sid",
-    "python": "3.7.6",
-    "cpu_count": 6,
-    "args": [],
-    "state": "finished",
-    "jobType": null,
-    "mode": "run",
-    "project": "prodigyai",
-    "heartbeatAt": "2020-05-11T02:57:46.615253",
-    "exitcode": 0
-}
diff --git a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/wandb-summary.json b/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/wandb-summary.json
deleted file mode 100644
index df3e0c2..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025727-33cb2k5u/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"loss": NaN, "_step": 137, "epoch": 99, "graph": {"_type": "graph-file", "path": "media/graph/graph_summary_6f818560.graph.json", "sha256": "6f81856020e576034c23528bd5c8cc156872ed948db82ecb89f721130ae37682", "size": 1318}, "_runtime": 32.56118988990784, "accuracy": 0.37109375, "val_loss": NaN, "best_loss": 1.622144103050232, "_timestamp": 1589165866.1800683, "best_epoch": 2, "val_accuracy": 0.8984375}
diff --git a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/config.yaml b/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/config.yaml
deleted file mode 100644
index 8c1a8a9..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/config.yaml
+++ /dev/null
@@ -1,34 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.8.35
-    framework: keras
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.7.6
-dataset:
-  desc: null
-  value: esugj36b.h5
-params:
-  desc: null
-  value:
-    batch_size:
-      desc: Size of each mini-batch
-      value: 256
-    dropout:
-      desc: null
-      value: 0.1
-    epochs:
-      desc: Number of epochs to train over
-      value: 100
-    num_features:
-      desc: null
-      value: 40
-    window_length:
-      desc: null
-      value: 200
-yaml:
-  desc: null
-  value: yaml/tabl.yaml
diff --git a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/cp.ckpt b/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/cp.ckpt
deleted file mode 100644
index 8fd2a8d..0000000
Binary files a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/cp.ckpt and /dev/null differ
diff --git a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/diff.patch b/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/diff.patch
deleted file mode 100644
index 214be23..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/diff.patch
+++ /dev/null
@@ -1,1270 +0,0 @@
-diff --git a/pipeline/deeplob.py b/pipeline/deeplob.py
-index 2d6c076..fef42d4 100644
---- a/pipeline/deeplob.py
-+++ b/pipeline/deeplob.py
-@@ -2,6 +2,7 @@
- import pandas as pd
- import pickle
- import numpy as np
-+import tensorflow
- import tensorflow as tf
- from tensorflow.keras import layers
- 
-@@ -25,21 +26,17 @@ from keras.utils import np_utils
- import matplotlib.pyplot as plt
- import math
- from numba import njit, prange
--
--# Init wandb
--import wandb
--from wandb.keras import WandbCallback
--wandb.init(project="prodigyai")
-+import yaml
- 
- # set random seeds
- np.random.seed(1)
- tf.random.set_seed(2)
- 
- import keras
--from keras.callbacks import ModelCheckpoint
--from keras import backend as K
-+from tensorflow.keras.callbacks import ModelCheckpoint
- import h5py
--
-+import wandb
-+from wandb.keras import WandbCallback
- # check if using gpu
- gpus = tf.config.list_physical_devices()
- any_gpus = [s for s in gpus if "GPU" in s[0]]
-@@ -105,6 +102,23 @@ if cwd == home + "/":
-     cwd = cwd + "/ProdigyAI"
-     path_adjust = cwd
- 
-+# Init wandb
-+yaml_path = path_adjust + "yaml/deeplob.yaml"
-+with open(yaml_path) as file:
-+    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
-+
-+config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
-+wandb.init(dir="~/ProdigyAI/",
-+           project="prodigyai",
-+           config=config_dictionary,
-+           resume=True)
-+
-+window_length = wandb.config['params']['window_length']['value']
-+num_features = wandb.config['params']['num_features']['value']
-+epochs = wandb.config['params']['epochs']['value']
-+batch_size = wandb.config['params']['batch_size']['value']
-+number_of_lstm = wandb.config['params']['number_of_lstm']['value']
-+
- # limit gpu usage for keras
- gpu_devices = tf.config.experimental.list_physical_devices("GPU")
- for device in gpu_devices:
-@@ -216,7 +230,8 @@ import h5py
- 
- home = str(Path.home())
- 
--file_name = "arch=DLOB&name=two_model&WL=200&pt=1&sl=1&min_ret=4.347826086956522e-05&vbs=0.5&head=0&skip=0&vol_max=4.3498260869565224e-05&vol_min=4.348826086956522e-05&filter=none&cm_vol_mod=0&sw=on&fd=off&input=obook&ntb=True&tslbc=True.h5"
-+file_name = "esugj36b.h5"
-+wandb.config.update({'dataset': file_name})
- path = home + "/ProdigyAI/data/preprocessed/" + file_name
- h5f = h5py.File(path, "r")
- prices_for_window_index_array_train = h5f[
-@@ -268,6 +283,7 @@ def generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data):
- 
- class DataGenerator(tf.compat.v2.keras.utils.Sequence):
-     def __init__(self,
-+                 checkpoint_path,
-                  prices_for_window_index_array,
-                  input_features_normalized,
-                  y_data,
-@@ -277,6 +293,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
-                  to_fit,
-                  shuffle=True):
-         self.batch_size = batch_size
-+        self.checkpoint_path = checkpoint_path
-         self.prices_for_window_index_array = prices_for_window_index_array
-         self.input_features_normalized = input_features_normalized
-         self.labels = y_data
-@@ -323,6 +340,8 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
-     def on_epoch_end(self):
- 
-+        wandb.save(self.checkpoint_path)
-+
-         self.indexes = np.arange(len(self.prices_for_window_index_array))
- 
-         if self.shuffle:
-@@ -351,8 +370,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
- 
- n_classes = 3
--dim = (200, 40)
--batch_size = 64
-+dim = (window_length, num_features)
- to_fit = True
- 
- # for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,
-@@ -371,70 +389,6 @@ to_fit = True
- 
- ## PRODIGY AI HOCKUS POCKUS END
- 
--
--### Model Architecture
--def create_deeplob(T, NF, number_of_lstm):
--    input_lmd = Input(shape=(T, NF, 1))
--
--    # build the convolutional block
--    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    conv_first1 = Conv2D(32, (1, 10))(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--    conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
--    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
--
--    # build the inception module
--    convsecond_1 = Conv2D(64, (1, 1), padding="same")(conv_first1)
--    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
--    convsecond_1 = Conv2D(64, (3, 1), padding="same")(convsecond_1)
--    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
--
--    convsecond_2 = Conv2D(64, (1, 1), padding="same")(conv_first1)
--    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
--    convsecond_2 = Conv2D(64, (5, 1), padding="same")(convsecond_2)
--    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
--
--    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),
--                                padding="same")(conv_first1)
--    convsecond_3 = Conv2D(64, (1, 1), padding="same")(convsecond_3)
--    convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)
--
--    convsecond_output = concatenate([convsecond_1, convsecond_2, convsecond_3],
--                                    axis=3)
--
--    # use the MC dropout here
--    conv_reshape = Reshape(
--        (int(convsecond_output.shape[1]),
--         int(convsecond_output.shape[3])))(convsecond_output)
--
--    conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)
--
--    # build the output layer
--    out = Dense(3, activation="softmax")(conv_lstm)
--    model = Model(inputs=input_lmd, outputs=out)
--    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
--    model.compile(optimizer=adam,
--                  loss="categorical_crossentropy",
--                  metrics=["accuracy"])
--
--    return model
--
--
- checkpoint_path = path_adjust + "temp/cp.ckpt"
- # Create a callback that saves the model's weights
- cp_callback = ModelCheckpoint(filepath=checkpoint_path,
-@@ -446,9 +400,8 @@ if check_point_file.exists() and resuming == "resuming":
-     print("weights loaded")
-     model.load_weights(checkpoint_path)
- 
--batch_size = 64
--
--train_generator = DataGenerator(prices_for_window_index_array_train,
-+train_generator = DataGenerator(checkpoint_path,
-+                                prices_for_window_index_array_train,
-                                 input_features_normalized_train,
-                                 y_train,
-                                 batch_size,
-@@ -457,7 +410,8 @@ train_generator = DataGenerator(prices_for_window_index_array_train,
-                                 to_fit,
-                                 shuffle=True)
- 
--val_generator = DataGenerator(prices_for_window_index_array_val,
-+val_generator = DataGenerator(checkpoint_path,
-+                              prices_for_window_index_array_val,
-                               input_features_normalized_val,
-                               y_val,
-                               batch_size,
-@@ -469,14 +423,87 @@ val_generator = DataGenerator(prices_for_window_index_array_val,
- steps_per_epoch = len(train_generator)
- validation_steps = len(val_generator)
- 
--deeplob = create_deeplob(200, 40, 64)
--deeplob.fit(train_generator,
--            steps_per_epoch=steps_per_epoch,
--            validation_steps=validation_steps,
--            epochs=100,
--            verbose=2,
--            validation_data=val_generator,
--            callbacks=[cp_callback, WandbCallback()])
-+if wandb.run.resumed:
-+    wandb.restore("model-best.h5",run_path="garthtrickett/prodigyai/" + wandb.run.id)
-+    # restore the best model
-+    deeplob = tensorflow.keras.models.load_model(wandb.restore("model-best.h5").name)
-+    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
-+    deeplob.compile(optimizer=adam,
-+                    loss="categorical_crossentropy",
-+                    metrics=["accuracy"])
-+else:
-+    ### Model Architecture
-+    def create_deeplob(T, NF, number_of_lstm):
-+        input_lmd = Input(shape=(T, NF, 1))
-+
-+        # build the convolutional block
-+        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        conv_first1 = Conv2D(32, (1, 10))(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+        conv_first1 = Conv2D(32, (4, 1), padding="same")(conv_first1)
-+        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)
-+
-+        # build the inception module
-+        convsecond_1 = Conv2D(64, (1, 1), padding="same")(conv_first1)
-+        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
-+        convsecond_1 = Conv2D(64, (3, 1), padding="same")(convsecond_1)
-+        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)
-+
-+        convsecond_2 = Conv2D(64, (1, 1), padding="same")(conv_first1)
-+        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
-+        convsecond_2 = Conv2D(64, (5, 1), padding="same")(convsecond_2)
-+        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)
-+
-+        convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),
-+                                    padding="same")(conv_first1)
-+        convsecond_3 = Conv2D(64, (1, 1), padding="same")(convsecond_3)
-+        convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)
-+
-+        convsecond_output = concatenate(
-+            [convsecond_1, convsecond_2, convsecond_3], axis=3)
-+
-+        # use the MC dropout here
-+        conv_reshape = Reshape(
-+            (int(convsecond_output.shape[1]),
-+             int(convsecond_output.shape[3])))(convsecond_output)
-+
-+        conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)
-+
-+        # build the output layer
-+        out = Dense(3, activation="softmax")(conv_lstm)
-+        model = Model(inputs=input_lmd, outputs=out)
-+        adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)
-+        model.compile(optimizer=adam,
-+                      loss="categorical_crossentropy",
-+                      metrics=["accuracy"])
-+
-+        return model
-+
-+    deeplob = create_deeplob(window_length, num_features, number_of_lstm)
-+deeplob.fit(
-+    train_generator,
-+    steps_per_epoch=steps_per_epoch,
-+    validation_steps=validation_steps,
-+    epochs=100,
-+    verbose=2,
-+    validation_data=val_generator,
-+    callbacks=[cp_callback,
-+               WandbCallback(save_model=True, monitor="loss")])
- 
- finished_weights_path = path_adjust + "temp/cp_end.ckpt"
- deeplob.save_weights(finished_weights_path)
-diff --git a/pipeline/preprocessing.py b/pipeline/preprocessing.py
-index 932ae0b..714d325 100644
---- a/pipeline/preprocessing.py
-+++ b/pipeline/preprocessing.py
-@@ -136,54 +136,67 @@ if stage == 2:
-     side = X_for_all_labels["predicted_bins"]
-     # Could use the probabilities (instead of [1,0,0] use [0.2,0.55,0.25]
- 
-+import yaml
-+import wandb
-+
-+yaml_path = path_adjust + "yaml/preprocessing.yaml"
-+with open(yaml_path) as file:
-+    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
-+
-+config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
-+
-+wandb.init(
-+    dir="~/ProdigyAI/",
-+    project="prodigyai",
-+    config=config_dictionary,
-+)
-+
-+minimum_return = eval(wandb.config['params']['minimum_return']['value'])
-+vertical_barrier_seconds = eval(
-+    wandb.config['params']['vertical_barrier_seconds']['value'])
-+
- # Parameters
- parameters = dict()
--model_arch = "TABL"
--parameters["arch"] = model_arch
--parameters["name"] = model
--parameters["WL"] = 200  # WINDOW LONG
--parameters["pt"] = 1
--parameters["sl"] = 1
--parameters["min_ret"] = 0.001 * 1 / 23
--parameters["vbs"] = round(1 / 2,
--                          3)  # Increasing this decreases vertical touches
--parameters["head"] = 1000  # take only first x number of rows 0 means of
--parameters["skip"] = 0  # sample every n'th row if skip > 0
--# get even classes at fraction = 0.8 so the training set is balanced then set to 1
--# 3 million rows is about the limit of before it starts taking ages / memory maxing
--parameters["vol_max"] = (
--    parameters["min_ret"] + 0.00000002
-+wandb.config['params']['head'][
-+    'value'] = 1000  # take only first x number of rows 0 means of
-+volume_max = (
-+    minimum_return + wandb.config['params']['vol_max_modifier']['value']
- )  # The higher this is the more an increase in volatility requries an increase
- # in return to be considered buy/sell (Increasing this increases end barrier vertical touches)
--parameters["vol_min"] = parameters["min_ret"] + 0.00000001
-+volume_min = minimum_return + wandb.config['params']['vol_min_modifier'][
-+    'value']
- 
--parameters["filter"] = "none"
-+filter_type = wandb.config['params']['filter_type']['value']
- 
--if parameters["filter"] == "cm":
--    parameters["cm_vol_mod"] = 500
-+if filter_type == "cm":
-+    cusum_filter_vol_modifier = wandb.config['params'][
-+        'cusum_filter_volume_modifier']['value']
- else:
--    parameters["cm_vol_mod"] = 0
-+    cusum_filter_vol_modifier = 0
- 
--parameters["sw"] = "on"  # sample weights
--parameters["fd"] = "off"
-+use_sample_weights = wandb.config['params']['use_sample_weights']['value']
-+use_sample_weights = wandb.config['params']['use_fractional_differentiation'][
-+    'value']
- 
--parameters["input"] = "obook"
-+input_type = wandb.config['params']['input_type']['value']
- 
--parameters["ntb"] = True  # non time bars
-+# parameters["ntb"] = True  # non time bars
- 
--if parameters["ntb"] == True:
--    # Pick whether you want to add in the time since last bar input feature
--    # time since last bar column
--    parameters["tslbc"] = True  # time since last bar column
--else:
--    # Pick whether you want to add in the volume input feature
--    parameters["vbc"] = True  # volume bar column
-+# if parameters["ntb"] == True:
-+#     # Pick whether you want to add in the time since last bar input feature
-+#     # time since last bar column
-+#     parameters["tslbc"] = True  # time since last bar column
-+# else:
-+#     # Pick whether you want to add in the volume input feature
-+#     parameters["vbc"] = True  # volume bar column
- 
- # Create the txt file string
--parameter_string = "&".join("{}{}{}".format(key, "=", val)
--                            for key, val in parameters.items())
-+parameter_string = wandb.run.id
- 
--pt_sl = [parameters["pt"], parameters["sl"]]
-+pt_sl = [
-+    wandb.config['params']['profit_taking_multiplier']['value'],
-+    wandb.config['params']['stop_loss_multiplier']['value']
-+]
- cpus = cpu_count() - 1
- 
- regenerate_features_and_labels = True
-@@ -192,10 +205,10 @@ if regenerate_features_and_labels == True:
-     if stage == 1:
-         # Side
-         print("starting data load")
--        head = parameters["head"]
-+        head = wandb.config['params']['head']['value']
- 
-         # # read parquet file of dollar bars
--        if parameters["input"] == "bars":
-+        if input_type == "bars":
-             # Mlfinlab bars
-             data = pq.read_pandas(
-                 path_adjust + "data/bars/"
-@@ -216,7 +229,7 @@ if regenerate_features_and_labels == True:
-             data.index = pd.to_datetime(data.index, infer_datetime_format=True)
- 
-         # read parquet file of raw ticks
--        if parameters["input"] == "ticks":
-+        if input_type == "ticks":
-             data = pq.read_pandas(
-                 path_adjust + "data/bars/" +
-                 "btcusdt_agg_trades_raw_tick_data.parquet").to_pandas()
-@@ -238,11 +251,7 @@ if regenerate_features_and_labels == True:
-             # Should do something else than drop the duplicates (maybe trades doesnt have duplicate indexes rather than aggtrades)
-             data = data.loc[~data.index.duplicated(keep="first")]
- 
--            # skip most rows if this is > 1
--            if parameters["skip"] > 0:
--                data = data.iloc[::parameters["skip"], :]
--
--        if parameters["input"] == "obook":
-+        if input_type == "orderbook":
-             with open(path_adjust + "temp/orderbook_data_name.txt",
-                       "r") as text_file:
-                 orderbook_preprocessed_file_name = text_file.read()
-@@ -275,7 +284,7 @@ if regenerate_features_and_labels == True:
-         # duplicate_fast_search(data.index.duplicated())
- 
-         # Fractional differentiation
--        if parameters["fd"] == "on":
-+        if use_sample_weights == "on":
-             data_series = data["close"].to_frame()
-             # # generate 100 points
-             # nsample = 1000
-@@ -313,16 +322,16 @@ if regenerate_features_and_labels == True:
- 
-         start = time.time()
-         volatility_level_array = volatility_levels_numba(
--            np.ascontiguousarray(data.close.values), parameters["WL"])
-+            np.ascontiguousarray(data.close.values),
-+            wandb.config['params']['window_length']['value'])
-         data["window_volatility_level"] = volatility_level_array
- 
-         # Should adjust the max value
-         # To get more vertical touches we can
-         # either increase vol_max or
-         # decrease the window seconds
--        scaler = MinMaxScaler(
--            feature_range=(parameters["vol_min"],
--                           parameters["vol_max"]))  # normalization
-+        scaler = MinMaxScaler(feature_range=(volume_min,
-+                                             volume_max))  # normalization
- 
-         normed_window_volatility_level = scaler.fit_transform(
-             data[["window_volatility_level"]])
-@@ -338,7 +347,7 @@ if regenerate_features_and_labels == True:
-         close_np_array, close_index_np_array = pandas_series_to_numba_ready_np_arrays(
-             close_copy)
- 
--        volatility_threshold = volatility_threshold * parameters["cm_vol_mod"]
-+        volatility_threshold = volatility_threshold * cusum_filter_vol_modifier
-         print("data_len = " + str(len(data)))
-         start = time.time()
-         sampled_idx = filter_events(
-@@ -346,7 +355,7 @@ if regenerate_features_and_labels == True:
-             close_np_array,
-             close_index_np_array,
-             volatility_threshold,
--            parameters["filter"],
-+            filter_type,
-         )
-         print("sampled_idx_len = " + str(len(sampled_idx)))
-         end = time.time()
-@@ -356,7 +365,8 @@ if regenerate_features_and_labels == True:
-         # size
-         start = time.time()
-         volatility_level_array = volatility_levels_numba(
--            data.close.values, parameters["WL"])
-+            data.close.values,
-+            wandb.config['params']['window_length']['value'])
-         data["window_volatility_level"] = volatility_level_array
- 
-     # This code runs for both first and second stage preprocessing
-@@ -364,7 +374,7 @@ if regenerate_features_and_labels == True:
-     vertical_barrier_timestamps = ml.labeling.add_vertical_barrier(
-         t_events=sampled_idx,
-         close=data["close"],
--        num_seconds=parameters["vbs"])
-+        num_seconds=vertical_barrier_seconds)
-     end = time.time()
-     print("vertical barrier" + str(end - start))
- 
-@@ -376,7 +386,7 @@ if regenerate_features_and_labels == True:
-         t_events=sampled_idx,
-         pt_sl=pt_sl,
-         target=data["window_volatility_level"],
--        min_ret=parameters["min_ret"],
-+        min_ret=minimum_return,
-         num_threads=cpus * 2,
-         vertical_barrier_times=vertical_barrier_timestamps,
-         side_prediction=side,
-@@ -533,26 +543,27 @@ if stage == 1:
-     ### ORDERBOOK VOLUME DATA
-     volumes_for_all_labels = volumes.loc[data.close.index]
- 
--    ## TRADE DATA
--    input_features_trade = []
--    close_array = data.close.values
--    input_features_trade.append(close_array)
-+    # ## TRADE DATA
-+    # input_features_trade = []
-+    # close_array = data.close.values
-+    # input_features_trade.append(close_array)
- 
--    if parameters["ntb"] == False and parameters["vbc"] == True:
--        volume_array = data.volume.values
--        input_features_trade.append(volume_array)
--    if parameters["ntb"] == True and parameters["tslbc"] == True:
--        time_since_last_bar_array = data.time_since_last_bar.values
--        input_features_trade.append(time_since_last_bar_array)
-+    # if parameters["ntb"] == False and parameters["vbc"] == True:
-+    #     volume_array = data.volume.values
-+    #     input_features_trade.append(volume_array)
-+    # if parameters["ntb"] == True and parameters["tslbc"] == True:
-+    #     time_since_last_bar_array = data.time_since_last_bar.values
-+    #     input_features_trade.append(time_since_last_bar_array)
- 
-     end_time = time.time()
-     print(end_time - start_time)
- 
--    # min max limits
-+    # Type of scaling to apply
-+    scaling_type = wandb.config['params']['scaling_type']['value']
- 
--    minimum = -1
--    maximum = 1
--    scaling_type = "z_score"
-+    # min max limits
-+    minimum = wandb.config['params']['scaling_maximum']['value']
-+    maximum = wandb.config['params']['scaling_minimum']['value']
- 
-     ### Split intothe training/validation/test sets
-     print("splitting into train/va/test sets start")
-@@ -573,11 +584,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_train = volumes_for_all_labels.iloc[
-         train_close_array_integer_index[0] -
--        parameters["WL"]:train_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        train_close_array_integer_index[-1] + 2]
- 
-     close_index_array_train = close_index_array[
-         train_close_array_integer_index[0] -
--        parameters["WL"]:train_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        train_close_array_integer_index[-1] + 2]
- 
-     end_time = time.time()
- 
-@@ -623,7 +636,7 @@ if stage == 1:
-     # print("Make window started")
-     # start_time = time.time()
- 
--    # padding = parameters["WL"] * 2
-+    # padding = wandb.config['params']['window_length']['value'] * 2
- 
-     # split_by = 100000
- 
-@@ -652,7 +665,7 @@ if stage == 1:
-     #         len(prices_for_window_index_array_train[start_index:end_index]),
-     #         input_features_normalized_train[:, close_and_input_start_index:
-     #                                         close_and_input_end_index],
--    #         parameters["WL"],
-+    #         wandb.config['params']['window_length']['value'],
-     #         model_arch,
-     #     )
- 
-@@ -697,11 +710,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_val = volumes_for_all_labels.iloc[
-         val_close_array_integer_index[0] -
--        parameters["WL"]:val_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        val_close_array_integer_index[-1] + 2]
- 
-     close_index_array_val = close_index_array[
-         val_close_array_integer_index[0] -
--        parameters["WL"]:val_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        val_close_array_integer_index[-1] + 2]
- 
-     input_features_val = make_input_features_from_orderbook_data(
-         volumes_for_all_labels_val)
-@@ -721,7 +736,7 @@ if stage == 1:
-     #     #     prices_for_window_index_array_val,
-     #     #     close_index_array_val,
-     #     #     input_features_normalized_val,
--    #     #     parameters["WL"],
-+    #     #     wandb.config['params']['window_length']['value'],
-     #     #     model_arch,
-     #     # )
- 
-@@ -736,11 +751,13 @@ if stage == 1:
- 
-     volumes_for_all_labels_test = volumes_for_all_labels.iloc[
-         test_close_array_integer_index[0] -
--        parameters["WL"]:test_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        test_close_array_integer_index[-1] + 2]
- 
-     close_index_array_test = close_index_array[
-         test_close_array_integer_index[0] -
--        parameters["WL"]:test_close_array_integer_index[-1] + 2]
-+        wandb.config['params']['window_length']['value']:
-+        test_close_array_integer_index[-1] + 2]
- 
-     input_features_test = make_input_features_from_orderbook_data(
-         volumes_for_all_labels_test)
-@@ -761,7 +778,7 @@ if stage == 1:
- #     #     prices_for_window_index_array_test,
- #     #     close_index_array_test,
- #     #     input_features_normalized_test,
--#     #     parameters["WL"],
-+#     #     wandb.config['params']['window_length']['value'],
- #     #     model_arch,
- #     # )
- 
-@@ -782,8 +799,9 @@ ax = plt.gca()
- 
- data.iloc[window_index - 200:window_index + 200].plot(y="close",
-                                                       use_index=True)
--plot_window_and_touch_and_label(window_index, parameters["WL"], data,
--                                triple_barrier_events, labels)
-+plot_window_and_touch_and_label(
-+    window_index, wandb.config['params']['window_length']['value'], data,
-+    triple_barrier_events, labels)
- 
- data.iloc[window_index - 10:window_index + 30]
- 
-@@ -837,9 +855,9 @@ print("writing train/val/test to .h5 files finished taking " +
- #     h5f.create_dataset("P", data=P)
- # h5f.create_dataset("y", data=y)
- # h5f.create_dataset("sampled_idx_epoch", data=sampled_idx_epoch)
--# if parameters["sw"] == "on":
-+# if use_sample_weights == "on":
- #     h5f.create_dataset("sample_weights", data=sample_weights)
--# elif parameters["sw"] == "off":
-+# elif use_sample_weights == "off":
- #     h5f.create_dataset("sample_weights", data=np.zeros(1))
- # h5f.close()
- 
-diff --git a/pipeline/tabl.py b/pipeline/tabl.py
-index 87d51f8..f0244ec 100644
---- a/pipeline/tabl.py
-+++ b/pipeline/tabl.py
-@@ -6,7 +6,7 @@ import sys
- 
- import imblearn
- from imblearn.over_sampling import SMOTE
--
-+import tensorflow
- import matplotlib.pyplot as plt
- 
- sys.path.append("..")
-@@ -17,29 +17,26 @@ home = str(Path.home())
- sys.path.append(home + "/ProdigyAI")
- 
- from numba import njit, prange
--import tensorflow as tf
-+
- import math
- 
--import keras
- import numpy as np
- from third_party_libraries.TABL import Models
- from third_party_libraries.keras_lr_finder.lr_finder import LRFinder
- from third_party_libraries.CLR.clr_callbacks import CyclicLR
- 
--from tensorflow.keras.losses import categorical_crossentropy
--from tensorflow.keras.optimizers import SGD, Adam
-+import keras
-+from keras.losses import categorical_crossentropy
-+from keras.optimizers import SGD, Adam
- from keras.callbacks import ModelCheckpoint
- 
-+import tensorflow as tf
- # Init wandb
- import wandb
- from wandb.keras import WandbCallback
- 
- import yaml
- 
--# import tensorflow.keras as keras
--# import tensorflow.keras.layers as layers
--# import tensorflow.keras.backend as K
--
- # Sorting out whether we are using the ipython kernel or not
- try:
-     get_ipython()
-@@ -95,25 +92,24 @@ with open(yaml_path) as file:
-     yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
- 
- config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
--wandb.init(
--    dir="~/ProdigyAI/",
--    project="prodigyai",
--    config=config_dictionary,
--)
--
--# try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes
--
--# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1
--template = [[40, 200], [60, 10], [120, 5], [3, 1]]
--
--# example data
--example_x = np.random.rand(1000, 40, 10)
--np.min(example_x)
--np.max(example_x)
--np.mean(example_x)
--example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)
--template = [[40, 200], [60, 10], [120, 5], [3, 1]]
--# 200(WINDOW LENGTH)
-+wandb.init(dir="~/ProdigyAI/",
-+           project="prodigyai",
-+           config=config_dictionary,
-+           resume=True)
-+
-+window_length = wandb.config['params']['window_length']['value']
-+num_features = wandb.config['params']['num_features']['value']
-+epochs = wandb.config['params']['epochs']['value']
-+batch_size = wandb.config['params']['batch_size']['value']
-+dropout = wandb.config['params']['dropout']['value']
-+
-+# # try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes
-+# # example data
-+# example_x = np.random.rand(1000, 40, 10)
-+# np.min(example_x)
-+# np.max(example_x)
-+# np.mean(example_x)
-+# example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)
- 
- ## PRODIGY AI HOCKUS POCKUS START
- from pathlib import Path
-@@ -121,7 +117,7 @@ import h5py
- 
- home = str(Path.home())
- 
--file_name = "arch=TABL&name=two_model&WL=200&pt=1&sl=1&min_ret=4.347826086956522e-05&vbs=0.5&head=1000&skip=0&vol_max=4.3498260869565224e-05&vol_min=4.348826086956522e-05&filter=none&cm_vol_mod=0&sw=on&fd=off&input=obook&ntb=True&tslbc=True.h5"
-+file_name = "esugj36b.h5"
- wandb.config.update({'dataset': file_name})
- path = home + "/ProdigyAI/data/preprocessed/" + file_name
- h5f = h5py.File(path, "r")
-@@ -265,8 +261,7 @@ class DataGenerator(tf.compat.v2.keras.utils.Sequence):
- 
- 
- n_classes = 3
--dim = (200, 40)
--batch_size = 64
-+dim = (window_length, num_features)
- to_fit = True
- 
- # for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,
-@@ -280,22 +275,15 @@ to_fit = True
- #     import pdb
- #     pdb.set_trace()
- 
-+# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1
-+
-+template = [[num_features, window_length], [60, 10], [120, 5], [3, 1]]
-+
- # get Bilinear model
- projection_regularizer = None
- projection_constraint = keras.constraints.max_norm(3.0, axis=0)
- attention_regularizer = None
- attention_constraint = keras.constraints.max_norm(5.0, axis=1)
--dropout = 0.1
--
--model = Models.TABL(
--    template,
--    dropout,
--    projection_regularizer,
--    projection_constraint,
--    attention_regularizer,
--    attention_constraint,
--)
--model.summary()
- 
- # path = path_adjust + "data/lob_2010/train_and_test_tabl.h5"
- # h5f = h5py.File(path, "r")
-@@ -442,6 +430,18 @@ clr = CyclicLR(base_lr=base_lr,
- 
- optimizer = keras.optimizers.Adam()
- 
-+model = Models.TABL(
-+    template,
-+    dropout,
-+    projection_regularizer,
-+    projection_constraint,
-+    attention_regularizer,
-+    attention_constraint,
-+)
-+model.summary()
-+
-+
-+
- # Compile the model
- model.compile(loss=loss_function, optimizer=optimizer, metrics=["accuracy"])
- # Create a callback that saves the model's weights
-@@ -449,10 +449,15 @@ cp_callback = ModelCheckpoint(filepath=checkpoint_path,
-                               save_weights_only=True,
-                               verbose=1)
- 
--check_point_file = Path(checkpoint_path)
--if check_point_file.exists() and resuming == "resuming":
--    print("weights loaded")
--    model.load_weights(checkpoint_path)
-+if wandb.run.resumed:
-+    weights = wandb.restore("cp.ckpt",
-+                            run_path="garthtrickett/prodigyai/" + wandb.run.id)
-+    model.load_weights(weights.name)
-+
-+# check_point_file = Path(checkpoint_path)
-+# if check_point_file.exists() and resuming == "resuming":
-+#     print("weights loaded")
-+#     model.load_weights(checkpoint_path)
- # Fit data to model
- # history = model.fit(trainX_CNN,
- #                     trainY_CNN,
-@@ -460,9 +465,6 @@ if check_point_file.exists() and resuming == "resuming":
- #                     epochs=20,
- #                     callbacks=[clr, cp_callback])
- 
--finished_weights_path = path_adjust + "temp/cp_end.ckpt"
--model.save_weights(finished_weights_path)
--
- # create class weight
- # class_weight = {0: 1e6 / 300.0, 1: 1e6 / 400.0, 2: 1e6 / 300.0}
- train_generator = DataGenerator(checkpoint_path,
-@@ -489,12 +491,14 @@ steps_per_epoch = len(train_generator)
- validation_steps = len(val_generator)
- 
- # example sata training
--model.fit(train_generator,
--          steps_per_epoch=steps_per_epoch,
--          validation_steps=validation_steps,
--          epochs=1000,
--          validation_data=val_generator,
--          callbacks=[cp_callback, WandbCallback()])
-+model.fit(
-+    train_generator,
-+    steps_per_epoch=steps_per_epoch,
-+    validation_steps=validation_steps,
-+    epochs=epochs,
-+    validation_data=val_generator,
-+    callbacks=[cp_callback,
-+               WandbCallback(save_model=True, monitor="loss")])
- 
- model.save(os.path.join(wandb.run.dir, "model.h5"))
- # no class weight
-@@ -603,16 +607,16 @@ model.save(os.path.join(wandb.run.dir, "model.h5"))
- # X_val = X_val.reshape((X_val.shape[0], 40, 10))
- 
- # train one epoch to find the learning rate
--model.fit(
--    X_train,
--    y_train,
--    validation_data=(X_val, y_val),
--    batch_size=256,
--    epochs=1,
--    shuffle=False,
--)  # no class weight
--
--score = model.evaluate(x=X_test, y=y_test, batch_size=256)
-+# model.fit(
-+#     X_train,
-+#     y_train,
-+#     validation_data=(X_val, y_val),
-+#     batch_size=256,
-+#     epochs=1,
-+#     shuffle=False,
-+# )  # no class weight
-+
-+# score = model.evaluate(x=X_test, y=y_test, batch_size=256)
- 
- # Save model to wandb
- 
-diff --git a/temp/cp.ckpt.data-00000-of-00001 b/temp/cp.ckpt.data-00000-of-00001
-index 56c00e4..3d9600c 100644
-Binary files a/temp/cp.ckpt.data-00000-of-00001 and b/temp/cp.ckpt.data-00000-of-00001 differ
-diff --git a/temp/cp.ckpt.index b/temp/cp.ckpt.index
-index 3770713..d6d3b4b 100644
-Binary files a/temp/cp.ckpt.index and b/temp/cp.ckpt.index differ
-diff --git a/temp/cp_end.ckpt b/temp/cp_end.ckpt
-index c1c1d9e..62e0a67 100644
-Binary files a/temp/cp_end.ckpt and b/temp/cp_end.ckpt differ
-diff --git a/temp/cp_end.ckpt.data-00000-of-00001 b/temp/cp_end.ckpt.data-00000-of-00001
-index 1d572d0..12d3157 100644
-Binary files a/temp/cp_end.ckpt.data-00000-of-00001 and b/temp/cp_end.ckpt.data-00000-of-00001 differ
-diff --git a/temp/cp_end.ckpt.index b/temp/cp_end.ckpt.index
-index 15de09a..0ab67e1 100644
-Binary files a/temp/cp_end.ckpt.index and b/temp/cp_end.ckpt.index differ
-diff --git a/temp/is_script_finished.txt b/temp/is_script_finished.txt
-index 40ca0d3..bec7a08 100644
---- a/temp/is_script_finished.txt
-+++ b/temp/is_script_finished.txt
-@@ -1 +1 @@
--full_script_time64.39192986488342
-\ No newline at end of file
-+full_script_time25.707124948501587
-\ No newline at end of file
-Submodule third_party_libraries/TABL contains modified content
-Submodule third_party_libraries/TABL 7f25314..df3e92d:
-diff --git a/third_party_libraries/TABL/Layers.py b/third_party_libraries/TABL/Layers.py
-index 9020002..b299e57 100644
---- a/third_party_libraries/TABL/Layers.py
-+++ b/third_party_libraries/TABL/Layers.py
-@@ -3,8 +3,9 @@
- """
- @author: Dat Tran (dat.tranthanh@tut.fi)
- """
-+import keras
- from keras import backend as K
--from keras.engine.topology import Layer
-+from keras.layers import Layer
- from keras import activations as Activations
- from keras import initializers as Initializers
- 
-@@ -18,25 +19,25 @@ class Constraint(object):
- 
-     def get_config(self):
-         return {}
--    
-+
-+
- class MinMax(Constraint):
-     """
-     Customized min-max constraint for scalar
-     """
--
-     def __init__(self, min_value=0.0, max_value=10.0):
-         self.min_value = min_value
-         self.max_value = max_value
-+
-     def __call__(self, w):
--        
--        return K.clip(w,self.min_value,self.max_value)
-+
-+        return K.clip(w, self.min_value, self.max_value)
- 
-     def get_config(self):
--        return {'min_value': self.min_value,
--                'max_value': self.max_value}
--        
-+        return {'min_value': self.min_value, 'max_value': self.max_value}
- 
--def nmodeproduct(x,w,mode):
-+
-+def nmodeproduct(x, w, mode):
-     """
-     n-mode product for 2D matrices
-     x: NxHxW
-@@ -45,74 +46,83 @@ def nmodeproduct(x,w,mode):
-     
-     output: NxhxW (mode1) or NxHxw (mode2)
-     """
--    if mode==2:
--        x=K.dot(x,w)
-+    if mode == 2:
-+        x = K.dot(x, w)
-     else:
--        x=K.permute_dimensions(x,(0,2,1))
--        x=K.dot(x,w)
--        x=K.permute_dimensions(x,(0,2,1))
-+        x = K.permute_dimensions(x, (0, 2, 1))
-+        x = K.dot(x, w)
-+        x = K.permute_dimensions(x, (0, 2, 1))
-     return x
- 
-+
- class BL(Layer):
-     """
-     Bilinear Layer
-     """
--    def __init__(self, output_dim,
-+    def __init__(self,
-+                 output_dim,
-                  kernel_regularizer=None,
--                 kernel_constraint=None,**kwargs):
-+                 kernel_constraint=None,
-+                 **kwargs):
-         """
-         output_dim : output dimensions of 2D tensor, should be a list of len 2, e.g. [30,20]
-         kernel_regularizer : keras regularizer object
-         kernel_constraint: keras constraint object
-         """
--        
-+
-         self.output_dim = output_dim
--        self.kernel_regularizer=kernel_regularizer
--        self.kernel_constraint=kernel_constraint
--        
-+        self.kernel_regularizer = kernel_regularizer
-+        self.kernel_constraint = kernel_constraint
-+
-         super(BL, self).__init__(**kwargs)
- 
-     def build(self, input_shape):
--        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
--                                      initializer='he_uniform',
--                                      regularizer=self.kernel_regularizer,
--                                      constraint=self.kernel_constraint,
--                                      trainable=True)
--        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
--                                      initializer='he_uniform',
--                                      regularizer=self.kernel_regularizer,
--                                      constraint=self.kernel_constraint,
--                                      trainable=True)
--
--        self.bias=self.add_weight(name='bias',shape=(self.output_dim[0],self.output_dim[1]),
--                              initializer='zeros',trainable=True)
-+        self.W1 = self.add_weight(name='W1',
-+                                  shape=(input_shape[1], self.output_dim[0]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.kernel_regularizer,
-+                                  constraint=self.kernel_constraint,
-+                                  trainable=True)
-+        self.W2 = self.add_weight(name='W2',
-+                                  shape=(input_shape[2], self.output_dim[1]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.kernel_regularizer,
-+                                  constraint=self.kernel_constraint,
-+                                  trainable=True)
-+
-+        self.bias = self.add_weight(name='bias',
-+                                    shape=(self.output_dim[0],
-+                                           self.output_dim[1]),
-+                                    initializer='zeros',
-+                                    trainable=True)
-         super(BL, self).build(input_shape)
- 
-     def call(self, x):
-         print(K.int_shape(x))
--        x = nmodeproduct(x,self.W1,1)
--        x = nmodeproduct(x,self.W2,2)
--        x = K.bias_add(x,self.bias)
--        
--        if self.output_dim[1]==1:
--            x = K.squeeze(x,axis=-1)
-+        x = nmodeproduct(x, self.W1, 1)
-+        x = nmodeproduct(x, self.W2, 2)
-+        x = K.bias_add(x, self.bias)
-+
-+        if self.output_dim[1] == 1:
-+            x = K.squeeze(x, axis=-1)
-         print(K.int_shape(x))
-         return x
- 
-     def compute_output_shape(self, input_shape):
--        if self.output_dim[1]==1:
-+        if self.output_dim[1] == 1:
-             return (input_shape[0], self.output_dim[0])
-         else:
-             return (input_shape[0], self.output_dim[0], self.output_dim[1])
--        
--        
-+
-+
- class TABL(Layer):
-     """
-     Temporal Attention augmented Bilinear Layer
-     https://arxiv.org/abs/1712.00975
-     
-     """
--    def __init__(self, output_dim,
-+    def __init__(self,
-+                 output_dim,
-                  projection_regularizer=None,
-                  projection_constraint=None,
-                  attention_regularizer=None,
-@@ -125,45 +135,52 @@ class TABL(Layer):
-         attention_regularizer: keras regularizer object for attention matrix
-         attention_constraint: keras constraint object for attention matrix
-         """
--        
-+
-         self.output_dim = output_dim
-         self.projection_regularizer = projection_regularizer
-         self.projection_constraint = projection_constraint
-         self.attention_regularizer = attention_regularizer
-         self.attention_constraint = attention_constraint
--        
-+
-         super(TABL, self).__init__(**kwargs)
- 
-     def build(self, input_shape):
--        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
--                                      initializer='he_uniform',
--                                      regularizer=self.projection_regularizer,
--                                      constraint=self.projection_constraint,
--                                      trainable=True)
--        
--        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
--                                      initializer='he_uniform',
--                                      regularizer=self.projection_regularizer,
--                                      constraint=self.projection_constraint,
--                                      trainable=True)
--        
--        self.W = self.add_weight(name='W',shape=(input_shape[2], input_shape[2]),
--                                      initializer=Initializers.Constant(1.0/input_shape[2]),
--                                      regularizer=self.attention_regularizer,
--                                      constraint=self.attention_constraint,
--                                      trainable=True)
--        
--        self.alpha = self.add_weight(name='alpha',shape=(1,),
--                                      initializer=Initializers.Constant(0.5),
--                                      constraint=MinMax(),
--                                      trainable=True)
--
--
--        self.bias=self.add_weight(name='bias',shape=(1, self.output_dim[0],self.output_dim[1]),
--                              initializer='zeros',trainable=True)
--        
-+        self.W1 = self.add_weight(name='W1',
-+                                  shape=(input_shape[1], self.output_dim[0]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.projection_regularizer,
-+                                  constraint=self.projection_constraint,
-+                                  trainable=True)
-+
-+        self.W2 = self.add_weight(name='W2',
-+                                  shape=(input_shape[2], self.output_dim[1]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.projection_regularizer,
-+                                  constraint=self.projection_constraint,
-+                                  trainable=True)
-+
-+        self.W = self.add_weight(name='W',
-+                                 shape=(input_shape[2], input_shape[2]),
-+                                 initializer=Initializers.Constant(
-+                                     1.0 / input_shape[2]),
-+                                 regularizer=self.attention_regularizer,
-+                                 constraint=self.attention_constraint,
-+                                 trainable=True)
-+
-+        self.alpha = self.add_weight(name='alpha',
-+                                     shape=(1, ),
-+                                     initializer=Initializers.Constant(0.5),
-+                                     constraint=MinMax(),
-+                                     trainable=True)
-+
-+        self.bias = self.add_weight(name='bias',
-+                                    shape=(1, self.output_dim[0],
-+                                           self.output_dim[1]),
-+                                    initializer='zeros',
-+                                    trainable=True)
-+
-         self.in_shape = input_shape
--        
-+
-         super(TABL, self).build(input_shape)
- 
-     def call(self, x):
-@@ -174,27 +191,26 @@ class TABL(Layer):
-         W: D2 x D2
-         """
-         # first mode projection
--        x = nmodeproduct(x,self.W1,1) # N x d1 x D2
--        # enforcing constant (1) on the diagonal 
--        W = self.W-self.W*K.eye(self.in_shape[2],dtype='float32')+K.eye(self.in_shape[2],dtype='float32')/self.in_shape[2]
--        # calculate attention 
--        attention = Activations.softmax(nmodeproduct(x,W,2),axis=-1) # N x d1 x D2
-+        x = nmodeproduct(x, self.W1, 1)  # N x d1 x D2
-+        # enforcing constant (1) on the diagonal
-+        W = self.W - self.W * K.eye(self.in_shape[2], dtype='float32') + K.eye(
-+            self.in_shape[2], dtype='float32') / self.in_shape[2]
-+        # calculate attention
-+        attention = Activations.softmax(nmodeproduct(x, W, 2),
-+                                        axis=-1)  # N x d1 x D2
-         # apply attention
--        x = self.alpha*x + (1.0 - self.alpha)*x*attention
-+        x = self.alpha * x + (1.0 - self.alpha) * x * attention
-         # second mode projection
--        x = nmodeproduct(x,self.W2,2)
-+        x = nmodeproduct(x, self.W2, 2)
-         # bias add
-         x = x + self.bias
--        
--        if self.output_dim[1]==1:
--            x = K.squeeze(x,axis=-1)
-+
-+        if self.output_dim[1] == 1:
-+            x = K.squeeze(x, axis=-1)
-         return x
- 
-     def compute_output_shape(self, input_shape):
--        if self.output_dim[1]==1:
-+        if self.output_dim[1] == 1:
-             return (input_shape[0], self.output_dim[0])
-         else:
-             return (input_shape[0], self.output_dim[0], self.output_dim[1])
--
--
--
-diff --git a/third_party_libraries/TABL/Models.py b/third_party_libraries/TABL/Models.py
-index e2bf62c..d5c4c45 100644
---- a/third_party_libraries/TABL/Models.py
-+++ b/third_party_libraries/TABL/Models.py
-@@ -4,7 +4,6 @@
- @author: Dat Tran (dat.tranthanh@tut.fi)
- """
- 
--
- from third_party_libraries.TABL import Layers
- import keras
- 
-@@ -74,7 +73,8 @@ def TABL(
- 
-     x = inputs
-     for k in range(1, len(template) - 1):
--        x = Layers.BL(template[k], projection_regularizer, projection_constraint)(x)
-+        x = Layers.BL(template[k], projection_regularizer,
-+                      projection_constraint)(x)
-         x = keras.layers.Activation("relu")(x)
-         x = keras.layers.Dropout(dropout)(x)
- 
-@@ -94,4 +94,3 @@ def TABL(
-     model.compile(optimizer, "categorical_crossentropy", ["acc"])
- 
-     return model
--
-diff --git a/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc
-new file mode 100644
-index 0000000..ff0a753
-Binary files /dev/null and b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc differ
-diff --git a/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc
-new file mode 100644
-index 0000000..a57f1ba
-Binary files /dev/null and b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc differ
-diff --git a/yaml/tabl.yaml b/yaml/tabl.yaml
-index 2cdc7b2..456e287 100644
---- a/yaml/tabl.yaml
-+++ b/yaml/tabl.yaml
-@@ -4,4 +4,14 @@ epochs:
-   value: 100
- batch_size:
-   desc: Size of each mini-batch
--  value: 32
-+  value: 256
-+window_length:
-+  desc: null
-+  value: 200
-+num_features:
-+  desc: null
-+  value: 40
-+dropout:
-+  desc: null
-+  value: 0.1
-+
diff --git a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/media/graph/graph_summary_6f818560.graph.json b/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/media/graph/graph_summary_6f818560.graph.json
deleted file mode 100644
index fcd5b7c..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/media/graph/graph_summary_6f818560.graph.json
+++ /dev/null
@@ -1 +0,0 @@
-{"format": "keras", "nodes": [{"name": "input_1", "id": "input_1", "class_name": "InputLayer", "output_shape": [null, 40, 200], "num_parameters": 0}, {"name": "bl_1", "id": "bl_1", "class_name": "BL", "output_shape": [null, 60, 10], "num_parameters": 5000}, {"name": "activation_1", "id": "activation_1", "class_name": "Activation", "output_shape": [null, 60, 10], "num_parameters": 0}, {"name": "dropout_1", "id": "dropout_1", "class_name": "Dropout", "output_shape": [null, 60, 10], "num_parameters": 0}, {"name": "bl_2", "id": "bl_2", "class_name": "BL", "output_shape": [null, 120, 5], "num_parameters": 7850}, {"name": "activation_2", "id": "activation_2", "class_name": "Activation", "output_shape": [null, 120, 5], "num_parameters": 0}, {"name": "dropout_2", "id": "dropout_2", "class_name": "Dropout", "output_shape": [null, 120, 5], "num_parameters": 0}, {"name": "tabl_1", "id": "tabl_1", "class_name": "TABL", "output_shape": [null, 3], "num_parameters": 394}, {"name": "activation_3", "id": "activation_3", "class_name": "Activation", "output_shape": [null, 3], "num_parameters": 0}], "edges": [["input_1", "bl_1"], ["bl_1", "activation_1"], ["activation_1", "dropout_1"], ["dropout_1", "bl_2"], ["bl_2", "activation_2"], ["activation_2", "dropout_2"], ["dropout_2", "tabl_1"], ["tabl_1", "activation_3"]]}
\ No newline at end of file
diff --git a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/model-best.h5 b/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/model-best.h5
deleted file mode 100644
index 376a27e..0000000
Binary files a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/model-best.h5 and /dev/null differ
diff --git a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/requirements.txt b/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/requirements.txt
deleted file mode 100644
index c4244a7..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/requirements.txt
+++ /dev/null
@@ -1,339 +0,0 @@
-absl-py==0.9.0
-alabaster==0.7.12
-anaconda-client==1.7.2
-anaconda-project==0.8.3
-appdirs==1.4.3
-argparse==1.4.0
-asn1crypto==1.3.0
-astor==0.8.1
-astroid==2.3.3
-astropy==4.0
-asyncore-wsgi==0.0.9
-atomicwrites==1.3.0
-attrs==19.3.0
-autobahn==20.2.1
-autoflake==1.3.1
-automat==0.8.0
-babel==2.8.0
-backcall==0.1.0
-backports.os==0.1.1
-backports.shutil-get-terminal-size==1.0.0
-beautifulsoup4==4.8.2
-bitarray==1.2.1
-bkcharts==0.2
-black==19.10b0
-bleach==3.1.0
-blis==0.4.1
-bokeh==1.4.0
-boto3==1.12.37
-boto==2.49.0
-botocore==1.15.37
-bottle==0.12.18
-bottleneck==1.3.1
-cachetools==4.0.0
-catalogue==1.0.0
-cchardet==2.1.6
-certifi==2019.11.28
-cffi==1.13.2
-chardet==3.0.4
-click==7.0
-cloudpickle==1.2.2
-clyent==1.2.2
-colorama==0.4.3
-configparser==5.0.0
-constantly==15.1.0
-contextlib2==0.6.0.post1
-cryptography==2.8
-cvxpy==1.0.31
-cycler==0.10.0
-cymem==2.0.3
-cython==0.29.14
-cytoolz==0.10.1
-dask==2.10.1
-dateparser==0.7.2
-decorator==4.4.1
-defusedxml==0.6.0
-dill==0.3.1.1
-distributed==2.10.0
-docker-pycreds==0.4.0
-docutils==0.15.2
-easyprocess==0.2.10
-ecos==2.0.7.post1
-entrypoints==0.3
-et-xmlfile==1.0.1
-fastai2==0.0.17
-fastcache==1.1.0
-fastcore==0.1.17
-fastprogress==0.2.2
-filelock==3.0.12
-flake8==3.7.9
-flask==1.1.1
-fsspec==0.6.2
-future==0.18.2
-gast==0.2.2
-gevent==1.4.0
-gitdb==4.0.5
-gitpython==3.1.2
-glob2==0.7
-gmpy2==2.0.8
-google-api-core==1.16.0
-google-api-python-client==1.8.0
-google-auth-httplib2==0.0.3
-google-auth-oauthlib==0.4.1
-google-auth==1.13.1
-google-cloud-profiler==1.0.9
-google-pasta==0.1.8
-googleapis-common-protos==1.51.0
-gql==0.2.0
-graphql-core==1.1
-greenlet==0.4.15
-grpcio==1.27.1
-h5py==2.10.0
-hanging-threads==2.0.5
-heapdict==1.0.1
-html5lib==1.0.1
-httplib2==0.17.2
-hyperlink==19.0.0
-hypothesis==5.4.1
-idna==2.8
-imageio==2.6.1
-imagesize==1.2.0
-imbalanced-learn==0.6.1
-imblearn==0.0
-importlib-metadata==1.5.0
-incremental==17.5.0
-inflect==4.1.0
-ipykernel==5.1.4
-ipython-genutils==0.2.0
-ipython==7.12.0
-ipywidgets==7.5.1
-isort==4.3.21
-itsdangerous==1.1.0
-jaraco.itertools==5.0.0
-jdcal==1.4.1
-jedi==0.16.0
-jeepney==0.4.2
-jieba==0.42.1
-jinja2==2.11.1
-jmespath==0.9.5
-joblib==0.14.1
-json5==0.9.0
-jsonschema==3.2.0
-jupyter-client==5.3.4
-jupyter-console==6.1.0
-jupyter-core==4.6.1
-jupyter==1.0.0
-jupyterlab-server==1.0.6
-jupyterlab==1.2.6
-keras-applications==1.0.8
-keras-bert==0.81.0
-keras-embed-sim==0.7.0
-keras-layer-normalization==0.14.0
-keras-multi-head==0.22.0
-keras-pos-embd==0.11.0
-keras-position-wise-feed-forward==0.6.0
-keras-preprocessing==1.1.0
-keras-self-attention==0.41.0
-keras-transformer==0.32.0
-keras==2.3.1
-keyring==21.1.0
-kiwisolver==1.1.0
-ktrain==0.12.3
-langdetect==1.0.8
-lazy-object-proxy==1.4.3
-libarchive-c==2.8
-lief==0.9.0
-llvmlite==0.31.0
-locket==0.2.0
-lxml==4.5.0
-markdown==3.2
-markupsafe==1.1.1
-matplotlib==3.1.3
-mccabe==0.6.1
-mistune==0.8.4
-mkl-fft==1.0.15
-mkl-random==1.1.0
-mkl-service==2.3.0
-mock==3.0.5
-more-itertools==8.2.0
-mpmath==1.1.0
-msgpack==1.0.0
-multipledispatch==0.6.0
-multiprocess==0.70.9
-murmurhash==1.0.2
-nbconvert==5.6.1
-nbformat==5.0.4
-networkx==2.4
-nltk==3.4.5
-nose==1.3.7
-notebook==6.0.3
-numba==0.48.0
-numexpr==2.7.1
-numpy==1.18.1
-numpydoc==0.9.2
-nvidia-ml-py3==7.352.0
-oauthlib==3.1.0
-olefile==0.46
-opencv-python==4.2.0.34
-openpyxl==3.0.3
-opt-einsum==3.1.0
-osqp==0.6.1
-packaging==20.1
-pandas==1.0.3
-pandocfilters==1.4.2
-parse==1.6.6
-parso==0.6.0
-partd==1.1.0
-path==13.1.0
-pathlib2==2.3.5
-pathspec==0.7.0
-pathtools==0.1.2
-patsy==0.5.1
-pep8==1.7.1
-pexpect==4.8.0
-pickleshare==0.7.5
-pillow==7.0.0
-pip==20.0.2
-pkginfo==1.5.0.1
-plac==1.1.3
-pluggy==0.13.1
-ply==3.11
-preshed==3.0.2
-prometheus-client==0.7.1
-promise==2.3
-prompt-toolkit==3.0.3
-protobuf==3.11.3
-psutil==5.6.7
-psycopg2-binary==2.8.4
-ptyprocess==0.6.0
-py==1.8.1
-pyarrow==0.16.0
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycodestyle==2.5.0
-pycosat==0.6.3
-pycparser==2.19
-pycrypto==2.6.1
-pycurl==7.43.0.5
-pydbus==0.6.0
-pyflakes==2.1.1
-pygments==2.5.2
-pyhamcrest==2.0.0
-pylint==2.4.4
-pyodbc==4.0.28
-pyopenssl==19.1.0
-pyparsing==2.4.6
-pyrsistent==0.15.7
-pysocks==1.7.1
-pytest-arraydiff==0.3
-pytest-astropy-header==0.1.2
-pytest-astropy==0.7.0
-pytest-doctestplus==0.5.0
-pytest-openfiles==0.4.0
-pytest-remotedata==0.3.2
-pytest==5.3.5
-python-dateutil==2.8.1
-pyts==0.10.0
-pytz==2019.3
-pyunpack==0.1.2
-pywavelets==1.1.1
-pyyaml==5.3
-pyzmq==18.1.1
-qtawesome==0.6.1
-qtconsole==4.6.0
-qtpy==1.9.0
-regex==2020.1.8
-requests-oauthlib==1.3.0
-requests==2.22.0
-rope==0.16.0
-rsa==4.0
-ruamel-yaml==0.15.87
-s3transfer==0.3.3
-sacremoses==0.0.38
-scikit-image==0.16.2
-scikit-learn==0.22.1
-scipy==1.4.1
-scoop==0.7.1.1
-scs==2.1.2
-seaborn==0.10.0
-secretstorage==3.1.2
-send2trash==1.5.0
-sentencepiece==0.1.85
-sentry-sdk==0.14.3
-seqeval==0.0.12
-service-identity==18.1.0
-setuptools==45.1.0.post20200127
-shortuuid==1.0.1
-simplegeneric==0.8.1
-singledispatch==3.4.0.3
-six==1.14.0
-smmap==3.0.4
-snowballstemmer==2.0.0
-sortedcollections==1.1.2
-sortedcontainers==2.1.0
-soupsieve==1.9.5
-spacy==2.2.3
-sphinx==2.3.1
-sphinxcontrib-applehelp==1.0.1
-sphinxcontrib-devhelp==1.0.1
-sphinxcontrib-htmlhelp==1.0.2
-sphinxcontrib-jsmath==1.0.1
-sphinxcontrib-qthelp==1.0.2
-sphinxcontrib-serializinghtml==1.1.3
-sphinxcontrib-websupport==1.1.2
-spyder-kernels==0.5.2
-spyder==3.3.6
-sqlalchemy==1.3.13
-srsly==1.0.1
-statsmodels==0.11.0
-subprocess32==3.5.4
-sympy==1.5.1
-syntok==1.2.2
-tables==3.6.1
-tblib==1.6.0
-tensorboard==2.1.1
-tensorflow-datasets==2.1.0
-tensorflow-estimator==2.1.0
-tensorflow-metadata==0.21.1
-tensorflow==2.1.0
-termcolor==1.1.0
-terminado==0.8.3
-testpath==0.4.4
-thinc==7.3.1
-tokenizers==0.5.2
-toml==0.10.0
-toolz==0.10.0
-torch==1.4.0
-torchvision==0.5.0
-tornado==6.0.3
-tqdm==4.42.0
-traitlets==4.3.3
-transformers==2.8.0
-twisted==19.10.0
-txaio==20.1.1
-typed-ast==1.4.1
-tzlocal==2.0.0
-unicodecsv==0.14.1
-uritemplate==3.0.1
-urllib3==1.25.8
-wandb==0.8.35
-wasabi==0.6.0
-watchdog==0.10.2
-wcwidth==0.1.8
-web-pdb==1.5.1
-webencodings==0.5.1
-websocket-client==0.57.0
-weresync==1.1.5
-werkzeug==0.16.1
-wheel==0.34.2
-widgetsnbextension==3.5.1
-wrapt==1.11.2
-wurlitzer==2.0.0
-xlrd==1.2.0
-xlsxwriter==1.2.7
-xlwt==1.3.0
-yapf==0.29.0
-yapsy==1.11.223
-zict==1.0.0
-zipp==2.1.0
-zope.interface==4.7.1
\ No newline at end of file
diff --git a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/wandb-events.jsonl b/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/wandb-events.jsonl
deleted file mode 100644
index 8e3c8c0..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/wandb-events.jsonl
+++ /dev/null
@@ -1 +0,0 @@
-{"system.cpu": 37.31, "system.memory": 11.93, "system.disk": 23.5, "system.proc.memory.availableMB": 34459.21, "system.proc.memory.rssMB": 678.28, "system.proc.memory.percent": 1.73, "system.proc.cpu.threads": 32.43, "system.network.sent": 423725, "system.network.recv": 146237, "_wandb": true, "_timestamp": 1589165936, "_runtime": 11}
diff --git a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/wandb-history.jsonl b/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/wandb-history.jsonl
deleted file mode 100644
index 8ddbed6..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/wandb-history.jsonl
+++ /dev/null
@@ -1,56 +0,0 @@
-{"epoch": 0, "val_loss": NaN, "val_accuracy": 0.1328125, "loss": 1.741127073764801, "accuracy": 0.306640625, "_runtime": 3.772897720336914, "_timestamp": 1589165928.7608876, "_step": 0}
-{"epoch": 1, "val_loss": 1.0072065591812134, "val_accuracy": 0.125, "loss": 1.4667609333992004, "accuracy": 0.36328125, "_runtime": 3.911870002746582, "_timestamp": 1589165928.89986, "_step": 1}
-{"epoch": 2, "val_loss": 0.7799588441848755, "val_accuracy": 0.15234375, "loss": 1.3886090517044067, "accuracy": 0.396484375, "_runtime": 4.096334457397461, "_timestamp": 1589165929.0843244, "_step": 2}
-{"epoch": 3, "val_loss": 0.7252475619316101, "val_accuracy": 0.16796875, "loss": 1.2339805364608765, "accuracy": 0.41015625, "_runtime": 4.230475902557373, "_timestamp": 1589165929.2184658, "_step": 3}
-{"epoch": 4, "val_loss": 0.7714945673942566, "val_accuracy": 0.16015625, "loss": 1.1895484924316406, "accuracy": 0.423828125, "_runtime": 4.40803599357605, "_timestamp": 1589165929.396026, "_step": 4}
-{"epoch": 5, "val_loss": 0.8650654554367065, "val_accuracy": 0.109375, "loss": 1.1054646968841553, "accuracy": 0.4453125, "_runtime": 4.544745922088623, "_timestamp": 1589165929.5327358, "_step": 5}
-{"epoch": 6, "val_loss": 0.8921298384666443, "val_accuracy": 0.1015625, "loss": 1.066806137561798, "accuracy": 0.478515625, "_runtime": 4.70844292640686, "_timestamp": 1589165929.6964328, "_step": 6}
-{"epoch": 7, "val_loss": 0.8392581939697266, "val_accuracy": 0.09765625, "loss": 1.005333125591278, "accuracy": 0.5, "_runtime": 4.888227939605713, "_timestamp": 1589165929.8762178, "_step": 7}
-{"epoch": 8, "val_loss": NaN, "val_accuracy": 0.1328125, "loss": 0.96576789021492, "accuracy": 0.546875, "_runtime": 5.020724296569824, "_timestamp": 1589165930.0087142, "_step": 8}
-{"epoch": 9, "val_loss": 0.6791225671768188, "val_accuracy": 0.13671875, "loss": 0.9063300490379333, "accuracy": 0.552734375, "_runtime": 5.205139636993408, "_timestamp": 1589165930.1931295, "_step": 9}
-{"epoch": 10, "val_loss": NaN, "val_accuracy": 0.21875, "loss": 0.934079647064209, "accuracy": 0.55859375, "_runtime": 5.348330497741699, "_timestamp": 1589165930.3363204, "_step": 10}
-{"epoch": 11, "val_loss": 0.6361308097839355, "val_accuracy": 0.15625, "loss": 0.8786826431751251, "accuracy": 0.599609375, "_runtime": 5.50573205947876, "_timestamp": 1589165930.493722, "_step": 11}
-{"epoch": 12, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.466796875, "_runtime": 5.64377760887146, "_timestamp": 1589165930.6317675, "_step": 12}
-{"epoch": 13, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.827475070953369, "_timestamp": 1589165930.815465, "_step": 13}
-{"epoch": 14, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 5.950774669647217, "_timestamp": 1589165930.9387646, "_step": 14}
-{"epoch": 15, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.0610387325286865, "_timestamp": 1589165931.0490286, "_step": 15}
-{"epoch": 16, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.223412752151489, "_timestamp": 1589165931.2114027, "_step": 16}
-{"epoch": 17, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.355444431304932, "_timestamp": 1589165931.3434343, "_step": 17}
-{"epoch": 18, "val_loss": NaN, "val_accuracy": 0.89453125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.456733703613281, "_timestamp": 1589165931.4447236, "_step": 18}
-{"epoch": 19, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.634314060211182, "_timestamp": 1589165931.622304, "_step": 19}
-{"epoch": 20, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.757528066635132, "_timestamp": 1589165931.745518, "_step": 20}
-{"epoch": 21, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 6.882893323898315, "_timestamp": 1589165931.8708832, "_step": 21}
-{"epoch": 22, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.063675880432129, "_timestamp": 1589165932.0516658, "_step": 22}
-{"epoch": 23, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.182437419891357, "_timestamp": 1589165932.1704273, "_step": 23}
-{"epoch": 24, "val_loss": NaN, "val_accuracy": 0.64453125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.372696876525879, "_timestamp": 1589165932.3606868, "_step": 24}
-{"epoch": 25, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.486723184585571, "_timestamp": 1589165932.474713, "_step": 25}
-{"epoch": 26, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.68166708946228, "_timestamp": 1589165932.669657, "_step": 26}
-{"epoch": 27, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.800389528274536, "_timestamp": 1589165932.7883794, "_step": 27}
-{"epoch": 28, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 7.980109930038452, "_timestamp": 1589165932.9680998, "_step": 28}
-{"epoch": 29, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.099136114120483, "_timestamp": 1589165933.087126, "_step": 29}
-{"epoch": 30, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.27849006652832, "_timestamp": 1589165933.26648, "_step": 30}
-{"epoch": 31, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.405267000198364, "_timestamp": 1589165933.393257, "_step": 31}
-{"epoch": 32, "val_loss": NaN, "val_accuracy": 0.64453125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.56814694404602, "_timestamp": 1589165933.5561368, "_step": 32}
-{"epoch": 33, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.716113090515137, "_timestamp": 1589165933.704103, "_step": 33}
-{"epoch": 34, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.828965902328491, "_timestamp": 1589165933.8169558, "_step": 34}
-{"epoch": 35, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 8.991286039352417, "_timestamp": 1589165933.979276, "_step": 35}
-{"epoch": 36, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.113661766052246, "_timestamp": 1589165934.1016517, "_step": 36}
-{"epoch": 37, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.235792636871338, "_timestamp": 1589165934.2237825, "_step": 37}
-{"epoch": 38, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.398523569107056, "_timestamp": 1589165934.3865135, "_step": 38}
-{"epoch": 39, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.511854410171509, "_timestamp": 1589165934.4998443, "_step": 39}
-{"epoch": 40, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.70461392402649, "_timestamp": 1589165934.6926038, "_step": 40}
-{"epoch": 41, "val_loss": NaN, "val_accuracy": 0.6953125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.820807933807373, "_timestamp": 1589165934.8087978, "_step": 41}
-{"epoch": 42, "val_loss": NaN, "val_accuracy": 0.64453125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 9.995427131652832, "_timestamp": 1589165934.983417, "_step": 42}
-{"epoch": 43, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 10.140400648117065, "_timestamp": 1589165935.1283906, "_step": 43}
-{"epoch": 44, "val_loss": NaN, "val_accuracy": 0.8046875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 10.261469841003418, "_timestamp": 1589165935.2494597, "_step": 44}
-{"epoch": 45, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 10.419806003570557, "_timestamp": 1589165935.407796, "_step": 45}
-{"epoch": 46, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 10.545149803161621, "_timestamp": 1589165935.5331397, "_step": 46}
-{"epoch": 47, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 10.66680383682251, "_timestamp": 1589165935.6547937, "_step": 47}
-{"epoch": 48, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 10.839008331298828, "_timestamp": 1589165935.8269982, "_step": 48}
-{"epoch": 49, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 10.968673467636108, "_timestamp": 1589165935.9566634, "_step": 49}
-{"epoch": 50, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 11.129915237426758, "_timestamp": 1589165936.1179051, "_step": 50}
-{"epoch": 51, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 11.249707698822021, "_timestamp": 1589165936.2376976, "_step": 51}
-{"epoch": 52, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 11.42902135848999, "_timestamp": 1589165936.4170113, "_step": 52}
-{"epoch": 53, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 11.553518295288086, "_timestamp": 1589165936.5415082, "_step": 53}
-{"epoch": 54, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 11.67345643043518, "_timestamp": 1589165936.6614463, "_step": 54}
-{"epoch": 55, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 11.843818187713623, "_timestamp": 1589165936.831808, "_step": 55}
diff --git a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/wandb-metadata.json b/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/wandb-metadata.json
deleted file mode 100644
index 1c681ab..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/wandb-metadata.json
+++ /dev/null
@@ -1,22 +0,0 @@
-{
-    "root": "/home/garthtrickett/ProdigyAI",
-    "program": "tabl.py",
-    "git": {
-        "remote": "https://github.com/garthtrickett/ProdigyAI",
-        "commit": "7667fefed2018a0664418c25c97e3c08866a6f71"
-    },
-    "email": "garthtrickett@gmail.com",
-    "startedAt": "2020-05-11T02:58:45.216958",
-    "host": "preemp-cpu-1",
-    "username": "garthtrickett",
-    "executable": "/home/garthtrickett/miniconda3/envs/ProdigyAI/bin/python",
-    "os": "Linux-5.3.0-1018-gcp-x86_64-with-debian-buster-sid",
-    "python": "3.7.6",
-    "cpu_count": 6,
-    "args": [],
-    "state": "running",
-    "jobType": null,
-    "mode": "run",
-    "project": "prodigyai",
-    "heartbeatAt": "2020-05-11T02:58:45.797864"
-}
diff --git a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/wandb-summary.json b/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/wandb-summary.json
deleted file mode 100644
index aff7355..0000000
--- a/~/ProdigyAI/wandb/run-20200511_025845-1jasgwb6/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"graph": {"_type": "graph-file", "path": "media/graph/graph_summary_6f818560.graph.json", "sha256": "6f81856020e576034c23528bd5c8cc156872ed948db82ecb89f721130ae37682", "size": 1318}, "_runtime": 11.843818187713623, "loss": NaN, "val_accuracy": 0.8984375, "_timestamp": 1589165936.831808, "val_loss": NaN, "_step": 55, "accuracy": 0.37109375, "epoch": 55, "best_loss": 0.8786826431751251, "best_epoch": 11}
diff --git a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/config.yaml b/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/config.yaml
deleted file mode 100644
index 8c1a8a9..0000000
--- a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/config.yaml
+++ /dev/null
@@ -1,34 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.8.35
-    framework: keras
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.7.6
-dataset:
-  desc: null
-  value: esugj36b.h5
-params:
-  desc: null
-  value:
-    batch_size:
-      desc: Size of each mini-batch
-      value: 256
-    dropout:
-      desc: null
-      value: 0.1
-    epochs:
-      desc: Number of epochs to train over
-      value: 100
-    num_features:
-      desc: null
-      value: 40
-    window_length:
-      desc: null
-      value: 200
-yaml:
-  desc: null
-  value: yaml/tabl.yaml
diff --git a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/cp.ckpt b/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/cp.ckpt
deleted file mode 100644
index 6692b16..0000000
Binary files a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/cp.ckpt and /dev/null differ
diff --git a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/diff.patch b/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/diff.patch
deleted file mode 100644
index 5156723..0000000
--- a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/diff.patch
+++ /dev/null
@@ -1,422 +0,0 @@
-diff --git a/pipeline/tabl.py b/pipeline/tabl.py
-index f0244ec..b08d227 100644
---- a/pipeline/tabl.py
-+++ b/pipeline/tabl.py
-@@ -90,12 +90,25 @@ if cwd == home + "/":
- yaml_path = path_adjust + "yaml/tabl.yaml"
- with open(yaml_path) as file:
-     yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
--
- config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
-+
-+try:
-+    with open("temp/run_name_and_status.txt", "r") as text_file:
-+        run_name_and_status = text_file.read()
-+    if "finished" not in run_name_and_status:
-+        resume = True
-+    else:
-+        resume = False
-+except:
-+    resume = False
-+
-+import pdb
-+pdb.set_trace()
-+
- wandb.init(dir="~/ProdigyAI/",
-            project="prodigyai",
-            config=config_dictionary,
--           resume=True)
-+           resume=resume)
- 
- window_length = wandb.config['params']['window_length']['value']
- num_features = wandb.config['params']['num_features']['value']
-@@ -440,8 +453,6 @@ model = Models.TABL(
- )
- model.summary()
- 
--
--
- # Compile the model
- model.compile(loss=loss_function, optimizer=optimizer, metrics=["accuracy"])
- # Create a callback that saves the model's weights
-@@ -490,17 +501,24 @@ val_generator = DataGenerator(checkpoint_path,
- steps_per_epoch = len(train_generator)
- validation_steps = len(val_generator)
- 
-+with open(path_adjust + "temp/run_name_and_status.txt", "w+") as text_file:
-+    text_file.write(wandb.run.id)
-+
- # example sata training
- model.fit(
-     train_generator,
-     steps_per_epoch=steps_per_epoch,
-     validation_steps=validation_steps,
--    epochs=epochs,
-+    epochs=10000,
-     validation_data=val_generator,
-     callbacks=[cp_callback,
-                WandbCallback(save_model=True, monitor="loss")])
- 
- model.save(os.path.join(wandb.run.dir, "model.h5"))
-+
-+with open(path_adjust + "temp/run_name_and_status.txt", "w+") as text_file:
-+    text_file.write(wandb.run.id + "___finished")
-+
- # no class weight
- 
- ## lob FI-2010 DATA PREPERATION
-diff --git a/pipeline/~/ProdigyAI/wandb/wandb-resume.json b/pipeline/~/ProdigyAI/wandb/wandb-resume.json
-deleted file mode 100644
-index 6f8568f..0000000
---- a/pipeline/~/ProdigyAI/wandb/wandb-resume.json
-+++ /dev/null
-@@ -1 +0,0 @@
--{"run_id": "3g8o9tjh"}
-\ No newline at end of file
-diff --git a/temp/cp.ckpt b/temp/cp.ckpt
-deleted file mode 100644
-index b98d31b..0000000
-Binary files a/temp/cp.ckpt and /dev/null differ
-diff --git a/temp/cp_end.ckpt b/temp/cp_end.ckpt
-deleted file mode 100644
-index 62e0a67..0000000
-Binary files a/temp/cp_end.ckpt and /dev/null differ
-Submodule third_party_libraries/TABL contains modified content
-Submodule third_party_libraries/TABL df3e92d..c573383:
-diff --git a/third_party_libraries/TABL/Layers.py b/third_party_libraries/TABL/Layers.py
-index 9020002..b299e57 100644
---- a/third_party_libraries/TABL/Layers.py
-+++ b/third_party_libraries/TABL/Layers.py
-@@ -3,8 +3,9 @@
- """
- @author: Dat Tran (dat.tranthanh@tut.fi)
- """
-+import keras
- from keras import backend as K
--from keras.engine.topology import Layer
-+from keras.layers import Layer
- from keras import activations as Activations
- from keras import initializers as Initializers
- 
-@@ -18,25 +19,25 @@ class Constraint(object):
- 
-     def get_config(self):
-         return {}
--    
-+
-+
- class MinMax(Constraint):
-     """
-     Customized min-max constraint for scalar
-     """
--
-     def __init__(self, min_value=0.0, max_value=10.0):
-         self.min_value = min_value
-         self.max_value = max_value
-+
-     def __call__(self, w):
--        
--        return K.clip(w,self.min_value,self.max_value)
-+
-+        return K.clip(w, self.min_value, self.max_value)
- 
-     def get_config(self):
--        return {'min_value': self.min_value,
--                'max_value': self.max_value}
--        
-+        return {'min_value': self.min_value, 'max_value': self.max_value}
- 
--def nmodeproduct(x,w,mode):
-+
-+def nmodeproduct(x, w, mode):
-     """
-     n-mode product for 2D matrices
-     x: NxHxW
-@@ -45,74 +46,83 @@ def nmodeproduct(x,w,mode):
-     
-     output: NxhxW (mode1) or NxHxw (mode2)
-     """
--    if mode==2:
--        x=K.dot(x,w)
-+    if mode == 2:
-+        x = K.dot(x, w)
-     else:
--        x=K.permute_dimensions(x,(0,2,1))
--        x=K.dot(x,w)
--        x=K.permute_dimensions(x,(0,2,1))
-+        x = K.permute_dimensions(x, (0, 2, 1))
-+        x = K.dot(x, w)
-+        x = K.permute_dimensions(x, (0, 2, 1))
-     return x
- 
-+
- class BL(Layer):
-     """
-     Bilinear Layer
-     """
--    def __init__(self, output_dim,
-+    def __init__(self,
-+                 output_dim,
-                  kernel_regularizer=None,
--                 kernel_constraint=None,**kwargs):
-+                 kernel_constraint=None,
-+                 **kwargs):
-         """
-         output_dim : output dimensions of 2D tensor, should be a list of len 2, e.g. [30,20]
-         kernel_regularizer : keras regularizer object
-         kernel_constraint: keras constraint object
-         """
--        
-+
-         self.output_dim = output_dim
--        self.kernel_regularizer=kernel_regularizer
--        self.kernel_constraint=kernel_constraint
--        
-+        self.kernel_regularizer = kernel_regularizer
-+        self.kernel_constraint = kernel_constraint
-+
-         super(BL, self).__init__(**kwargs)
- 
-     def build(self, input_shape):
--        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
--                                      initializer='he_uniform',
--                                      regularizer=self.kernel_regularizer,
--                                      constraint=self.kernel_constraint,
--                                      trainable=True)
--        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
--                                      initializer='he_uniform',
--                                      regularizer=self.kernel_regularizer,
--                                      constraint=self.kernel_constraint,
--                                      trainable=True)
--
--        self.bias=self.add_weight(name='bias',shape=(self.output_dim[0],self.output_dim[1]),
--                              initializer='zeros',trainable=True)
-+        self.W1 = self.add_weight(name='W1',
-+                                  shape=(input_shape[1], self.output_dim[0]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.kernel_regularizer,
-+                                  constraint=self.kernel_constraint,
-+                                  trainable=True)
-+        self.W2 = self.add_weight(name='W2',
-+                                  shape=(input_shape[2], self.output_dim[1]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.kernel_regularizer,
-+                                  constraint=self.kernel_constraint,
-+                                  trainable=True)
-+
-+        self.bias = self.add_weight(name='bias',
-+                                    shape=(self.output_dim[0],
-+                                           self.output_dim[1]),
-+                                    initializer='zeros',
-+                                    trainable=True)
-         super(BL, self).build(input_shape)
- 
-     def call(self, x):
-         print(K.int_shape(x))
--        x = nmodeproduct(x,self.W1,1)
--        x = nmodeproduct(x,self.W2,2)
--        x = K.bias_add(x,self.bias)
--        
--        if self.output_dim[1]==1:
--            x = K.squeeze(x,axis=-1)
-+        x = nmodeproduct(x, self.W1, 1)
-+        x = nmodeproduct(x, self.W2, 2)
-+        x = K.bias_add(x, self.bias)
-+
-+        if self.output_dim[1] == 1:
-+            x = K.squeeze(x, axis=-1)
-         print(K.int_shape(x))
-         return x
- 
-     def compute_output_shape(self, input_shape):
--        if self.output_dim[1]==1:
-+        if self.output_dim[1] == 1:
-             return (input_shape[0], self.output_dim[0])
-         else:
-             return (input_shape[0], self.output_dim[0], self.output_dim[1])
--        
--        
-+
-+
- class TABL(Layer):
-     """
-     Temporal Attention augmented Bilinear Layer
-     https://arxiv.org/abs/1712.00975
-     
-     """
--    def __init__(self, output_dim,
-+    def __init__(self,
-+                 output_dim,
-                  projection_regularizer=None,
-                  projection_constraint=None,
-                  attention_regularizer=None,
-@@ -125,45 +135,52 @@ class TABL(Layer):
-         attention_regularizer: keras regularizer object for attention matrix
-         attention_constraint: keras constraint object for attention matrix
-         """
--        
-+
-         self.output_dim = output_dim
-         self.projection_regularizer = projection_regularizer
-         self.projection_constraint = projection_constraint
-         self.attention_regularizer = attention_regularizer
-         self.attention_constraint = attention_constraint
--        
-+
-         super(TABL, self).__init__(**kwargs)
- 
-     def build(self, input_shape):
--        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
--                                      initializer='he_uniform',
--                                      regularizer=self.projection_regularizer,
--                                      constraint=self.projection_constraint,
--                                      trainable=True)
--        
--        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
--                                      initializer='he_uniform',
--                                      regularizer=self.projection_regularizer,
--                                      constraint=self.projection_constraint,
--                                      trainable=True)
--        
--        self.W = self.add_weight(name='W',shape=(input_shape[2], input_shape[2]),
--                                      initializer=Initializers.Constant(1.0/input_shape[2]),
--                                      regularizer=self.attention_regularizer,
--                                      constraint=self.attention_constraint,
--                                      trainable=True)
--        
--        self.alpha = self.add_weight(name='alpha',shape=(1,),
--                                      initializer=Initializers.Constant(0.5),
--                                      constraint=MinMax(),
--                                      trainable=True)
--
--
--        self.bias=self.add_weight(name='bias',shape=(1, self.output_dim[0],self.output_dim[1]),
--                              initializer='zeros',trainable=True)
--        
-+        self.W1 = self.add_weight(name='W1',
-+                                  shape=(input_shape[1], self.output_dim[0]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.projection_regularizer,
-+                                  constraint=self.projection_constraint,
-+                                  trainable=True)
-+
-+        self.W2 = self.add_weight(name='W2',
-+                                  shape=(input_shape[2], self.output_dim[1]),
-+                                  initializer='he_uniform',
-+                                  regularizer=self.projection_regularizer,
-+                                  constraint=self.projection_constraint,
-+                                  trainable=True)
-+
-+        self.W = self.add_weight(name='W',
-+                                 shape=(input_shape[2], input_shape[2]),
-+                                 initializer=Initializers.Constant(
-+                                     1.0 / input_shape[2]),
-+                                 regularizer=self.attention_regularizer,
-+                                 constraint=self.attention_constraint,
-+                                 trainable=True)
-+
-+        self.alpha = self.add_weight(name='alpha',
-+                                     shape=(1, ),
-+                                     initializer=Initializers.Constant(0.5),
-+                                     constraint=MinMax(),
-+                                     trainable=True)
-+
-+        self.bias = self.add_weight(name='bias',
-+                                    shape=(1, self.output_dim[0],
-+                                           self.output_dim[1]),
-+                                    initializer='zeros',
-+                                    trainable=True)
-+
-         self.in_shape = input_shape
--        
-+
-         super(TABL, self).build(input_shape)
- 
-     def call(self, x):
-@@ -174,27 +191,26 @@ class TABL(Layer):
-         W: D2 x D2
-         """
-         # first mode projection
--        x = nmodeproduct(x,self.W1,1) # N x d1 x D2
--        # enforcing constant (1) on the diagonal 
--        W = self.W-self.W*K.eye(self.in_shape[2],dtype='float32')+K.eye(self.in_shape[2],dtype='float32')/self.in_shape[2]
--        # calculate attention 
--        attention = Activations.softmax(nmodeproduct(x,W,2),axis=-1) # N x d1 x D2
-+        x = nmodeproduct(x, self.W1, 1)  # N x d1 x D2
-+        # enforcing constant (1) on the diagonal
-+        W = self.W - self.W * K.eye(self.in_shape[2], dtype='float32') + K.eye(
-+            self.in_shape[2], dtype='float32') / self.in_shape[2]
-+        # calculate attention
-+        attention = Activations.softmax(nmodeproduct(x, W, 2),
-+                                        axis=-1)  # N x d1 x D2
-         # apply attention
--        x = self.alpha*x + (1.0 - self.alpha)*x*attention
-+        x = self.alpha * x + (1.0 - self.alpha) * x * attention
-         # second mode projection
--        x = nmodeproduct(x,self.W2,2)
-+        x = nmodeproduct(x, self.W2, 2)
-         # bias add
-         x = x + self.bias
--        
--        if self.output_dim[1]==1:
--            x = K.squeeze(x,axis=-1)
-+
-+        if self.output_dim[1] == 1:
-+            x = K.squeeze(x, axis=-1)
-         return x
- 
-     def compute_output_shape(self, input_shape):
--        if self.output_dim[1]==1:
-+        if self.output_dim[1] == 1:
-             return (input_shape[0], self.output_dim[0])
-         else:
-             return (input_shape[0], self.output_dim[0], self.output_dim[1])
--
--
--
-diff --git a/third_party_libraries/TABL/Models.py b/third_party_libraries/TABL/Models.py
-index e2bf62c..d5c4c45 100644
---- a/third_party_libraries/TABL/Models.py
-+++ b/third_party_libraries/TABL/Models.py
-@@ -4,7 +4,6 @@
- @author: Dat Tran (dat.tranthanh@tut.fi)
- """
- 
--
- from third_party_libraries.TABL import Layers
- import keras
- 
-@@ -74,7 +73,8 @@ def TABL(
- 
-     x = inputs
-     for k in range(1, len(template) - 1):
--        x = Layers.BL(template[k], projection_regularizer, projection_constraint)(x)
-+        x = Layers.BL(template[k], projection_regularizer,
-+                      projection_constraint)(x)
-         x = keras.layers.Activation("relu")(x)
-         x = keras.layers.Dropout(dropout)(x)
- 
-@@ -94,4 +94,3 @@ def TABL(
-     model.compile(optimizer, "categorical_crossentropy", ["acc"])
- 
-     return model
--
-diff --git a/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc
-index 12f31e7..ff0a753 100644
-Binary files a/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc and b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc differ
-diff --git a/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc
-index d8950e3..523d214 100644
-Binary files a/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc and b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc differ
-diff --git a/yaml/tabl.yaml b/yaml/tabl.yaml
-index 456e287..a4361e3 100644
---- a/yaml/tabl.yaml
-+++ b/yaml/tabl.yaml
-@@ -1,7 +1,7 @@
- # sample config defaults file
- epochs:
-   desc: Number of epochs to train over
--  value: 100
-+  value: 200
- batch_size:
-   desc: Size of each mini-batch
-   value: 256
diff --git a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/media/graph/graph_summary_6f818560.graph.json b/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/media/graph/graph_summary_6f818560.graph.json
deleted file mode 100644
index fcd5b7c..0000000
--- a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/media/graph/graph_summary_6f818560.graph.json
+++ /dev/null
@@ -1 +0,0 @@
-{"format": "keras", "nodes": [{"name": "input_1", "id": "input_1", "class_name": "InputLayer", "output_shape": [null, 40, 200], "num_parameters": 0}, {"name": "bl_1", "id": "bl_1", "class_name": "BL", "output_shape": [null, 60, 10], "num_parameters": 5000}, {"name": "activation_1", "id": "activation_1", "class_name": "Activation", "output_shape": [null, 60, 10], "num_parameters": 0}, {"name": "dropout_1", "id": "dropout_1", "class_name": "Dropout", "output_shape": [null, 60, 10], "num_parameters": 0}, {"name": "bl_2", "id": "bl_2", "class_name": "BL", "output_shape": [null, 120, 5], "num_parameters": 7850}, {"name": "activation_2", "id": "activation_2", "class_name": "Activation", "output_shape": [null, 120, 5], "num_parameters": 0}, {"name": "dropout_2", "id": "dropout_2", "class_name": "Dropout", "output_shape": [null, 120, 5], "num_parameters": 0}, {"name": "tabl_1", "id": "tabl_1", "class_name": "TABL", "output_shape": [null, 3], "num_parameters": 394}, {"name": "activation_3", "id": "activation_3", "class_name": "Activation", "output_shape": [null, 3], "num_parameters": 0}], "edges": [["input_1", "bl_1"], ["bl_1", "activation_1"], ["activation_1", "dropout_1"], ["dropout_1", "bl_2"], ["bl_2", "activation_2"], ["activation_2", "dropout_2"], ["dropout_2", "tabl_1"], ["tabl_1", "activation_3"]]}
\ No newline at end of file
diff --git a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/requirements.txt b/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/requirements.txt
deleted file mode 100644
index c4244a7..0000000
--- a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/requirements.txt
+++ /dev/null
@@ -1,339 +0,0 @@
-absl-py==0.9.0
-alabaster==0.7.12
-anaconda-client==1.7.2
-anaconda-project==0.8.3
-appdirs==1.4.3
-argparse==1.4.0
-asn1crypto==1.3.0
-astor==0.8.1
-astroid==2.3.3
-astropy==4.0
-asyncore-wsgi==0.0.9
-atomicwrites==1.3.0
-attrs==19.3.0
-autobahn==20.2.1
-autoflake==1.3.1
-automat==0.8.0
-babel==2.8.0
-backcall==0.1.0
-backports.os==0.1.1
-backports.shutil-get-terminal-size==1.0.0
-beautifulsoup4==4.8.2
-bitarray==1.2.1
-bkcharts==0.2
-black==19.10b0
-bleach==3.1.0
-blis==0.4.1
-bokeh==1.4.0
-boto3==1.12.37
-boto==2.49.0
-botocore==1.15.37
-bottle==0.12.18
-bottleneck==1.3.1
-cachetools==4.0.0
-catalogue==1.0.0
-cchardet==2.1.6
-certifi==2019.11.28
-cffi==1.13.2
-chardet==3.0.4
-click==7.0
-cloudpickle==1.2.2
-clyent==1.2.2
-colorama==0.4.3
-configparser==5.0.0
-constantly==15.1.0
-contextlib2==0.6.0.post1
-cryptography==2.8
-cvxpy==1.0.31
-cycler==0.10.0
-cymem==2.0.3
-cython==0.29.14
-cytoolz==0.10.1
-dask==2.10.1
-dateparser==0.7.2
-decorator==4.4.1
-defusedxml==0.6.0
-dill==0.3.1.1
-distributed==2.10.0
-docker-pycreds==0.4.0
-docutils==0.15.2
-easyprocess==0.2.10
-ecos==2.0.7.post1
-entrypoints==0.3
-et-xmlfile==1.0.1
-fastai2==0.0.17
-fastcache==1.1.0
-fastcore==0.1.17
-fastprogress==0.2.2
-filelock==3.0.12
-flake8==3.7.9
-flask==1.1.1
-fsspec==0.6.2
-future==0.18.2
-gast==0.2.2
-gevent==1.4.0
-gitdb==4.0.5
-gitpython==3.1.2
-glob2==0.7
-gmpy2==2.0.8
-google-api-core==1.16.0
-google-api-python-client==1.8.0
-google-auth-httplib2==0.0.3
-google-auth-oauthlib==0.4.1
-google-auth==1.13.1
-google-cloud-profiler==1.0.9
-google-pasta==0.1.8
-googleapis-common-protos==1.51.0
-gql==0.2.0
-graphql-core==1.1
-greenlet==0.4.15
-grpcio==1.27.1
-h5py==2.10.0
-hanging-threads==2.0.5
-heapdict==1.0.1
-html5lib==1.0.1
-httplib2==0.17.2
-hyperlink==19.0.0
-hypothesis==5.4.1
-idna==2.8
-imageio==2.6.1
-imagesize==1.2.0
-imbalanced-learn==0.6.1
-imblearn==0.0
-importlib-metadata==1.5.0
-incremental==17.5.0
-inflect==4.1.0
-ipykernel==5.1.4
-ipython-genutils==0.2.0
-ipython==7.12.0
-ipywidgets==7.5.1
-isort==4.3.21
-itsdangerous==1.1.0
-jaraco.itertools==5.0.0
-jdcal==1.4.1
-jedi==0.16.0
-jeepney==0.4.2
-jieba==0.42.1
-jinja2==2.11.1
-jmespath==0.9.5
-joblib==0.14.1
-json5==0.9.0
-jsonschema==3.2.0
-jupyter-client==5.3.4
-jupyter-console==6.1.0
-jupyter-core==4.6.1
-jupyter==1.0.0
-jupyterlab-server==1.0.6
-jupyterlab==1.2.6
-keras-applications==1.0.8
-keras-bert==0.81.0
-keras-embed-sim==0.7.0
-keras-layer-normalization==0.14.0
-keras-multi-head==0.22.0
-keras-pos-embd==0.11.0
-keras-position-wise-feed-forward==0.6.0
-keras-preprocessing==1.1.0
-keras-self-attention==0.41.0
-keras-transformer==0.32.0
-keras==2.3.1
-keyring==21.1.0
-kiwisolver==1.1.0
-ktrain==0.12.3
-langdetect==1.0.8
-lazy-object-proxy==1.4.3
-libarchive-c==2.8
-lief==0.9.0
-llvmlite==0.31.0
-locket==0.2.0
-lxml==4.5.0
-markdown==3.2
-markupsafe==1.1.1
-matplotlib==3.1.3
-mccabe==0.6.1
-mistune==0.8.4
-mkl-fft==1.0.15
-mkl-random==1.1.0
-mkl-service==2.3.0
-mock==3.0.5
-more-itertools==8.2.0
-mpmath==1.1.0
-msgpack==1.0.0
-multipledispatch==0.6.0
-multiprocess==0.70.9
-murmurhash==1.0.2
-nbconvert==5.6.1
-nbformat==5.0.4
-networkx==2.4
-nltk==3.4.5
-nose==1.3.7
-notebook==6.0.3
-numba==0.48.0
-numexpr==2.7.1
-numpy==1.18.1
-numpydoc==0.9.2
-nvidia-ml-py3==7.352.0
-oauthlib==3.1.0
-olefile==0.46
-opencv-python==4.2.0.34
-openpyxl==3.0.3
-opt-einsum==3.1.0
-osqp==0.6.1
-packaging==20.1
-pandas==1.0.3
-pandocfilters==1.4.2
-parse==1.6.6
-parso==0.6.0
-partd==1.1.0
-path==13.1.0
-pathlib2==2.3.5
-pathspec==0.7.0
-pathtools==0.1.2
-patsy==0.5.1
-pep8==1.7.1
-pexpect==4.8.0
-pickleshare==0.7.5
-pillow==7.0.0
-pip==20.0.2
-pkginfo==1.5.0.1
-plac==1.1.3
-pluggy==0.13.1
-ply==3.11
-preshed==3.0.2
-prometheus-client==0.7.1
-promise==2.3
-prompt-toolkit==3.0.3
-protobuf==3.11.3
-psutil==5.6.7
-psycopg2-binary==2.8.4
-ptyprocess==0.6.0
-py==1.8.1
-pyarrow==0.16.0
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycodestyle==2.5.0
-pycosat==0.6.3
-pycparser==2.19
-pycrypto==2.6.1
-pycurl==7.43.0.5
-pydbus==0.6.0
-pyflakes==2.1.1
-pygments==2.5.2
-pyhamcrest==2.0.0
-pylint==2.4.4
-pyodbc==4.0.28
-pyopenssl==19.1.0
-pyparsing==2.4.6
-pyrsistent==0.15.7
-pysocks==1.7.1
-pytest-arraydiff==0.3
-pytest-astropy-header==0.1.2
-pytest-astropy==0.7.0
-pytest-doctestplus==0.5.0
-pytest-openfiles==0.4.0
-pytest-remotedata==0.3.2
-pytest==5.3.5
-python-dateutil==2.8.1
-pyts==0.10.0
-pytz==2019.3
-pyunpack==0.1.2
-pywavelets==1.1.1
-pyyaml==5.3
-pyzmq==18.1.1
-qtawesome==0.6.1
-qtconsole==4.6.0
-qtpy==1.9.0
-regex==2020.1.8
-requests-oauthlib==1.3.0
-requests==2.22.0
-rope==0.16.0
-rsa==4.0
-ruamel-yaml==0.15.87
-s3transfer==0.3.3
-sacremoses==0.0.38
-scikit-image==0.16.2
-scikit-learn==0.22.1
-scipy==1.4.1
-scoop==0.7.1.1
-scs==2.1.2
-seaborn==0.10.0
-secretstorage==3.1.2
-send2trash==1.5.0
-sentencepiece==0.1.85
-sentry-sdk==0.14.3
-seqeval==0.0.12
-service-identity==18.1.0
-setuptools==45.1.0.post20200127
-shortuuid==1.0.1
-simplegeneric==0.8.1
-singledispatch==3.4.0.3
-six==1.14.0
-smmap==3.0.4
-snowballstemmer==2.0.0
-sortedcollections==1.1.2
-sortedcontainers==2.1.0
-soupsieve==1.9.5
-spacy==2.2.3
-sphinx==2.3.1
-sphinxcontrib-applehelp==1.0.1
-sphinxcontrib-devhelp==1.0.1
-sphinxcontrib-htmlhelp==1.0.2
-sphinxcontrib-jsmath==1.0.1
-sphinxcontrib-qthelp==1.0.2
-sphinxcontrib-serializinghtml==1.1.3
-sphinxcontrib-websupport==1.1.2
-spyder-kernels==0.5.2
-spyder==3.3.6
-sqlalchemy==1.3.13
-srsly==1.0.1
-statsmodels==0.11.0
-subprocess32==3.5.4
-sympy==1.5.1
-syntok==1.2.2
-tables==3.6.1
-tblib==1.6.0
-tensorboard==2.1.1
-tensorflow-datasets==2.1.0
-tensorflow-estimator==2.1.0
-tensorflow-metadata==0.21.1
-tensorflow==2.1.0
-termcolor==1.1.0
-terminado==0.8.3
-testpath==0.4.4
-thinc==7.3.1
-tokenizers==0.5.2
-toml==0.10.0
-toolz==0.10.0
-torch==1.4.0
-torchvision==0.5.0
-tornado==6.0.3
-tqdm==4.42.0
-traitlets==4.3.3
-transformers==2.8.0
-twisted==19.10.0
-txaio==20.1.1
-typed-ast==1.4.1
-tzlocal==2.0.0
-unicodecsv==0.14.1
-uritemplate==3.0.1
-urllib3==1.25.8
-wandb==0.8.35
-wasabi==0.6.0
-watchdog==0.10.2
-wcwidth==0.1.8
-web-pdb==1.5.1
-webencodings==0.5.1
-websocket-client==0.57.0
-weresync==1.1.5
-werkzeug==0.16.1
-wheel==0.34.2
-widgetsnbextension==3.5.1
-wrapt==1.11.2
-wurlitzer==2.0.0
-xlrd==1.2.0
-xlsxwriter==1.2.7
-xlwt==1.3.0
-yapf==0.29.0
-yapsy==1.11.223
-zict==1.0.0
-zipp==2.1.0
-zope.interface==4.7.1
\ No newline at end of file
diff --git a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/wandb-events.jsonl b/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/wandb-events.jsonl
deleted file mode 100644
index be320f2..0000000
--- a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/wandb-events.jsonl
+++ /dev/null
@@ -1,12 +0,0 @@
-{"system.network.sent":1962018,"system.gpu.0.powerPercent":13.33,"system.network.recv":248355,"system.gpu.process.0.memory":1.12,"system.disk":55,"system.gpu.process.0.powerPercent":13.4,"system.gpu.0.powerWatts":39.99,"system.gpu.process.0.memoryAllocated":99.69,"_wandb":true,"system.gpu.0.temp":36,"system.gpu.process.0.temp":36,"system.gpu.0.memory":1.11,"system.gpu.0.gpu":9.78,"_runtime":33,"system.gpu.process.0.gpu":10.38,"system.proc.memory.rssMB":1400.29,"system.gpu.process.0.powerWatts":40.21,"system.proc.memory.availableMB":35508.92,"system.cpu":25.1,"system.proc.cpu.threads":37.67,"system.memory":9.26,"system.proc.memory.percent":3.58,"system.gpu.0.memoryAllocated":99.31,"_timestamp":1589174230}
-{"system.network.sent":199475,"system.gpu.0.powerPercent":13.19,"system.network.recv":216874,"system.gpu.process.0.memory":1.38,"system.disk":55,"system.gpu.process.0.powerPercent":13.24,"system.gpu.0.powerWatts":39.57,"system.gpu.process.0.memoryAllocated":99.69,"_wandb":true,"system.gpu.0.temp":36,"system.gpu.process.0.temp":36,"system.gpu.0.memory":1.33,"system.gpu.0.gpu":10,"_runtime":48,"system.gpu.process.0.gpu":10.5,"system.proc.memory.rssMB":1471.94,"system.gpu.process.0.powerWatts":39.73,"system.proc.memory.availableMB":35430.36,"system.cpu":24.32,"system.proc.cpu.threads":38.11,"system.memory":9.42,"system.proc.memory.percent":3.76,"system.gpu.0.memoryAllocated":99.31,"_timestamp":1589174320}
-{"system.network.sent":1965067,"system.gpu.0.powerPercent":13.18,"system.network.recv":250217,"system.gpu.process.0.memory":1.25,"system.disk":55,"system.gpu.process.0.powerPercent":13.26,"system.gpu.0.powerWatts":39.55,"system.gpu.process.0.memoryAllocated":99.69,"_wandb":true,"system.gpu.0.temp":36,"system.gpu.process.0.temp":36,"system.gpu.0.memory":1.22,"system.gpu.0.gpu":10.22,"_runtime":63,"system.gpu.process.0.gpu":10.62,"system.proc.memory.rssMB":1398.04,"system.gpu.process.0.powerWatts":39.77,"system.proc.memory.availableMB":35504.23,"system.cpu":25.17,"system.proc.cpu.threads":38.56,"system.memory":9.26,"system.proc.memory.percent":3.57,"system.gpu.0.memoryAllocated":99.31,"_timestamp":1589174385}
-{"system.network.sent":1837976,"system.gpu.0.powerPercent":12.09,"system.network.recv":237091,"system.gpu.process.0.memory":0,"system.disk":55,"system.gpu.process.0.powerPercent":12.09,"system.gpu.0.powerWatts":36.28,"system.gpu.process.0.memoryAllocated":95.86,"_wandb":true,"system.gpu.0.temp":36,"system.gpu.process.0.temp":36,"system.gpu.0.memory":0,"system.gpu.0.gpu":0.13,"_runtime":92,"system.gpu.process.0.gpu":0.14,"system.proc.memory.rssMB":1462.31,"system.gpu.process.0.powerWatts":36.28,"system.proc.memory.availableMB":37233.25,"system.cpu":2.31,"system.proc.cpu.threads":27.13,"system.memory":4.85,"system.proc.memory.percent":3.74,"system.gpu.0.memoryAllocated":89.47,"_timestamp":1589174558}
-{"system.network.sent":1857379,"system.gpu.0.powerPercent":12.12,"system.network.recv":272245,"system.gpu.process.0.memory":0,"system.disk":55,"system.gpu.process.0.powerPercent":12.12,"system.gpu.0.powerWatts":36.35,"system.gpu.process.0.memoryAllocated":95.86,"_wandb":true,"system.gpu.0.temp":36,"system.gpu.process.0.temp":36,"system.gpu.0.memory":0,"system.gpu.0.gpu":0,"_runtime":122,"system.gpu.process.0.gpu":0,"system.proc.memory.rssMB":1537.98,"system.gpu.process.0.powerWatts":36.35,"system.proc.memory.availableMB":37171.73,"system.cpu":0.18,"system.proc.cpu.threads":29,"system.memory":5,"system.proc.memory.percent":3.93,"system.gpu.0.memoryAllocated":95.86,"_timestamp":1589174588}
-{"system.network.sent":1871424,"system.gpu.0.powerPercent":12.1,"system.network.recv":292500,"system.gpu.process.0.memory":0,"system.disk":55,"system.gpu.process.0.powerPercent":12.1,"system.gpu.0.powerWatts":36.29,"system.gpu.process.0.memoryAllocated":95.86,"_wandb":true,"system.gpu.0.temp":36,"system.gpu.process.0.temp":36,"system.gpu.0.memory":0,"system.gpu.0.gpu":0,"_runtime":152,"system.gpu.process.0.gpu":0,"system.proc.memory.rssMB":1537.98,"system.gpu.process.0.powerWatts":36.29,"system.proc.memory.availableMB":37170.86,"system.cpu":0.16,"system.proc.cpu.threads":29,"system.memory":5,"system.proc.memory.percent":3.93,"system.gpu.0.memoryAllocated":95.86,"_timestamp":1589174618}
-{"system.network.sent":1878718,"system.gpu.0.powerPercent":12.09,"system.network.recv":304101,"system.gpu.process.0.memory":0,"system.disk":55,"system.gpu.process.0.powerPercent":12.09,"system.gpu.0.powerWatts":36.26,"system.gpu.process.0.memoryAllocated":95.86,"_wandb":true,"system.gpu.0.temp":36,"system.gpu.process.0.temp":36,"system.gpu.0.memory":0,"system.gpu.0.gpu":0,"_runtime":182,"system.gpu.process.0.gpu":0,"system.proc.memory.rssMB":1537.98,"system.gpu.process.0.powerWatts":36.26,"system.proc.memory.availableMB":37170.86,"system.cpu":0.13,"system.proc.cpu.threads":29,"system.memory":5,"system.proc.memory.percent":3.93,"system.gpu.0.memoryAllocated":95.86,"_timestamp":1589174649}
-{"system.network.sent":1888725,"system.gpu.0.powerPercent":12.1,"system.network.recv":324699,"system.gpu.process.0.memory":0,"system.disk":55,"system.gpu.process.0.powerPercent":12.1,"system.gpu.0.powerWatts":36.29,"system.gpu.process.0.memoryAllocated":95.86,"_wandb":true,"system.gpu.0.temp":36,"system.gpu.process.0.temp":36,"system.gpu.0.memory":0,"system.gpu.0.gpu":0,"_runtime":213,"system.gpu.process.0.gpu":0,"system.proc.memory.rssMB":1537.98,"system.gpu.process.0.powerWatts":36.29,"system.proc.memory.availableMB":37170.1,"system.cpu":0.17,"system.proc.cpu.threads":29,"system.memory":5,"system.proc.memory.percent":3.93,"system.gpu.0.memoryAllocated":95.86,"_timestamp":1589174679}
-{"system.network.sent":1894390,"system.gpu.0.powerPercent":12.1,"system.network.recv":335392,"system.gpu.process.0.memory":0,"system.disk":55,"system.gpu.process.0.powerPercent":12.1,"system.gpu.0.powerWatts":36.29,"system.gpu.process.0.memoryAllocated":95.86,"_wandb":true,"system.gpu.0.temp":36,"system.gpu.process.0.temp":36,"system.gpu.0.memory":0,"system.gpu.0.gpu":0,"_runtime":243,"system.gpu.process.0.gpu":0,"system.proc.memory.rssMB":1537.98,"system.gpu.process.0.powerWatts":36.29,"system.proc.memory.availableMB":37169.72,"system.cpu":0.13,"system.proc.cpu.threads":29,"system.memory":5,"system.proc.memory.percent":3.93,"system.gpu.0.memoryAllocated":95.86,"_timestamp":1589174709}
-{"system.network.sent":2073642,"system.gpu.0.powerPercent":12.21,"system.network.recv":314631,"system.gpu.process.0.memory":0.57,"system.disk":55,"system.gpu.process.0.powerPercent":12.21,"system.gpu.0.powerWatts":36.62,"system.gpu.process.0.memoryAllocated":96.1,"_wandb":true,"system.gpu.0.temp":36,"system.gpu.process.0.temp":36,"system.gpu.0.memory":0.53,"system.gpu.0.gpu":3.73,"_runtime":272,"system.gpu.process.0.gpu":4,"system.proc.memory.rssMB":1806.77,"system.gpu.process.0.powerWatts":36.64,"system.proc.memory.availableMB":36905.47,"system.cpu":10.15,"system.proc.cpu.threads":37.67,"system.memory":5.67,"system.proc.memory.percent":4.62,"system.gpu.0.memoryAllocated":89.7,"_timestamp":1589174983}
-{"system.cpu": 89.23, "system.memory": 20.34, "system.disk": 23.5, "system.proc.memory.availableMB": 31166.58, "system.proc.memory.rssMB": 762.96, "system.proc.memory.percent": 1.95, "system.proc.cpu.threads": 31.53, "system.network.sent": 1131545, "system.network.recv": 1043463, "_wandb": true, "_timestamp": 1589175536, "_runtime": 301}
-{"system.cpu": 91.3, "system.memory": 21.48, "system.disk": 23.5, "system.proc.memory.availableMB": 30728.64, "system.proc.memory.rssMB": 946.79, "system.proc.memory.percent": 2.42, "system.proc.cpu.threads": 34.5, "system.network.sent": 1342648, "system.network.recv": 1221379, "_wandb": true, "_timestamp": 1589175542, "_runtime": 307}
diff --git a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/wandb-history.jsonl b/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/wandb-history.jsonl
deleted file mode 100644
index f81e46f..0000000
--- a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/wandb-history.jsonl
+++ /dev/null
@@ -1,119 +0,0 @@
-{"_step":859,"loss":"NaN","_runtime":131.68648886680603,"accuracy":0.3671875,"epoch":411,"val_accuracy":0.63671875,"_timestamp":1.5891750090454595e+09,"val_loss":"NaN"}
-{"_step":860,"loss":"NaN","_runtime":131.79107189178467,"accuracy":0.369140625,"epoch":412,"val_accuracy":0.8984375,"_timestamp":1.5891750091500425e+09,"val_loss":"NaN"}
-{"_step":861,"loss":"NaN","_runtime":131.89158511161804,"accuracy":0.37109375,"epoch":413,"val_accuracy":0.6015625,"_timestamp":1.5891750092505558e+09,"val_loss":"NaN"}
-{"_step":862,"loss":"NaN","_runtime":131.99419593811035,"accuracy":0.369140625,"epoch":414,"val_accuracy":0.8984375,"_timestamp":1.5891750093531666e+09,"val_loss":"NaN"}
-{"_step":863,"loss":"NaN","_runtime":132.18018317222595,"accuracy":0.37109375,"epoch":415,"val_accuracy":0.8984375,"_timestamp":1.5891750095391538e+09,"val_loss":"NaN"}
-{"_step":864,"loss":"NaN","_runtime":132.29829859733582,"accuracy":0.369140625,"epoch":416,"val_accuracy":0.8984375,"_timestamp":1.5891750096572692e+09,"val_loss":"NaN"}
-{"_step":865,"loss":"NaN","_runtime":132.4200291633606,"accuracy":0.37109375,"epoch":417,"val_accuracy":0.8984375,"_timestamp":1.5891750097789998e+09,"val_loss":"NaN"}
-{"_step":866,"loss":"NaN","_runtime":132.5260181427002,"accuracy":0.369140625,"epoch":418,"val_accuracy":0.69140625,"_timestamp":1.5891750098849888e+09,"val_loss":"NaN"}
-{"_step":867,"loss":"NaN","_runtime":132.61498069763184,"accuracy":0.369140625,"epoch":419,"val_accuracy":0.63671875,"_timestamp":1.5891750099739513e+09,"val_loss":"NaN"}
-{"_step":868,"loss":"NaN","_runtime":132.71130681037903,"accuracy":0.369140625,"epoch":420,"val_accuracy":0.62109375,"_timestamp":1.5891750100702775e+09,"val_loss":"NaN"}
-{"epoch": 0, "val_loss": NaN, "val_accuracy": 0.83203125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 145.8294539451599, "_timestamp": 1589175514.332443, "_step": 869}
-{"epoch": 1, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 146.07154178619385, "_timestamp": 1589175514.5745308, "_step": 870}
-{"epoch": 2, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 146.30144810676575, "_timestamp": 1589175514.8044372, "_step": 871}
-{"epoch": 3, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 146.59911608695984, "_timestamp": 1589175515.1021051, "_step": 872}
-{"epoch": 4, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 146.8984818458557, "_timestamp": 1589175515.401471, "_step": 873}
-{"epoch": 5, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 147.17240405082703, "_timestamp": 1589175515.675393, "_step": 874}
-{"epoch": 6, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 147.47535300254822, "_timestamp": 1589175515.978342, "_step": 875}
-{"epoch": 7, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 147.77264261245728, "_timestamp": 1589175516.2756317, "_step": 876}
-{"epoch": 8, "val_loss": NaN, "val_accuracy": 0.58203125, "loss": NaN, "accuracy": 0.369140625, "_runtime": 148.0989954471588, "_timestamp": 1589175516.6019845, "_step": 877}
-{"epoch": 9, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 148.3345549106598, "_timestamp": 1589175516.837544, "_step": 878}
-{"epoch": 10, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 148.55761528015137, "_timestamp": 1589175517.0606043, "_step": 879}
-{"epoch": 11, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 148.81860899925232, "_timestamp": 1589175517.321598, "_step": 880}
-{"epoch": 12, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.3671875, "_runtime": 149.10801935195923, "_timestamp": 1589175517.6110084, "_step": 881}
-{"epoch": 13, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 149.5173201560974, "_timestamp": 1589175518.0203092, "_step": 882}
-{"epoch": 14, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 149.79223775863647, "_timestamp": 1589175518.2952268, "_step": 883}
-{"epoch": 15, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 150.1301212310791, "_timestamp": 1589175518.6331103, "_step": 884}
-{"epoch": 16, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 150.43212342262268, "_timestamp": 1589175518.9351125, "_step": 885}
-{"epoch": 17, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.3671875, "_runtime": 150.6927707195282, "_timestamp": 1589175519.1957598, "_step": 886}
-{"epoch": 18, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 150.92326712608337, "_timestamp": 1589175519.4262562, "_step": 887}
-{"epoch": 19, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 151.18615674972534, "_timestamp": 1589175519.6891458, "_step": 888}
-{"epoch": 20, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 151.4283049106598, "_timestamp": 1589175519.931294, "_step": 889}
-{"epoch": 21, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 151.63620018959045, "_timestamp": 1589175520.1391892, "_step": 890}
-{"epoch": 22, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 152.04993438720703, "_timestamp": 1589175520.5529234, "_step": 891}
-{"epoch": 23, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 152.35772848129272, "_timestamp": 1589175520.8607175, "_step": 892}
-{"epoch": 24, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 152.57880353927612, "_timestamp": 1589175521.0817926, "_step": 893}
-{"epoch": 25, "val_loss": NaN, "val_accuracy": 0.64453125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 152.89634037017822, "_timestamp": 1589175521.3993294, "_step": 894}
-{"epoch": 26, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 153.15721607208252, "_timestamp": 1589175521.6602051, "_step": 895}
-{"epoch": 27, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 153.35663175582886, "_timestamp": 1589175521.8596208, "_step": 896}
-{"epoch": 28, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 153.5683000087738, "_timestamp": 1589175522.071289, "_step": 897}
-{"epoch": 29, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 153.82974934577942, "_timestamp": 1589175522.3327384, "_step": 898}
-{"epoch": 30, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 154.0735149383545, "_timestamp": 1589175522.576504, "_step": 899}
-{"epoch": 31, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 154.33112144470215, "_timestamp": 1589175522.8341105, "_step": 900}
-{"epoch": 32, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 154.57077503204346, "_timestamp": 1589175523.073764, "_step": 901}
-{"epoch": 33, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 154.76859521865845, "_timestamp": 1589175523.2715843, "_step": 902}
-{"epoch": 34, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 155.06649255752563, "_timestamp": 1589175523.5694816, "_step": 903}
-{"epoch": 35, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 155.3928096294403, "_timestamp": 1589175523.8957987, "_step": 904}
-{"epoch": 36, "val_loss": NaN, "val_accuracy": 0.7109375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 155.68726778030396, "_timestamp": 1589175524.1902568, "_step": 905}
-{"epoch": 37, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 155.916490316391, "_timestamp": 1589175524.4194794, "_step": 906}
-{"epoch": 38, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 156.16148924827576, "_timestamp": 1589175524.6644783, "_step": 907}
-{"epoch": 39, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 156.37866616249084, "_timestamp": 1589175524.8816552, "_step": 908}
-{"epoch": 40, "val_loss": NaN, "val_accuracy": 0.57421875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 156.69284296035767, "_timestamp": 1589175525.195832, "_step": 909}
-{"epoch": 41, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 157.00797414779663, "_timestamp": 1589175525.5109632, "_step": 910}
-{"epoch": 42, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 157.24442172050476, "_timestamp": 1589175525.7474108, "_step": 911}
-{"epoch": 43, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 157.53400444984436, "_timestamp": 1589175526.0369935, "_step": 912}
-{"epoch": 44, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 157.8078875541687, "_timestamp": 1589175526.3108766, "_step": 913}
-{"epoch": 45, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 158.06004977226257, "_timestamp": 1589175526.5630388, "_step": 914}
-{"epoch": 46, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 158.335116147995, "_timestamp": 1589175526.8381052, "_step": 915}
-{"epoch": 47, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 158.61446571350098, "_timestamp": 1589175527.1174548, "_step": 916}
-{"epoch": 48, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 158.8516252040863, "_timestamp": 1589175527.3546143, "_step": 917}
-{"epoch": 49, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 159.104736328125, "_timestamp": 1589175527.6077254, "_step": 918}
-{"epoch": 50, "val_loss": NaN, "val_accuracy": 0.62109375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 159.34810853004456, "_timestamp": 1589175527.8510976, "_step": 919}
-{"epoch": 51, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 159.5759675502777, "_timestamp": 1589175528.0789566, "_step": 920}
-{"epoch": 52, "val_loss": NaN, "val_accuracy": 0.80078125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 159.8464879989624, "_timestamp": 1589175528.349477, "_step": 921}
-{"epoch": 53, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 160.06804585456848, "_timestamp": 1589175528.571035, "_step": 922}
-{"epoch": 54, "val_loss": NaN, "val_accuracy": 0.85546875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 160.4318459033966, "_timestamp": 1589175528.934835, "_step": 923}
-{"epoch": 55, "val_loss": NaN, "val_accuracy": 0.5546875, "loss": NaN, "accuracy": 0.37109375, "_runtime": 160.63089060783386, "_timestamp": 1589175529.1338797, "_step": 924}
-{"epoch": 56, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 160.85198283195496, "_timestamp": 1589175529.354972, "_step": 925}
-{"epoch": 57, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 161.03861832618713, "_timestamp": 1589175529.5416074, "_step": 926}
-{"epoch": 58, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 161.26980996131897, "_timestamp": 1589175529.772799, "_step": 927}
-{"epoch": 59, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 161.50362515449524, "_timestamp": 1589175530.0066142, "_step": 928}
-{"epoch": 60, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 161.73083186149597, "_timestamp": 1589175530.233821, "_step": 929}
-{"epoch": 61, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 161.94778323173523, "_timestamp": 1589175530.4507723, "_step": 930}
-{"epoch": 62, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 162.1587746143341, "_timestamp": 1589175530.6617637, "_step": 931}
-{"epoch": 63, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 162.392187833786, "_timestamp": 1589175530.895177, "_step": 932}
-{"epoch": 64, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 162.62690043449402, "_timestamp": 1589175531.1298895, "_step": 933}
-{"epoch": 65, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 162.84836268424988, "_timestamp": 1589175531.3513517, "_step": 934}
-{"epoch": 66, "val_loss": NaN, "val_accuracy": 0.5859375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 163.06788396835327, "_timestamp": 1589175531.570873, "_step": 935}
-{"epoch": 67, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 163.33180212974548, "_timestamp": 1589175531.8347912, "_step": 936}
-{"epoch": 68, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 163.5771083831787, "_timestamp": 1589175532.0800974, "_step": 937}
-{"epoch": 69, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 163.83328127861023, "_timestamp": 1589175532.3362703, "_step": 938}
-{"epoch": 70, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 164.0871241092682, "_timestamp": 1589175532.5901132, "_step": 939}
-{"epoch": 71, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 164.32704758644104, "_timestamp": 1589175532.8300366, "_step": 940}
-{"epoch": 72, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 164.61902928352356, "_timestamp": 1589175533.1220183, "_step": 941}
-{"epoch": 73, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 164.83954524993896, "_timestamp": 1589175533.3425343, "_step": 942}
-{"epoch": 74, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 165.11902618408203, "_timestamp": 1589175533.6220152, "_step": 943}
-{"epoch": 75, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 165.36301398277283, "_timestamp": 1589175533.866003, "_step": 944}
-{"epoch": 76, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.3671875, "_runtime": 165.6196448802948, "_timestamp": 1589175534.122634, "_step": 945}
-{"epoch": 77, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 165.86395502090454, "_timestamp": 1589175534.366944, "_step": 946}
-{"epoch": 78, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 166.10285234451294, "_timestamp": 1589175534.6058414, "_step": 947}
-{"epoch": 79, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 166.32550978660583, "_timestamp": 1589175534.8284988, "_step": 948}
-{"epoch": 80, "val_loss": NaN, "val_accuracy": 0.6015625, "loss": NaN, "accuracy": 0.37109375, "_runtime": 166.55007600784302, "_timestamp": 1589175535.053065, "_step": 949}
-{"epoch": 81, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 166.79152727127075, "_timestamp": 1589175535.2945163, "_step": 950}
-{"epoch": 82, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 167.03722620010376, "_timestamp": 1589175535.5402153, "_step": 951}
-{"epoch": 83, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 167.2901611328125, "_timestamp": 1589175535.7931502, "_step": 952}
-{"epoch": 84, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 167.54004645347595, "_timestamp": 1589175536.0430355, "_step": 953}
-{"epoch": 85, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 167.79178833961487, "_timestamp": 1589175536.2947774, "_step": 954}
-{"epoch": 86, "val_loss": NaN, "val_accuracy": 0.72265625, "loss": NaN, "accuracy": 0.37109375, "_runtime": 168.0635485649109, "_timestamp": 1589175536.5665376, "_step": 955}
-{"epoch": 87, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 168.37446784973145, "_timestamp": 1589175536.877457, "_step": 956}
-{"epoch": 88, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 168.65335035324097, "_timestamp": 1589175537.1563394, "_step": 957}
-{"epoch": 89, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 168.8935170173645, "_timestamp": 1589175537.396506, "_step": 958}
-{"epoch": 90, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 169.18229126930237, "_timestamp": 1589175537.6852803, "_step": 959}
-{"epoch": 91, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 169.52555108070374, "_timestamp": 1589175538.0285401, "_step": 960}
-{"epoch": 92, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 169.84520649909973, "_timestamp": 1589175538.3481956, "_step": 961}
-{"epoch": 93, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 170.13869428634644, "_timestamp": 1589175538.6416833, "_step": 962}
-{"epoch": 94, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 170.4468765258789, "_timestamp": 1589175538.9498656, "_step": 963}
-{"epoch": 95, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 170.72671914100647, "_timestamp": 1589175539.2297082, "_step": 964}
-{"epoch": 96, "val_loss": NaN, "val_accuracy": 0.58984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 170.95463013648987, "_timestamp": 1589175539.4576192, "_step": 965}
-{"epoch": 97, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 171.27019572257996, "_timestamp": 1589175539.7731848, "_step": 966}
-{"epoch": 98, "val_loss": NaN, "val_accuracy": 0.84375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 171.55184721946716, "_timestamp": 1589175540.0548363, "_step": 967}
-{"epoch": 99, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 171.8246579170227, "_timestamp": 1589175540.327647, "_step": 968}
-{"epoch": 100, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 172.08555674552917, "_timestamp": 1589175540.5885458, "_step": 969}
-{"epoch": 101, "val_loss": NaN, "val_accuracy": 0.6953125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 172.38598370552063, "_timestamp": 1589175540.8889728, "_step": 970}
-{"epoch": 102, "val_loss": NaN, "val_accuracy": 0.62109375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 172.63412618637085, "_timestamp": 1589175541.1371152, "_step": 971}
-{"epoch": 103, "val_loss": NaN, "val_accuracy": 0.6953125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 172.85596585273743, "_timestamp": 1589175541.358955, "_step": 972}
-{"epoch": 104, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.369140625, "_runtime": 173.12673234939575, "_timestamp": 1589175541.6297214, "_step": 973}
-{"epoch": 105, "val_loss": NaN, "val_accuracy": 0.6953125, "loss": NaN, "accuracy": 0.3671875, "_runtime": 173.4077603816986, "_timestamp": 1589175541.9107494, "_step": 974}
-{"epoch": 106, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 173.65252661705017, "_timestamp": 1589175542.1555157, "_step": 975}
-{"epoch": 107, "val_loss": NaN, "val_accuracy": 0.8984375, "loss": NaN, "accuracy": 0.37109375, "_runtime": 173.94177103042603, "_timestamp": 1589175542.44476, "_step": 976}
-{"epoch": 108, "val_loss": NaN, "val_accuracy": 0.578125, "loss": NaN, "accuracy": 0.37109375, "_runtime": 174.217529296875, "_timestamp": 1589175542.7205184, "_step": 977}
diff --git a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/wandb-metadata.json b/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/wandb-metadata.json
deleted file mode 100644
index d57475b..0000000
--- a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/wandb-metadata.json
+++ /dev/null
@@ -1,23 +0,0 @@
-{
-    "root": "/home/garthtrickett/ProdigyAI",
-    "program": "tabl.py",
-    "git": {
-        "remote": "https://github.com/garthtrickett/ProdigyAI",
-        "commit": "e692293ef1058c215dd407a890116c7b3826f244"
-    },
-    "email": "garthtrickett@gmail.com",
-    "startedAt": "2020-05-11T05:38:26.972307",
-    "host": "preemp-cpu-1",
-    "username": "garthtrickett",
-    "executable": "/home/garthtrickett/miniconda3/envs/ProdigyAI/bin/python",
-    "os": "Linux-5.3.0-1018-gcp-x86_64-with-debian-buster-sid",
-    "python": "3.7.6",
-    "cpu_count": 6,
-    "args": [],
-    "state": "killed",
-    "jobType": null,
-    "mode": "run",
-    "project": "prodigyai",
-    "heartbeatAt": "2020-05-11T05:39:03.292371",
-    "exitcode": 255
-}
diff --git a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/wandb-summary.json b/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/wandb-summary.json
deleted file mode 100644
index 3cf3b91..0000000
--- a/~/ProdigyAI/wandb/run-20200511_053826-1jasgwb6/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"loss": NaN, "_step": 977, "epoch": 108, "graph": {"_type": "graph-file", "path": "media/graph/graph_summary_6f818560.graph.json", "sha256": "6f81856020e576034c23528bd5c8cc156872ed948db82ecb89f721130ae37682", "size": 1318}, "_runtime": 174.217529296875, "accuracy": 0.37109375, "val_loss": NaN, "best_loss": 0.8786826431751251, "_timestamp": 1589175542.7205184, "best_epoch": 11, "val_accuracy": 0.578125}
diff --git a/~/ProdigyAI/wandb/wandb-resume.json b/~/ProdigyAI/wandb/wandb-resume.json
deleted file mode 100644
index 329e387..0000000
--- a/~/ProdigyAI/wandb/wandb-resume.json
+++ /dev/null
@@ -1 +0,0 @@
-{"run_id": "1jasgwb6"}
\ No newline at end of file

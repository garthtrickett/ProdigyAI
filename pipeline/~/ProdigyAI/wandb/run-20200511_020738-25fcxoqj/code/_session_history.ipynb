{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 0, 3)"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Activation,\n",
    "    Input,\n",
    "    Reshape,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    LeakyReLU,\n",
    "    concatenate,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from numba import njit, prange\n",
    "\n",
    "import yaml\n",
    "# set random seeds\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import h5py\n",
    "import wandb\n",
    "\n",
    "from wandb.keras import WandbCallback\n",
    "# check if using gpu\n",
    "\n",
    "gpus = tf.config.list_physical_devices()\n",
    "any_gpus = [s for s in gpus if \"GPU\" in s[0]]\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    resuming = \"NA\"\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    args = parser.parse_args()\n",
    "    if args.resuming != None:\n",
    "        resuming = args.resuming\n",
    "    else:\n",
    "        resuming = \"NA\"\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "# Init wandb\n",
    "\n",
    "yaml_path = path_adjust + \"yaml/deeplob.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "wandb.init(dir=\"~/ProdigyAI/\",\n",
    "           project=\"prodigyai\",\n",
    "           config=config_dictionary,\n",
    "           resume=True)\n",
    "\n",
    "window_length = wandb.config['params']['window_length']['value']\n",
    "num_features = wandb.config['params']['num_features']['value']\n",
    "epochs = wandb.config['params']['epochs']['value']\n",
    "batch_size = wandb.config['params']['batch_size']['value']\n",
    "\n",
    "number_of_lstm = wandb.config['params']['number_of_lstm']['value']\n",
    "# limit gpu usage for keras\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "import tensorflow.python.keras.backend as K\n",
    "\n",
    "sess = K.get_session()\n",
    "## lob FI-2010 DATA PREPERATION\n",
    "# The first 40 columns of the FI-2010 dataset are 10 levels ask and bid information\n",
    "# for a limit order book and we only use these 40 features in our network.\n",
    "# The last 5 columns of the FI-2010 dataset are the labels with different prediction horizons.\n",
    "# def prepare_x(data):\n",
    "#     df1 = data[:40, :].T\n",
    "#     return np.array(df1)\n",
    "# def get_label(data):\n",
    "#     lob = data[-5:, :].T\n",
    "#     return lob\n",
    "# def data_classification(X, Y, T):\n",
    "#     [N, D] = X.shape\n",
    "#     df = np.array(X)\n",
    "#     dY = np.array(Y)\n",
    "#     dataY = dY[T - 1 : N]\n",
    "#     dataX = np.zeros((N - T + 1, T, D))\n",
    "#     for i in range(T, N + 1):\n",
    "#         dataX[i - T] = df[i - T : i, :]\n",
    "#     return dataX.reshape(dataX.shape + (1,)), dataY\n",
    "# # please change the data_path to your local path\n",
    "# data_path = (\n",
    "#     path_adjust\n",
    "#     + \"third_party_libraries/gam_rhn/95-FI2010/data/BenchmarkDatasets/NoAuction\"\n",
    "# )\n",
    "# dec_train = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Training/Train_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "# )\n",
    "# dec_test1 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "# )\n",
    "# dec_test2 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_8.txt\"\n",
    "# )\n",
    "# dec_test3 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_9.txt\"\n",
    "# )\n",
    "# dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "# # extract limit order book data from the FI-2010 dataset\n",
    "# train_lob = prepare_x(dec_train)\n",
    "# test_lob = prepare_x(dec_test)\n",
    "# mins, maxes, means, stds = get_feature_stats(train_lob)\n",
    "# # extract label from the FI-2010 dataset\n",
    "# train_label = get_label(dec_train)\n",
    "# test_label = get_label(dec_test)\n",
    "# # prepare training data. We feed past 100 observations into our algorithms and choose the prediction horizon.\n",
    "# trainX_CNN, trainY_CNN = data_classification(train_lob, train_label, T=100)\n",
    "# trainY_CNN = trainY_CNN[:,3] - 1\n",
    "# trainY_CNN = np_utils.to_categorical(trainY_CNN, 3)\n",
    "# # prepare test data.\n",
    "# testX_CNN, testY_CNN = data_classification(test_lob, test_label, T=100)\n",
    "# testY_CNN = testY_CNN[:,3] - 1\n",
    "# testY_CNN = np_utils.to_categorical(testY_CNN, 3)\n",
    "# h5f = h5py.File(path_adjust + \"data/lob_2010/train_and_test_deeplob.h5\", \"w\")\n",
    "# h5f.create_dataset(\"trainX_CNN\", data=trainX_CNN)\n",
    "# h5f.create_dataset(\"trainY_CNN\", data=trainY_CNN)\n",
    "# h5f.create_dataset(\"testX_CNN\", data=testX_CNN)\n",
    "# h5f.create_dataset(\"testY_CNN\", data=testY_CNN)\n",
    "# h5f.close()\n",
    "# path = path_adjust + \"data/lob_2010/train_and_test_deeplob.h5\"\n",
    "# h5f = h5py.File(path, \"r\")\n",
    "# trainX_CNN = h5f[\"trainX_CNN\"][:]\n",
    "# trainY_CNN = h5f[\"trainY_CNN\"][:]\n",
    "# testX_CNN = h5f[\"testX_CNN\"][:]\n",
    "# testY_CNN = h5f[\"testY_CNN\"][:]\n",
    "# h5f.close()\n",
    "# trainX_CNN.shape\n",
    "# testX_CNN.shape (139488, 100, 40, 1)\n",
    "# trainX_CNN.shape (254651, 100, 40, 1)\n",
    "# np.mean(trainX_CNN) == 0.60001\n",
    "# np.min(trainX_CNN) == 0\n",
    "# np.mean(trainX_CNN) == 0.12389889685674725\n",
    "# np.std(trainX_CNN) ==\n",
    "# np.std(trainX_CNN) == 0.12757670786242864\n",
    "# each 40 contains the 10 top bid and ask price and volumes for a timestep\n",
    "## PRODIGY AI HOCKUS POCKUS START\n",
    "\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "home = str(Path.home())\n",
    "file_name = \"esugj36b.h5\"\n",
    "wandb.config.update({'dataset': file_name})\n",
    "path = home + \"/ProdigyAI/data/preprocessed/\" + file_name\n",
    "h5f = h5py.File(path, \"r\")\n",
    "\n",
    "prices_for_window_index_array_train = h5f[\n",
    "    \"prices_for_window_index_array_train\"][:]\n",
    "\n",
    "prices_for_window_index_array_val = h5f[\"prices_for_window_index_array_val\"][:]\n",
    "\n",
    "prices_for_window_index_array_test = h5f[\n",
    "    \"prices_for_window_index_array_test\"][:]\n",
    "\n",
    "close_index_array_train = h5f[\"close_index_array_train\"][:]\n",
    "close_index_array_val = h5f[\"close_index_array_val\"][:]\n",
    "close_index_array_test = h5f[\"close_index_array_test\"][:]\n",
    "input_features_normalized_train = h5f[\"input_features_normalized_train\"][:]\n",
    "input_features_normalized_val = h5f[\"input_features_normalized_val\"][:]\n",
    "input_features_normalized_test = h5f[\"input_features_normalized_test\"][:]\n",
    "y_train = h5f[\"y_train\"][:]\n",
    "y_val = h5f[\"y_val\"][:]\n",
    "y_test = h5f[\"y_test\"][:]\n",
    "h5f.close()\n",
    "\n",
    "for i in range(len(close_index_array_train)):\n",
    "    if close_index_array_train[i] == prices_for_window_index_array_train[0]:\n",
    "        offset = i\n",
    "        break\n",
    "\n",
    "@njit\n",
    "def generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                     input_features_normalized):\n",
    "    X = np.empty((batch_size, *dim))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        for k in prange(dim[0]):\n",
    "            for l in prange(len(input_features_normalized)):\n",
    "                X[i][k][l] = input_features_normalized[l][ID + k]\n",
    "    # X[i, ] = self.input_features_normalized[ID+self.window_length, ]\n",
    "    return X.reshape((X.shape[0], X.shape[1], X.shape[2], 1))\n",
    "\n",
    "@njit\n",
    "def generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data):\n",
    "    y = np.empty((batch_size, n_classes))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        y[i, :] = y_data[ID]\n",
    "    return y\n",
    "\n",
    "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
    "    def __init__(self,\n",
    "                 checkpoint_path,\n",
    "                 prices_for_window_index_array,\n",
    "                 input_features_normalized,\n",
    "                 y_data,\n",
    "                 batch_size,\n",
    "                 dim,\n",
    "                 n_classes,\n",
    "                 to_fit,\n",
    "                 shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.prices_for_window_index_array = prices_for_window_index_array\n",
    "        self.input_features_normalized = input_features_normalized\n",
    "        self.labels = y_data\n",
    "        self.y_data = y_data\n",
    "        self.to_fit = to_fit\n",
    "        self.n_classes = n_classes\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.n = 0\n",
    "        self.list_IDs = np.arange(len(self.prices_for_window_index_array))\n",
    "        self.on_epoch_end()\n",
    "    def __next__(self):\n",
    "        # Get one batch of data\n",
    "        data = self.__getitem__(self.n)\n",
    "        # Batch index\n",
    "        self.n += 1\n",
    "        # If we have processed the entire dataset then\n",
    "        if self.n >= self.__len__():\n",
    "            self.on_epoch_end\n",
    "            self.n = 0\n",
    "        return data\n",
    "    def __len__(self):\n",
    "        # Return the number of batches of the dataset\n",
    "        return math.ceil(len(self.indexes) / self.batch_size)\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                               self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X = self._generate_x(list_IDs_temp)\n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(list_IDs_temp)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    def on_epoch_end(self):\n",
    "        wandb.save(self.checkpoint_path)\n",
    "        self.indexes = np.arange(len(self.prices_for_window_index_array))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def _generate_x(self, list_IDs_temp):\n",
    "        dim = self.dim\n",
    "        batch_size = self.batch_size\n",
    "        input_features_normalized = self.input_features_normalized\n",
    "        X = generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                             input_features_normalized)\n",
    "        return X\n",
    "    def _generate_y(self, list_IDs_temp):\n",
    "        batch_size = self.batch_size\n",
    "        n_classes = self.n_classes\n",
    "        y_data = self.y_data\n",
    "        y = generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data)\n",
    "        return y\n",
    "\n",
    "n_classes = 3\n",
    "dim = (window_length, num_features)\n",
    "\n",
    "to_fit = True\n",
    "# for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,\n",
    "#                                       input_features_normalized_train,\n",
    "#                                       y_train,\n",
    "#                                       batch_size,\n",
    "#                                       dim,\n",
    "#                                       n_classes,\n",
    "#                                       to_fit,\n",
    "#                                       shuffle=True):\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "# for X_batch, y_batch in generator(X_train, y_train, 64):\n",
    "#     print(\"ok\")\n",
    "## PRODIGY AI HOCKUS POCKUS END\n",
    "\n",
    "checkpoint_path = path_adjust + \"temp/cp.ckpt\"\n",
    "# Create a callback that saves the model's weights\n",
    "\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                              save_weights_only=True,\n",
    "                              verbose=1)\n",
    "\n",
    "check_point_file = Path(checkpoint_path)\n",
    "\n",
    "if check_point_file.exists() and resuming == \"resuming\":\n",
    "    print(\"weights loaded\")\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "train_generator = DataGenerator(checkpoint_path,\n",
    "                                prices_for_window_index_array_train,\n",
    "                                input_features_normalized_train,\n",
    "                                y_train,\n",
    "                                batch_size,\n",
    "                                dim,\n",
    "                                n_classes,\n",
    "                                to_fit,\n",
    "                                shuffle=True)\n",
    "\n",
    "val_generator = DataGenerator(checkpoint_path,\n",
    "                              prices_for_window_index_array_val,\n",
    "                              input_features_normalized_val,\n",
    "                              y_val,\n",
    "                              batch_size,\n",
    "                              dim,\n",
    "                              n_classes,\n",
    "                              to_fit,\n",
    "                              shuffle=True)\n",
    "\n",
    "steps_per_epoch = len(train_generator)\n",
    "validation_steps = len(val_generator)\n",
    "\n",
    "if wandb.run.resumed:\n",
    "    pdb.set_trace()\n",
    "    wandb.restore(\"model-best.h5\",run_path=\"garthtrickett/prodigyai/\" + wandb.run.id)\n",
    "    # restore the best model\n",
    "    deeplob = keras.models.load_model(wandb.restore(\"model-best.h5\").name)\n",
    "    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)\n",
    "    deeplob.compile(optimizer=adam,\n",
    "                    loss=\"categorical_crossentropy\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "else:\n",
    "    ### Model Architecture\n",
    "    def create_deeplob(T, NF, number_of_lstm):\n",
    "        input_lmd = Input(shape=(T, NF, 1))\n",
    "        # build the convolutional block\n",
    "        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (1, 10))(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        # build the inception module\n",
    "        convsecond_1 = Conv2D(64, (1, 1), padding=\"same\")(conv_first1)\n",
    "        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "        convsecond_1 = Conv2D(64, (3, 1), padding=\"same\")(convsecond_1)\n",
    "        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "        convsecond_2 = Conv2D(64, (1, 1), padding=\"same\")(conv_first1)\n",
    "        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "        convsecond_2 = Conv2D(64, (5, 1), padding=\"same\")(convsecond_2)\n",
    "        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "        convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),\n",
    "                                    padding=\"same\")(conv_first1)\n",
    "        convsecond_3 = Conv2D(64, (1, 1), padding=\"same\")(convsecond_3)\n",
    "        convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)\n",
    "        convsecond_output = concatenate(\n",
    "            [convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
    "        # use the MC dropout here\n",
    "        conv_reshape = Reshape(\n",
    "            (int(convsecond_output.shape[1]),\n",
    "             int(convsecond_output.shape[3])))(convsecond_output)\n",
    "        conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)\n",
    "        # build the output layer\n",
    "        out = Dense(3, activation=\"softmax\")(conv_lstm)\n",
    "        model = Model(inputs=input_lmd, outputs=out)\n",
    "        adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)\n",
    "        model.compile(optimizer=adam,\n",
    "                      loss=\"categorical_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "        return model\n",
    "    deeplob = create_deeplob(window_length, num_features, number_of_lstm)\n",
    "\n",
    "deeplob.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[cp_callback,\n",
    "               WandbCallback(save_model=True, monitor=\"loss\")])\n",
    "\n",
    "finished_weights_path = path_adjust + \"temp/cp_end.ckpt\"\n",
    "deeplob.save_weights(finished_weights_path)\n",
    "\n",
    "print(\"script finished\")\n",
    "# deeplob = create_deeplob(200, 40, 64)\n",
    "# deeplob.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     epochs=1000,\n",
    "#     batch_size=64,\n",
    "#     verbose=2,\n",
    "#     validation_data=(X_val, y_val),\n",
    "# )\n",
    "## THIS SHOULD BE COMBINED WITH LSTM LAYERS WITH stateful=True\n",
    "# deeplob = create_deeplob(200, 40, 64)\n",
    "# for i in range(200):\n",
    "#     deeplob.fit(\n",
    "#         X_train,\n",
    "#         y_train,\n",
    "#         epochs=1,\n",
    "#         batch_size=64,\n",
    "#         verbose=2,\n",
    "#         validation_data=(X_val, y_val),\n",
    "#         shuffle=False,\n",
    "#     )\n",
    "#     deeplob.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Activation,\n",
    "    Input,\n",
    "    Reshape,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    LeakyReLU,\n",
    "    concatenate,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from numba import njit, prange\n",
    "\n",
    "import yaml\n",
    "# set random seeds\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import h5py\n",
    "import wandb\n",
    "\n",
    "from wandb.keras import WandbCallback\n",
    "# check if using gpu\n",
    "\n",
    "gpus = tf.config.list_physical_devices()\n",
    "any_gpus = [s for s in gpus if \"GPU\" in s[0]]\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    resuming = \"NA\"\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    args = parser.parse_args()\n",
    "    if args.resuming != None:\n",
    "        resuming = args.resuming\n",
    "    else:\n",
    "        resuming = \"NA\"\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "# Init wandb\n",
    "\n",
    "yaml_path = path_adjust + \"yaml/deeplob.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "wandb.init(dir=\"~/ProdigyAI/\",\n",
    "           project=\"prodigyai\",\n",
    "           config=config_dictionary,\n",
    "           resume=True)\n",
    "\n",
    "window_length = wandb.config['params']['window_length']['value']\n",
    "num_features = wandb.config['params']['num_features']['value']\n",
    "epochs = wandb.config['params']['epochs']['value']\n",
    "batch_size = wandb.config['params']['batch_size']['value']\n",
    "\n",
    "number_of_lstm = wandb.config['params']['number_of_lstm']['value']\n",
    "# limit gpu usage for keras\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "import tensorflow.python.keras.backend as K\n",
    "\n",
    "sess = K.get_session()\n",
    "## lob FI-2010 DATA PREPERATION\n",
    "# The first 40 columns of the FI-2010 dataset are 10 levels ask and bid information\n",
    "# for a limit order book and we only use these 40 features in our network.\n",
    "# The last 5 columns of the FI-2010 dataset are the labels with different prediction horizons.\n",
    "# def prepare_x(data):\n",
    "#     df1 = data[:40, :].T\n",
    "#     return np.array(df1)\n",
    "# def get_label(data):\n",
    "#     lob = data[-5:, :].T\n",
    "#     return lob\n",
    "# def data_classification(X, Y, T):\n",
    "#     [N, D] = X.shape\n",
    "#     df = np.array(X)\n",
    "#     dY = np.array(Y)\n",
    "#     dataY = dY[T - 1 : N]\n",
    "#     dataX = np.zeros((N - T + 1, T, D))\n",
    "#     for i in range(T, N + 1):\n",
    "#         dataX[i - T] = df[i - T : i, :]\n",
    "#     return dataX.reshape(dataX.shape + (1,)), dataY\n",
    "# # please change the data_path to your local path\n",
    "# data_path = (\n",
    "#     path_adjust\n",
    "#     + \"third_party_libraries/gam_rhn/95-FI2010/data/BenchmarkDatasets/NoAuction\"\n",
    "# )\n",
    "# dec_train = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Training/Train_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "# )\n",
    "# dec_test1 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "# )\n",
    "# dec_test2 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_8.txt\"\n",
    "# )\n",
    "# dec_test3 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_9.txt\"\n",
    "# )\n",
    "# dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "# # extract limit order book data from the FI-2010 dataset\n",
    "# train_lob = prepare_x(dec_train)\n",
    "# test_lob = prepare_x(dec_test)\n",
    "# mins, maxes, means, stds = get_feature_stats(train_lob)\n",
    "# # extract label from the FI-2010 dataset\n",
    "# train_label = get_label(dec_train)\n",
    "# test_label = get_label(dec_test)\n",
    "# # prepare training data. We feed past 100 observations into our algorithms and choose the prediction horizon.\n",
    "# trainX_CNN, trainY_CNN = data_classification(train_lob, train_label, T=100)\n",
    "# trainY_CNN = trainY_CNN[:,3] - 1\n",
    "# trainY_CNN = np_utils.to_categorical(trainY_CNN, 3)\n",
    "# # prepare test data.\n",
    "# testX_CNN, testY_CNN = data_classification(test_lob, test_label, T=100)\n",
    "# testY_CNN = testY_CNN[:,3] - 1\n",
    "# testY_CNN = np_utils.to_categorical(testY_CNN, 3)\n",
    "# h5f = h5py.File(path_adjust + \"data/lob_2010/train_and_test_deeplob.h5\", \"w\")\n",
    "# h5f.create_dataset(\"trainX_CNN\", data=trainX_CNN)\n",
    "# h5f.create_dataset(\"trainY_CNN\", data=trainY_CNN)\n",
    "# h5f.create_dataset(\"testX_CNN\", data=testX_CNN)\n",
    "# h5f.create_dataset(\"testY_CNN\", data=testY_CNN)\n",
    "# h5f.close()\n",
    "# path = path_adjust + \"data/lob_2010/train_and_test_deeplob.h5\"\n",
    "# h5f = h5py.File(path, \"r\")\n",
    "# trainX_CNN = h5f[\"trainX_CNN\"][:]\n",
    "# trainY_CNN = h5f[\"trainY_CNN\"][:]\n",
    "# testX_CNN = h5f[\"testX_CNN\"][:]\n",
    "# testY_CNN = h5f[\"testY_CNN\"][:]\n",
    "# h5f.close()\n",
    "# trainX_CNN.shape\n",
    "# testX_CNN.shape (139488, 100, 40, 1)\n",
    "# trainX_CNN.shape (254651, 100, 40, 1)\n",
    "# np.mean(trainX_CNN) == 0.60001\n",
    "# np.min(trainX_CNN) == 0\n",
    "# np.mean(trainX_CNN) == 0.12389889685674725\n",
    "# np.std(trainX_CNN) ==\n",
    "# np.std(trainX_CNN) == 0.12757670786242864\n",
    "# each 40 contains the 10 top bid and ask price and volumes for a timestep\n",
    "## PRODIGY AI HOCKUS POCKUS START\n",
    "\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "home = str(Path.home())\n",
    "file_name = \"esugj36b.h5\"\n",
    "wandb.config.update({'dataset': file_name})\n",
    "path = home + \"/ProdigyAI/data/preprocessed/\" + file_name\n",
    "h5f = h5py.File(path, \"r\")\n",
    "\n",
    "prices_for_window_index_array_train = h5f[\n",
    "    \"prices_for_window_index_array_train\"][:]\n",
    "\n",
    "prices_for_window_index_array_val = h5f[\"prices_for_window_index_array_val\"][:]\n",
    "\n",
    "prices_for_window_index_array_test = h5f[\n",
    "    \"prices_for_window_index_array_test\"][:]\n",
    "\n",
    "close_index_array_train = h5f[\"close_index_array_train\"][:]\n",
    "close_index_array_val = h5f[\"close_index_array_val\"][:]\n",
    "close_index_array_test = h5f[\"close_index_array_test\"][:]\n",
    "input_features_normalized_train = h5f[\"input_features_normalized_train\"][:]\n",
    "input_features_normalized_val = h5f[\"input_features_normalized_val\"][:]\n",
    "input_features_normalized_test = h5f[\"input_features_normalized_test\"][:]\n",
    "y_train = h5f[\"y_train\"][:]\n",
    "y_val = h5f[\"y_val\"][:]\n",
    "y_test = h5f[\"y_test\"][:]\n",
    "h5f.close()\n",
    "\n",
    "for i in range(len(close_index_array_train)):\n",
    "    if close_index_array_train[i] == prices_for_window_index_array_train[0]:\n",
    "        offset = i\n",
    "        break\n",
    "\n",
    "@njit\n",
    "def generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                     input_features_normalized):\n",
    "    X = np.empty((batch_size, *dim))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        for k in prange(dim[0]):\n",
    "            for l in prange(len(input_features_normalized)):\n",
    "                X[i][k][l] = input_features_normalized[l][ID + k]\n",
    "    # X[i, ] = self.input_features_normalized[ID+self.window_length, ]\n",
    "    return X.reshape((X.shape[0], X.shape[1], X.shape[2], 1))\n",
    "\n",
    "@njit\n",
    "def generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data):\n",
    "    y = np.empty((batch_size, n_classes))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        y[i, :] = y_data[ID]\n",
    "    return y\n",
    "\n",
    "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
    "    def __init__(self,\n",
    "                 checkpoint_path,\n",
    "                 prices_for_window_index_array,\n",
    "                 input_features_normalized,\n",
    "                 y_data,\n",
    "                 batch_size,\n",
    "                 dim,\n",
    "                 n_classes,\n",
    "                 to_fit,\n",
    "                 shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.prices_for_window_index_array = prices_for_window_index_array\n",
    "        self.input_features_normalized = input_features_normalized\n",
    "        self.labels = y_data\n",
    "        self.y_data = y_data\n",
    "        self.to_fit = to_fit\n",
    "        self.n_classes = n_classes\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.n = 0\n",
    "        self.list_IDs = np.arange(len(self.prices_for_window_index_array))\n",
    "        self.on_epoch_end()\n",
    "    def __next__(self):\n",
    "        # Get one batch of data\n",
    "        data = self.__getitem__(self.n)\n",
    "        # Batch index\n",
    "        self.n += 1\n",
    "        # If we have processed the entire dataset then\n",
    "        if self.n >= self.__len__():\n",
    "            self.on_epoch_end\n",
    "            self.n = 0\n",
    "        return data\n",
    "    def __len__(self):\n",
    "        # Return the number of batches of the dataset\n",
    "        return math.ceil(len(self.indexes) / self.batch_size)\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                               self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X = self._generate_x(list_IDs_temp)\n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(list_IDs_temp)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    def on_epoch_end(self):\n",
    "        wandb.save(self.checkpoint_path)\n",
    "        self.indexes = np.arange(len(self.prices_for_window_index_array))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def _generate_x(self, list_IDs_temp):\n",
    "        dim = self.dim\n",
    "        batch_size = self.batch_size\n",
    "        input_features_normalized = self.input_features_normalized\n",
    "        X = generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                             input_features_normalized)\n",
    "        return X\n",
    "    def _generate_y(self, list_IDs_temp):\n",
    "        batch_size = self.batch_size\n",
    "        n_classes = self.n_classes\n",
    "        y_data = self.y_data\n",
    "        y = generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data)\n",
    "        return y\n",
    "\n",
    "n_classes = 3\n",
    "dim = (window_length, num_features)\n",
    "\n",
    "to_fit = True\n",
    "# for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,\n",
    "#                                       input_features_normalized_train,\n",
    "#                                       y_train,\n",
    "#                                       batch_size,\n",
    "#                                       dim,\n",
    "#                                       n_classes,\n",
    "#                                       to_fit,\n",
    "#                                       shuffle=True):\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "# for X_batch, y_batch in generator(X_train, y_train, 64):\n",
    "#     print(\"ok\")\n",
    "## PRODIGY AI HOCKUS POCKUS END\n",
    "\n",
    "checkpoint_path = path_adjust + \"temp/cp.ckpt\"\n",
    "# Create a callback that saves the model's weights\n",
    "\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                              save_weights_only=True,\n",
    "                              verbose=1)\n",
    "\n",
    "check_point_file = Path(checkpoint_path)\n",
    "\n",
    "if check_point_file.exists() and resuming == \"resuming\":\n",
    "    print(\"weights loaded\")\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "train_generator = DataGenerator(checkpoint_path,\n",
    "                                prices_for_window_index_array_train,\n",
    "                                input_features_normalized_train,\n",
    "                                y_train,\n",
    "                                batch_size,\n",
    "                                dim,\n",
    "                                n_classes,\n",
    "                                to_fit,\n",
    "                                shuffle=True)\n",
    "\n",
    "val_generator = DataGenerator(checkpoint_path,\n",
    "                              prices_for_window_index_array_val,\n",
    "                              input_features_normalized_val,\n",
    "                              y_val,\n",
    "                              batch_size,\n",
    "                              dim,\n",
    "                              n_classes,\n",
    "                              to_fit,\n",
    "                              shuffle=True)\n",
    "\n",
    "steps_per_epoch = len(train_generator)\n",
    "validation_steps = len(val_generator)\n",
    "\n",
    "if wandb.run.resumed:\n",
    "    wandb.restore(\"model-best.h5\",run_path=\"garthtrickett/prodigyai/\" + wandb.run.id)\n",
    "    # restore the best model\n",
    "    deeplob = keras.models.load_model(wandb.restore(\"model-best.h5\").name)\n",
    "    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)\n",
    "    deeplob.compile(optimizer=adam,\n",
    "                    loss=\"categorical_crossentropy\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "else:\n",
    "    ### Model Architecture\n",
    "    def create_deeplob(T, NF, number_of_lstm):\n",
    "        input_lmd = Input(shape=(T, NF, 1))\n",
    "        # build the convolutional block\n",
    "        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (1, 10))(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        # build the inception module\n",
    "        convsecond_1 = Conv2D(64, (1, 1), padding=\"same\")(conv_first1)\n",
    "        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "        convsecond_1 = Conv2D(64, (3, 1), padding=\"same\")(convsecond_1)\n",
    "        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "        convsecond_2 = Conv2D(64, (1, 1), padding=\"same\")(conv_first1)\n",
    "        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "        convsecond_2 = Conv2D(64, (5, 1), padding=\"same\")(convsecond_2)\n",
    "        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "        convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),\n",
    "                                    padding=\"same\")(conv_first1)\n",
    "        convsecond_3 = Conv2D(64, (1, 1), padding=\"same\")(convsecond_3)\n",
    "        convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)\n",
    "        convsecond_output = concatenate(\n",
    "            [convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
    "        # use the MC dropout here\n",
    "        conv_reshape = Reshape(\n",
    "            (int(convsecond_output.shape[1]),\n",
    "             int(convsecond_output.shape[3])))(convsecond_output)\n",
    "        conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)\n",
    "        # build the output layer\n",
    "        out = Dense(3, activation=\"softmax\")(conv_lstm)\n",
    "        model = Model(inputs=input_lmd, outputs=out)\n",
    "        adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)\n",
    "        model.compile(optimizer=adam,\n",
    "                      loss=\"categorical_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "        return model\n",
    "    deeplob = create_deeplob(window_length, num_features, number_of_lstm)\n",
    "\n",
    "deeplob.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[cp_callback,\n",
    "               WandbCallback(save_model=True, monitor=\"loss\")])\n",
    "\n",
    "finished_weights_path = path_adjust + \"temp/cp_end.ckpt\"\n",
    "deeplob.save_weights(finished_weights_path)\n",
    "\n",
    "print(\"script finished\")\n",
    "# deeplob = create_deeplob(200, 40, 64)\n",
    "# deeplob.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     epochs=1000,\n",
    "#     batch_size=64,\n",
    "#     verbose=2,\n",
    "#     validation_data=(X_val, y_val),\n",
    "# )\n",
    "## THIS SHOULD BE COMBINED WITH LSTM LAYERS WITH stateful=True\n",
    "# deeplob = create_deeplob(200, 40, 64)\n",
    "# for i in range(200):\n",
    "#     deeplob.fit(\n",
    "#         X_train,\n",
    "#         y_train,\n",
    "#         epochs=1,\n",
    "#         batch_size=64,\n",
    "#         verbose=2,\n",
    "#         validation_data=(X_val, y_val),\n",
    "#         shuffle=False,\n",
    "#     )\n",
    "#     deeplob.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Activation,\n",
    "    Input,\n",
    "    Reshape,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    LeakyReLU,\n",
    "    concatenate,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from numba import njit, prange\n",
    "\n",
    "import yaml\n",
    "# set random seeds\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import h5py\n",
    "import wandb\n",
    "\n",
    "from wandb.keras import WandbCallback\n",
    "# check if using gpu\n",
    "\n",
    "gpus = tf.config.list_physical_devices()\n",
    "any_gpus = [s for s in gpus if \"GPU\" in s[0]]\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    resuming = \"NA\"\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    args = parser.parse_args()\n",
    "    if args.resuming != None:\n",
    "        resuming = args.resuming\n",
    "    else:\n",
    "        resuming = \"NA\"\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "# Init wandb\n",
    "\n",
    "yaml_path = path_adjust + \"yaml/deeplob.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "wandb.init(dir=\"~/ProdigyAI/\",\n",
    "           project=\"prodigyai\",\n",
    "           config=config_dictionary,\n",
    "           resume=True)\n",
    "\n",
    "window_length = wandb.config['params']['window_length']['value']\n",
    "num_features = wandb.config['params']['num_features']['value']\n",
    "epochs = wandb.config['params']['epochs']['value']\n",
    "batch_size = wandb.config['params']['batch_size']['value']\n",
    "\n",
    "number_of_lstm = wandb.config['params']['number_of_lstm']['value']\n",
    "# limit gpu usage for keras\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "import tensorflow.python.keras.backend as K\n",
    "\n",
    "sess = K.get_session()\n",
    "## lob FI-2010 DATA PREPERATION\n",
    "# The first 40 columns of the FI-2010 dataset are 10 levels ask and bid information\n",
    "# for a limit order book and we only use these 40 features in our network.\n",
    "# The last 5 columns of the FI-2010 dataset are the labels with different prediction horizons.\n",
    "# def prepare_x(data):\n",
    "#     df1 = data[:40, :].T\n",
    "#     return np.array(df1)\n",
    "# def get_label(data):\n",
    "#     lob = data[-5:, :].T\n",
    "#     return lob\n",
    "# def data_classification(X, Y, T):\n",
    "#     [N, D] = X.shape\n",
    "#     df = np.array(X)\n",
    "#     dY = np.array(Y)\n",
    "#     dataY = dY[T - 1 : N]\n",
    "#     dataX = np.zeros((N - T + 1, T, D))\n",
    "#     for i in range(T, N + 1):\n",
    "#         dataX[i - T] = df[i - T : i, :]\n",
    "#     return dataX.reshape(dataX.shape + (1,)), dataY\n",
    "# # please change the data_path to your local path\n",
    "# data_path = (\n",
    "#     path_adjust\n",
    "#     + \"third_party_libraries/gam_rhn/95-FI2010/data/BenchmarkDatasets/NoAuction\"\n",
    "# )\n",
    "# dec_train = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Training/Train_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "# )\n",
    "# dec_test1 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "# )\n",
    "# dec_test2 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_8.txt\"\n",
    "# )\n",
    "# dec_test3 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_9.txt\"\n",
    "# )\n",
    "# dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "# # extract limit order book data from the FI-2010 dataset\n",
    "# train_lob = prepare_x(dec_train)\n",
    "# test_lob = prepare_x(dec_test)\n",
    "# mins, maxes, means, stds = get_feature_stats(train_lob)\n",
    "# # extract label from the FI-2010 dataset\n",
    "# train_label = get_label(dec_train)\n",
    "# test_label = get_label(dec_test)\n",
    "# # prepare training data. We feed past 100 observations into our algorithms and choose the prediction horizon.\n",
    "# trainX_CNN, trainY_CNN = data_classification(train_lob, train_label, T=100)\n",
    "# trainY_CNN = trainY_CNN[:,3] - 1\n",
    "# trainY_CNN = np_utils.to_categorical(trainY_CNN, 3)\n",
    "# # prepare test data.\n",
    "# testX_CNN, testY_CNN = data_classification(test_lob, test_label, T=100)\n",
    "# testY_CNN = testY_CNN[:,3] - 1\n",
    "# testY_CNN = np_utils.to_categorical(testY_CNN, 3)\n",
    "# h5f = h5py.File(path_adjust + \"data/lob_2010/train_and_test_deeplob.h5\", \"w\")\n",
    "# h5f.create_dataset(\"trainX_CNN\", data=trainX_CNN)\n",
    "# h5f.create_dataset(\"trainY_CNN\", data=trainY_CNN)\n",
    "# h5f.create_dataset(\"testX_CNN\", data=testX_CNN)\n",
    "# h5f.create_dataset(\"testY_CNN\", data=testY_CNN)\n",
    "# h5f.close()\n",
    "# path = path_adjust + \"data/lob_2010/train_and_test_deeplob.h5\"\n",
    "# h5f = h5py.File(path, \"r\")\n",
    "# trainX_CNN = h5f[\"trainX_CNN\"][:]\n",
    "# trainY_CNN = h5f[\"trainY_CNN\"][:]\n",
    "# testX_CNN = h5f[\"testX_CNN\"][:]\n",
    "# testY_CNN = h5f[\"testY_CNN\"][:]\n",
    "# h5f.close()\n",
    "# trainX_CNN.shape\n",
    "# testX_CNN.shape (139488, 100, 40, 1)\n",
    "# trainX_CNN.shape (254651, 100, 40, 1)\n",
    "# np.mean(trainX_CNN) == 0.60001\n",
    "# np.min(trainX_CNN) == 0\n",
    "# np.mean(trainX_CNN) == 0.12389889685674725\n",
    "# np.std(trainX_CNN) ==\n",
    "# np.std(trainX_CNN) == 0.12757670786242864\n",
    "# each 40 contains the 10 top bid and ask price and volumes for a timestep\n",
    "## PRODIGY AI HOCKUS POCKUS START\n",
    "\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "home = str(Path.home())\n",
    "file_name = \"esugj36b.h5\"\n",
    "wandb.config.update({'dataset': file_name})\n",
    "path = home + \"/ProdigyAI/data/preprocessed/\" + file_name\n",
    "h5f = h5py.File(path, \"r\")\n",
    "\n",
    "prices_for_window_index_array_train = h5f[\n",
    "    \"prices_for_window_index_array_train\"][:]\n",
    "\n",
    "prices_for_window_index_array_val = h5f[\"prices_for_window_index_array_val\"][:]\n",
    "\n",
    "prices_for_window_index_array_test = h5f[\n",
    "    \"prices_for_window_index_array_test\"][:]\n",
    "\n",
    "close_index_array_train = h5f[\"close_index_array_train\"][:]\n",
    "close_index_array_val = h5f[\"close_index_array_val\"][:]\n",
    "close_index_array_test = h5f[\"close_index_array_test\"][:]\n",
    "input_features_normalized_train = h5f[\"input_features_normalized_train\"][:]\n",
    "input_features_normalized_val = h5f[\"input_features_normalized_val\"][:]\n",
    "input_features_normalized_test = h5f[\"input_features_normalized_test\"][:]\n",
    "y_train = h5f[\"y_train\"][:]\n",
    "y_val = h5f[\"y_val\"][:]\n",
    "y_test = h5f[\"y_test\"][:]\n",
    "h5f.close()\n",
    "\n",
    "for i in range(len(close_index_array_train)):\n",
    "    if close_index_array_train[i] == prices_for_window_index_array_train[0]:\n",
    "        offset = i\n",
    "        break\n",
    "\n",
    "@njit\n",
    "def generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                     input_features_normalized):\n",
    "    X = np.empty((batch_size, *dim))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        for k in prange(dim[0]):\n",
    "            for l in prange(len(input_features_normalized)):\n",
    "                X[i][k][l] = input_features_normalized[l][ID + k]\n",
    "    # X[i, ] = self.input_features_normalized[ID+self.window_length, ]\n",
    "    return X.reshape((X.shape[0], X.shape[1], X.shape[2], 1))\n",
    "\n",
    "@njit\n",
    "def generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data):\n",
    "    y = np.empty((batch_size, n_classes))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        y[i, :] = y_data[ID]\n",
    "    return y\n",
    "\n",
    "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
    "    def __init__(self,\n",
    "                 checkpoint_path,\n",
    "                 prices_for_window_index_array,\n",
    "                 input_features_normalized,\n",
    "                 y_data,\n",
    "                 batch_size,\n",
    "                 dim,\n",
    "                 n_classes,\n",
    "                 to_fit,\n",
    "                 shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.prices_for_window_index_array = prices_for_window_index_array\n",
    "        self.input_features_normalized = input_features_normalized\n",
    "        self.labels = y_data\n",
    "        self.y_data = y_data\n",
    "        self.to_fit = to_fit\n",
    "        self.n_classes = n_classes\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.n = 0\n",
    "        self.list_IDs = np.arange(len(self.prices_for_window_index_array))\n",
    "        self.on_epoch_end()\n",
    "    def __next__(self):\n",
    "        # Get one batch of data\n",
    "        data = self.__getitem__(self.n)\n",
    "        # Batch index\n",
    "        self.n += 1\n",
    "        # If we have processed the entire dataset then\n",
    "        if self.n >= self.__len__():\n",
    "            self.on_epoch_end\n",
    "            self.n = 0\n",
    "        return data\n",
    "    def __len__(self):\n",
    "        # Return the number of batches of the dataset\n",
    "        return math.ceil(len(self.indexes) / self.batch_size)\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                               self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X = self._generate_x(list_IDs_temp)\n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(list_IDs_temp)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    def on_epoch_end(self):\n",
    "        wandb.save(self.checkpoint_path)\n",
    "        self.indexes = np.arange(len(self.prices_for_window_index_array))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def _generate_x(self, list_IDs_temp):\n",
    "        dim = self.dim\n",
    "        batch_size = self.batch_size\n",
    "        input_features_normalized = self.input_features_normalized\n",
    "        X = generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                             input_features_normalized)\n",
    "        return X\n",
    "    def _generate_y(self, list_IDs_temp):\n",
    "        batch_size = self.batch_size\n",
    "        n_classes = self.n_classes\n",
    "        y_data = self.y_data\n",
    "        y = generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data)\n",
    "        return y\n",
    "\n",
    "n_classes = 3\n",
    "dim = (window_length, num_features)\n",
    "\n",
    "to_fit = True\n",
    "# for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,\n",
    "#                                       input_features_normalized_train,\n",
    "#                                       y_train,\n",
    "#                                       batch_size,\n",
    "#                                       dim,\n",
    "#                                       n_classes,\n",
    "#                                       to_fit,\n",
    "#                                       shuffle=True):\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "# for X_batch, y_batch in generator(X_train, y_train, 64):\n",
    "#     print(\"ok\")\n",
    "## PRODIGY AI HOCKUS POCKUS END\n",
    "\n",
    "checkpoint_path = path_adjust + \"temp/cp.ckpt\"\n",
    "# Create a callback that saves the model's weights\n",
    "\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                              save_weights_only=True,\n",
    "                              verbose=1)\n",
    "\n",
    "check_point_file = Path(checkpoint_path)\n",
    "\n",
    "if check_point_file.exists() and resuming == \"resuming\":\n",
    "    print(\"weights loaded\")\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "train_generator = DataGenerator(checkpoint_path,\n",
    "                                prices_for_window_index_array_train,\n",
    "                                input_features_normalized_train,\n",
    "                                y_train,\n",
    "                                batch_size,\n",
    "                                dim,\n",
    "                                n_classes,\n",
    "                                to_fit,\n",
    "                                shuffle=True)\n",
    "\n",
    "val_generator = DataGenerator(checkpoint_path,\n",
    "                              prices_for_window_index_array_val,\n",
    "                              input_features_normalized_val,\n",
    "                              y_val,\n",
    "                              batch_size,\n",
    "                              dim,\n",
    "                              n_classes,\n",
    "                              to_fit,\n",
    "                              shuffle=True)\n",
    "\n",
    "steps_per_epoch = len(train_generator)\n",
    "validation_steps = len(val_generator)\n",
    "\n",
    "if wandb.run.resumed:\n",
    "    wandb.restore(\"model-best.h5\",run_path=\"garthtrickett/prodigyai/\" + wandb.run.id)\n",
    "    # restore the best model\n",
    "    deeplob = tensorflow.keras.models.load_model(wandb.restore(\"model-best.h5\").name)\n",
    "    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)\n",
    "    deeplob.compile(optimizer=adam,\n",
    "                    loss=\"categorical_crossentropy\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "else:\n",
    "    ### Model Architecture\n",
    "    def create_deeplob(T, NF, number_of_lstm):\n",
    "        input_lmd = Input(shape=(T, NF, 1))\n",
    "        # build the convolutional block\n",
    "        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (1, 10))(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        conv_first1 = Conv2D(32, (4, 1), padding=\"same\")(conv_first1)\n",
    "        conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "        # build the inception module\n",
    "        convsecond_1 = Conv2D(64, (1, 1), padding=\"same\")(conv_first1)\n",
    "        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "        convsecond_1 = Conv2D(64, (3, 1), padding=\"same\")(convsecond_1)\n",
    "        convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "        convsecond_2 = Conv2D(64, (1, 1), padding=\"same\")(conv_first1)\n",
    "        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "        convsecond_2 = Conv2D(64, (5, 1), padding=\"same\")(convsecond_2)\n",
    "        convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "        convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1),\n",
    "                                    padding=\"same\")(conv_first1)\n",
    "        convsecond_3 = Conv2D(64, (1, 1), padding=\"same\")(convsecond_3)\n",
    "        convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)\n",
    "        convsecond_output = concatenate(\n",
    "            [convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
    "        # use the MC dropout here\n",
    "        conv_reshape = Reshape(\n",
    "            (int(convsecond_output.shape[1]),\n",
    "             int(convsecond_output.shape[3])))(convsecond_output)\n",
    "        conv_lstm = layers.LSTM(number_of_lstm)(conv_reshape)\n",
    "        # build the output layer\n",
    "        out = Dense(3, activation=\"softmax\")(conv_lstm)\n",
    "        model = Model(inputs=input_lmd, outputs=out)\n",
    "        adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1)\n",
    "        model.compile(optimizer=adam,\n",
    "                      loss=\"categorical_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "        return model\n",
    "    deeplob = create_deeplob(window_length, num_features, number_of_lstm)\n",
    "\n",
    "deeplob.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[cp_callback,\n",
    "               WandbCallback(save_model=True, monitor=\"loss\")])\n",
    "\n",
    "finished_weights_path = path_adjust + \"temp/cp_end.ckpt\"\n",
    "deeplob.save_weights(finished_weights_path)\n",
    "\n",
    "print(\"script finished\")\n",
    "# deeplob = create_deeplob(200, 40, 64)\n",
    "# deeplob.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     epochs=1000,\n",
    "#     batch_size=64,\n",
    "#     verbose=2,\n",
    "#     validation_data=(X_val, y_val),\n",
    "# )\n",
    "## THIS SHOULD BE COMBINED WITH LSTM LAYERS WITH stateful=True\n",
    "# deeplob = create_deeplob(200, 40, 64)\n",
    "# for i in range(200):\n",
    "#     deeplob.fit(\n",
    "#         X_train,\n",
    "#         y_train,\n",
    "#         epochs=1,\n",
    "#         batch_size=64,\n",
    "#         verbose=2,\n",
    "#         validation_data=(X_val, y_val),\n",
    "#         shuffle=False,\n",
    "#     )\n",
    "#     deeplob.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

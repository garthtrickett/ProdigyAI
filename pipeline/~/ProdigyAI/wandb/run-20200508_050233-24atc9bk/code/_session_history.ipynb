{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 0, 3)"
     ]
    }
   ],
   "source": [
    "print(\"script initiated\")\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from numba import njit, prange\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import keras\n",
    "import numpy as np\n",
    "from third_party_libraries.TABL import Models\n",
    "from third_party_libraries.keras_lr_finder.lr_finder import LRFinder\n",
    "from third_party_libraries.CLR.clr_callbacks import CyclicLR\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Init wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'~/ProdigyAI/yaml/tabl.yaml'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "yaml_path = \"~/ProdigyAI/yaml/tabl.yaml\"\n",
    "yaml.load(yaml_path, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'~/ProdigyAI/yaml/tabl.yaml'"
     ]
    }
   ],
   "source": [
    "yaml.load(yaml_path, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dict = yaml.load(yaml_path, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'~/ProdigyAI/yaml/tabl.yaml'"
     ]
    }
   ],
   "source": [
    "a_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = \"~/ProdigyAI/yaml/tabl.yaml\"\n",
    "\n",
    "with open(r\"~/ProdigyAI/yaml/tabl.yaml\") as file:\n",
    "    a_dict = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = \"~/ProdigyAI/yaml/tabl.yaml\"\n",
    "\n",
    "with open(\"~/ProdigyAI/yaml/tabl.yaml\") as file:\n",
    "    a_dict = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/garthtrickett/ProdigyAI/yaml/tabl.yaml\") as file:\n",
    "    a_dict = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/garthtrickett/ProdigyAI/pipeline'"
     ]
    }
   ],
   "source": [
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"script initiated\")\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from numba import njit, prange\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import keras\n",
    "import numpy as np\n",
    "from third_party_libraries.TABL import Models\n",
    "from third_party_libraries.keras_lr_finder.lr_finder import LRFinder\n",
    "from third_party_libraries.CLR.clr_callbacks import CyclicLR\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Init wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import yaml\n",
    "# import tensorflow.keras as keras\n",
    "# import tensorflow.keras.layers as layers\n",
    "# import tensorflow.keras.backend as K\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    args = parser.parse_args()\n",
    "    if args.resuming != None:\n",
    "        resuming = args.resuming\n",
    "    else:\n",
    "        resuming = \"NA\"\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "yaml_path = \"~/ProdigyAI/yaml/tabl.yaml\"\n",
    "\n",
    "with open(\"yaml/tabl.yaml\") as file:\n",
    "    a_dict = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_adjust = cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_adjust + \"/yaml/tabl.yaml\") as file:\n",
    "    a_dict = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/garthtrickett/ProdigyAI/pipeline'"
     ]
    }
   ],
   "source": [
    "path_adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/garthtrickett/ProdigyAI/pipeline'"
     ]
    }
   ],
   "source": [
    "path_adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"script initiated\")\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from numba import njit, prange\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import keras\n",
    "import numpy as np\n",
    "from third_party_libraries.TABL import Models\n",
    "from third_party_libraries.keras_lr_finder.lr_finder import LRFinder\n",
    "from third_party_libraries.CLR.clr_callbacks import CyclicLR\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Init wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import yaml\n",
    "# import tensorflow.keras as keras\n",
    "# import tensorflow.keras.layers as layers\n",
    "# import tensorflow.keras.backend as K\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    args = parser.parse_args()\n",
    "    if args.resuming != None:\n",
    "        resuming = args.resuming\n",
    "    else:\n",
    "        resuming = \"NA\"\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../'"
     ]
    }
   ],
   "source": [
    "path_adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_adjust + \"yaml/tabl.yaml\") as file:\n",
    "    a_dict = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"script initiated\")\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from numba import njit, prange\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import keras\n",
    "import numpy as np\n",
    "from third_party_libraries.TABL import Models\n",
    "from third_party_libraries.keras_lr_finder.lr_finder import LRFinder\n",
    "from third_party_libraries.CLR.clr_callbacks import CyclicLR\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Init wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import yaml\n",
    "# import tensorflow.keras as keras\n",
    "# import tensorflow.keras.layers as layers\n",
    "# import tensorflow.keras.backend as K\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    args = parser.parse_args()\n",
    "    if args.resuming != None:\n",
    "        resuming = args.resuming\n",
    "    else:\n",
    "        resuming = \"NA\"\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "with open(path_adjust + \"yaml/tabl.yaml\") as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "wandb.init(\n",
    "    dir=\"~/ProdigyAI/\",\n",
    "    project=\"prodigyai\",\n",
    "    config=config_dictionary,\n",
    ")\n",
    "\n",
    "wandb.config.update({'dataset': 'ab131'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 0, 3)"
     ]
    }
   ],
   "source": [
    "print(\"script initiated\")\n",
    "import time\n",
    "very_start = time.time()\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import tables as tb\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from multiprocessing import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)  # don't use scientific notati\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "# from hanging_threads import start_monitoring\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler, StandardScaler\n",
    "import keras\n",
    "import third_party_libraries.finance_ml\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab as ml\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab.labeling.labeling as labeling\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab.sample_weights.attribution as attribution\n",
    "import third_party_libraries.snippets as snp\n",
    "from third_party_libraries.finance_ml.stats.vol import *\n",
    "\n",
    "from library.core import *\n",
    "# monitoring_thread = start_monitoring(seconds_frozen=360, test_interval=100)\n",
    "# import googlecloudprofiler\n",
    "# try:\n",
    "#     googlecloudprofiler.start(\n",
    "#         service=\"preemp-cpu-big-full-jeff_in-max\",\n",
    "#         # verbose is the logging level. 0-error, 1-warning, 2-info,\n",
    "#         # 3-debug. It defaults to 0 (error) if not set.\n",
    "#         verbose=3,\n",
    "#     )\n",
    "# except (ValueError, NotImplementedError) as exc:\n",
    "#     print(exc)  # Handle errors here\n",
    "\n",
    "arg_parse_stage = None\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--is_finished\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    parser.add_argument(\"-r\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    args = parser.parse_args()\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "try:\n",
    "    with open(path_adjust + \"temp/data_name_gpu.txt\", \"r\") as text_file:\n",
    "        gpu_file_name = text_file.read()\n",
    "        stage = 2\n",
    "except:\n",
    "    stage = 1\n",
    "\n",
    "side = None\n",
    "\n",
    "with open(path_adjust + \"temp/is_script_finished.txt\", \"w+\") as text_file:\n",
    "    text_file.write(\"start_script_time\" + str(very_start))\n",
    "\n",
    "if arg_parse_stage == 1:\n",
    "    stage = int(args.stage)\n",
    "\n",
    "print(\"the stage\" + str(stage))\n",
    "# Overide model and stage for testing\n",
    "\n",
    "model = \"two_model\"\n",
    "stage = 1\n",
    "print(\"the overidden stage\" + str(stage))\n",
    "\n",
    "if stage == 2:\n",
    "    # size\n",
    "    h5f = h5py.File(\"data/gpu_output/\" + gpu_file_name + \".h5\", \"r\")\n",
    "    X = h5f[\"X\"][:]\n",
    "    P = h5f[\"P\"][:]\n",
    "    sample_weights = h5f[\"sample_weights\"][:]\n",
    "    sampled_idx_epoch = h5f[\"sampled_idx_epoch\"][:]\n",
    "    h5f.close()\n",
    "    data = pq.read_pandas(\"data/gpu_output/\" + gpu_file_name +\n",
    "                          \"_data.parquet\").to_pandas()\n",
    "    X_for_all_labels = data.dropna(subset=[\"bins\"])\n",
    "    sampled_idx = pd.DatetimeIndex(sampled_idx_epoch)\n",
    "    X_for_all_labels[\"predicted_bins\"] = P\n",
    "    side = X_for_all_labels[\"predicted_bins\"]\n",
    "    # Could use the probabilities (instead of [1,0,0] use [0.2,0.55,0.25]\n",
    "\n",
    "import yaml\n",
    "import wandb\n",
    "yaml_path = path_adjust + \"yaml/preprocessing.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "wandb.init(\n",
    "    dir=\"~/ProdigyAI/\",\n",
    "    project=\"prodigyai\",\n",
    "    config=config_dictionary,\n",
    ")\n",
    "\n",
    "minimum_return = eval(wandb.config['params']['minimum_return']['value'])\n",
    "\n",
    "vertical_barrier_seconds = eval(\n",
    "    wandb.config['params']['vertical_barrier_seconds']['value'])\n",
    "\n",
    "volume_max = (\n",
    "    minimum_return + wandb.config['params']['vol_max_modifier']['value']\n",
    ")  # The higher this is the more an increase in volatility requries an increase\n",
    "# in return to be considered buy/sell (Increasing this increases end barrier vertical touches)\n",
    "\n",
    "volume_min = minimum_return + wandb.config['params']['vol_min_modifier'][\n",
    "    'value']\n",
    "\n",
    "filter_type = wandb.config['params']['filter_type']['value']\n",
    "\n",
    "if filter_type == \"cm\":\n",
    "    cusum_filter_vol_modifier = wandb.config['params'][\n",
    "        'cusum_filter_volume_modifier']['value']\n",
    "else:\n",
    "    cusum_filter_vol_modifier = 0\n",
    "\n",
    "use_sample_weights = wandb.config['params']['use_sample_weights']['value']\n",
    "\n",
    "use_sample_weights = wandb.config['params']['use_fractional_differentiation'][\n",
    "    'value']\n",
    "\n",
    "input_type = wandb.config['params']['input_type']['value']\n",
    "# parameters[\"ntb\"] = True  # non time bars\n",
    "# if parameters[\"ntb\"] == True:\n",
    "#     # Pick whether you want to add in the time since last bar input feature\n",
    "#     # time since last bar column\n",
    "#     parameters[\"tslbc\"] = True  # time since last bar column\n",
    "# else:\n",
    "#     # Pick whether you want to add in the volume input feature\n",
    "#     parameters[\"vbc\"] = True  # volume bar column\n",
    "# Create the txt file string\n",
    "\n",
    "parameter_string = wandb.run.id\n",
    "\n",
    "pt_sl = [\n",
    "    wandb.config['params']['profit_taking_multiplier']['value'],\n",
    "    wandb.config['params']['stop_loss_multiplier']['value']\n",
    "]\n",
    "\n",
    "cpus = cpu_count() - 1\n",
    "regenerate_features_and_labels = True\n",
    "\n",
    "if regenerate_features_and_labels == True:\n",
    "    # READ THE DATA\n",
    "    if stage == 1:\n",
    "        # Side\n",
    "        print(\"starting data load\")\n",
    "        head = wandb.config['params']['head']['value']\n",
    "        # # read parquet file of dollar bars\n",
    "        if input_type == \"bars\":\n",
    "            # Mlfinlab bars\n",
    "            data = pq.read_pandas(\n",
    "                path_adjust + \"data/bars/\"\n",
    "                \"btcusdt_agg_trades_50_volume_bars.parquet\").to_pandas()\n",
    "            data = data.drop(columns=[\n",
    "                \"open\",\n",
    "                \"high\",\n",
    "                \"low\",\n",
    "                # \"volume\",\n",
    "                \"seconds_since_last_bar\",\n",
    "            ])\n",
    "            # 1 min ohlcv ready made bars\n",
    "            # data = pq.read_pandas(\"data/bars/BTCUSDT_1m.parquet\").to_pandas()\n",
    "            # data[\"date_time\"] = pd.to_datetime(data[\"date_time\"], unit='ms')\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "            data = data.set_index(\"date_time\")\n",
    "            data.index = pd.to_datetime(data.index, infer_datetime_format=True)\n",
    "        # read parquet file of raw ticks\n",
    "        if input_type == \"ticks\":\n",
    "            data = pq.read_pandas(\n",
    "                path_adjust + \"data/bars/\" +\n",
    "                \"btcusdt_agg_trades_raw_tick_data.parquet\").to_pandas()\n",
    "            data = data.rename(columns={\n",
    "                \"date\": \"date_time\",\n",
    "                \"price\": \"close\",\n",
    "                \"volume\": \"volume\"\n",
    "            })\n",
    "            data = data.drop(columns=[\"volume\"])\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "            data = data.set_index(\"date_time\")\n",
    "            print(\"converting index to date_time\")\n",
    "            data.index = pd.to_datetime(data.index,\n",
    "                                        format=\"%m/%d/%Y %H:%M:%S.%f\")\n",
    "            print(\"index converted\")\n",
    "            # Should do something else than drop the duplicates (maybe trades doesnt have duplicate indexes rather than aggtrades)\n",
    "            data = data.loc[~data.index.duplicated(keep=\"first\")]\n",
    "        if input_type == \"orderbook\":\n",
    "            with open(path_adjust + \"temp/orderbook_data_name.txt\",\n",
    "                      \"r\") as text_file:\n",
    "                orderbook_preprocessed_file_name = text_file.read()\n",
    "            h5f = h5py.File(\n",
    "                path_adjust + \"data/orderbook_preprocessed/\" +\n",
    "                orderbook_preprocessed_file_name + \".h5\",\n",
    "                \"r\",\n",
    "            )\n",
    "            volumes_index_as_epoch = h5f[\"volumes_index_as_epoch\"][:]\n",
    "            volumes_np_array = h5f[\"volumes_np_array\"][:]\n",
    "            df_index_as_epoch = h5f[\"df_index_as_epoch\"][:]\n",
    "            df_np_array = h5f[\"df_np_array\"][:]\n",
    "            h5f.close()\n",
    "            volumes = pd.DataFrame(data=volumes_np_array,\n",
    "                                   index=volumes_index_as_epoch)\n",
    "            volumes.index = pd.to_datetime(volumes.index, unit=\"ms\")\n",
    "            data = pd.DataFrame(data=df_np_array, index=df_index_as_epoch)\n",
    "            data.index = pd.to_datetime(data.index, unit=\"ms\")\n",
    "            data.columns = [\"close\"]\n",
    "            data.index.name = \"date_time\"\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "        print(\"data load finished\")\n",
    "        # Checking for duplicates\n",
    "        # duplicate_fast_search(data.index.duplicated())\n",
    "        # Fractional differentiation\n",
    "        if use_sample_weights == \"on\":\n",
    "            data_series = data[\"close\"].to_frame()\n",
    "            # # generate 100 points\n",
    "            # nsample = 1000\n",
    "            # ## simulate a simple sinusoidal function\n",
    "            # x1 = np.linspace(0, 10, nsample)\n",
    "            # y = pd.Series(1*np.sin(2.0 * x1 + .5))\n",
    "            # y.plot()\n",
    "            # c_constant = 1.\n",
    "            # y_shifted = (y + c_constant).cumsum().rename('Shifted_series').to_frame()\n",
    "            # y_shifted.plot()\n",
    "            # df = y_shifted\n",
    "            # # df=(df-df.mean())/df.std()\n",
    "            # df['Shifted_series'][1:] = np.diff(df['Shifted_series'].values)\n",
    "            # df['Shifted_series'].plot()\n",
    "            kwargs = None\n",
    "            # data_series = np.log(data_series)  ## is it good to log this?\n",
    "            frac_diff_series, d = get_opt_d(  # reduces the number of rows and ends up with less vertical barriers touched\n",
    "                data_series,\n",
    "                ds=None,\n",
    "                maxlag=None,  # If we use raw tick data need at least head > 8000\n",
    "                thres=1e-5,\n",
    "                max_size=10000,\n",
    "                p_thres=1e-2,\n",
    "                autolag=None,\n",
    "                verbose=1,\n",
    "            )\n",
    "            data[\"close\"] = frac_diff_series\n",
    "            data = data.dropna(subset=[\"close\"])\n",
    "        data[\"window_volatility_level\"] = np.nan\n",
    "        start = time.time()\n",
    "        volatility_level_array = volatility_levels_numba(\n",
    "            np.ascontiguousarray(data.close.values),\n",
    "            wandb.config['params']['window_length']['value'])\n",
    "        data[\"window_volatility_level\"] = volatility_level_array\n",
    "        # Should adjust the max value\n",
    "        # To get more vertical touches we can\n",
    "        # either increase vol_max or\n",
    "        # decrease the window seconds\n",
    "        scaler = MinMaxScaler(feature_range=(volume_min,\n",
    "                                             volume_max))  # normalization\n",
    "        normed_window_volatility_level = scaler.fit_transform(\n",
    "            data[[\"window_volatility_level\"]])\n",
    "        data[\"window_volatility_level\"] = normed_window_volatility_level  #\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "        # CUSUM FILTER\n",
    "        volatility_threshold = data[\"window_volatility_level\"].mean()\n",
    "        close_copy = data.dropna().close.copy(deep=True)\n",
    "        close_np_array, close_index_np_array = pandas_series_to_numba_ready_np_arrays(\n",
    "            close_copy)\n",
    "        volatility_threshold = volatility_threshold * cusum_filter_vol_modifier\n",
    "        print(\"data_len = \" + str(len(data)))\n",
    "        start = time.time()\n",
    "        sampled_idx = filter_events(\n",
    "            data,\n",
    "            close_np_array,\n",
    "            close_index_np_array,\n",
    "            volatility_threshold,\n",
    "            filter_type,\n",
    "        )\n",
    "        print(\"sampled_idx_len = \" + str(len(sampled_idx)))\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "    if stage == 2:\n",
    "        # size\n",
    "        start = time.time()\n",
    "        volatility_level_array = volatility_levels_numba(\n",
    "            data.close.values,\n",
    "            wandb.config['params']['window_length']['value'])\n",
    "        data[\"window_volatility_level\"] = volatility_level_array\n",
    "    # This code runs for both first and second stage preprocessing\n",
    "    start = time.time()\n",
    "    vertical_barrier_timestamps = ml.labeling.add_vertical_barrier(\n",
    "        t_events=sampled_idx,\n",
    "        close=data[\"close\"],\n",
    "        num_seconds=vertical_barrier_seconds)\n",
    "    end = time.time()\n",
    "    print(\"vertical barrier\" + str(end - start))\n",
    "    start = time.time()\n",
    "    print(\"Getting triple barrier events\")\n",
    "    triple_barrier_events = ml.labeling.get_events(\n",
    "        close=data[\"close\"],\n",
    "        t_events=sampled_idx,\n",
    "        pt_sl=pt_sl,\n",
    "        target=data[\"window_volatility_level\"],\n",
    "        min_ret=minimum_return,\n",
    "        num_threads=cpus * 2,\n",
    "        vertical_barrier_times=vertical_barrier_timestamps,\n",
    "        side_prediction=side,\n",
    "        split_by=wandb.config['params']['split_by']\n",
    "        ['value']  # maybe we want this as large as we can while still fitting in ram\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"triple_barrier_events finished taking\" + str(end - start))\n",
    "    very_end = time.time()\n",
    "    with open(path_adjust + \"temp/is_script_finished.txt\", \"w+\") as text_file:\n",
    "        text_file.write(\"full_script_time\" + str(very_end - very_start))\n",
    "    start_time = time.time()\n",
    "    print(\"Returning Bins\")\n",
    "    labels = ml.labeling.get_bins(triple_barrier_events, data[\"close\"])\n",
    "    labels = ml.labeling.drop_labels(labels)\n",
    "    label_counts = labels.bin.value_counts()\n",
    "    print(\"label_counts\" + str(label_counts))\n",
    "    end_time = time.time()\n",
    "    print(\"returning bins finished taking\" + str(end_time - start_time))\n",
    "    # unique, counts = np.unique(y, return_counts=True)\n",
    "    sampled_idx_epoch = sampled_idx.astype(np.int64)\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/sampled_idx_epoch.h5\",\n",
    "        \"w\")\n",
    "    h5f.create_dataset(\"sampled_idx_epoch\", data=sampled_idx_epoch)\n",
    "    h5f.close()\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(labels)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/labels.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(data)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/data.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(triple_barrier_events)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/triple_barrier_events.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "else:\n",
    "    labels = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/labels.parquet\").to_pandas()\n",
    "    data = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/data.parquet\").to_pandas()\n",
    "    triple_barrier_events = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/triple_barrier_events.parquet\"\n",
    "    ).to_pandas()\n",
    "    with open(path_adjust + \"temp/orderbook_data_name.txt\", \"r\") as text_file:\n",
    "        orderbook_preprocessed_file_name = text_file.read()\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/orderbook_preprocessed/\" +\n",
    "        orderbook_preprocessed_file_name + \".h5\", \"r\")\n",
    "    volumes_index_as_epoch = h5f[\"volumes_index_as_epoch\"][:]\n",
    "    volumes_np_array = h5f[\"volumes_np_array\"][:]\n",
    "    h5f.close()\n",
    "    volumes = pd.DataFrame(data=volumes_np_array, index=volumes_index_as_epoch)\n",
    "    volumes.index = pd.to_datetime(volumes.index, unit=\"ms\")\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/sampled_idx_epoch.h5\",\n",
    "        \"r\")\n",
    "    sampled_idx_epoch = h5f[\"sampled_idx_epoch\"][:]\n",
    "    h5f.close()\n",
    "    sampled_idx = pd.DatetimeIndex(sampled_idx_epoch)\n",
    "\n",
    "if stage == 1:\n",
    "    # Get why from labels\n",
    "    y_dataframe = labels[\"bin\"]\n",
    "    data[\"bins\"] = labels[\"bin\"]\n",
    "    y = np.asarray(y_dataframe)\n",
    "    start_time = time.time()\n",
    "    # side\n",
    "    X_for_all_labels = data.loc[labels.index, :]\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    ### FOR HIGHWAY RNN\n",
    "    X = np.asarray(volumes.loc[labels.index, :])\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/preprocessed/\" + parameter_string + \"_gam_rhn.h5\",\n",
    "        \"w\")\n",
    "    h5f.create_dataset(\"X\", data=X)\n",
    "    h5f.create_dataset(\"y\", data=y)\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1y7dtrfu'"
     ]
    }
   ],
   "source": [
    "parameter_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"script initiated\")\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from numba import njit, prange\n",
    "import math\n",
    "import numpy as np\n",
    "from third_party_libraries.TABL import Models\n",
    "from third_party_libraries.keras_lr_finder.lr_finder import LRFinder\n",
    "from third_party_libraries.CLR.clr_callbacks import CyclicLR\n",
    "import keras\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf\n",
    "# Init wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import yaml\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "    resuming = \"NA\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    args = parser.parse_args()\n",
    "    if args.resuming != None:\n",
    "        resuming = args.resuming\n",
    "    else:\n",
    "        resuming = \"NA\"\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "yaml_path = path_adjust + \"yaml/tabl.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "try:\n",
    "    with open(\"temp/run_in_progress.txt\", \"r\") as text_file:\n",
    "        run_in_progress = text_file.read()\n",
    "        resume = True\n",
    "except:\n",
    "    resume = False\n",
    "\n",
    "wandb.init(dir=\"~/ProdigyAI/\",\n",
    "           project=\"prodigyai\",\n",
    "           config=config_dictionary,\n",
    "           resume=resume)\n",
    "\n",
    "window_length = wandb.config['params']['window_length']['value']\n",
    "num_features = wandb.config['params']['num_features']['value']\n",
    "epochs = wandb.config['params']['epochs']['value']\n",
    "batch_size = wandb.config['params']['batch_size']['value']\n",
    "\n",
    "dropout = wandb.config['params']['dropout']['value']\n",
    "# # try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes\n",
    "# # example data\n",
    "# example_x = np.random.rand(1000, 40, 10)\n",
    "# np.min(example_x)\n",
    "# np.max(example_x)\n",
    "# np.mean(example_x)\n",
    "# example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)\n",
    "## PRODIGY AI HOCKUS POCKUS START\n",
    "\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "home = str(Path.home())\n",
    "file_name = \"1fdpldp4.h5\"\n",
    "wandb.config.update({'dataset': file_name})\n",
    "path = home + \"/ProdigyAI/data/preprocessed/\" + file_name\n",
    "h5f = h5py.File(path, \"r\")\n",
    "\n",
    "prices_for_window_index_array_train = h5f[\n",
    "    \"prices_for_window_index_array_train\"][:]\n",
    "\n",
    "prices_for_window_index_array_val = h5f[\"prices_for_window_index_array_val\"][:]\n",
    "\n",
    "prices_for_window_index_array_test = h5f[\n",
    "    \"prices_for_window_index_array_test\"][:]\n",
    "\n",
    "close_index_array_train = h5f[\"close_index_array_train\"][:]\n",
    "close_index_array_val = h5f[\"close_index_array_val\"][:]\n",
    "close_index_array_test = h5f[\"close_index_array_test\"][:]\n",
    "input_features_normalized_train = h5f[\"input_features_normalized_train\"][:]\n",
    "input_features_normalized_val = h5f[\"input_features_normalized_val\"][:]\n",
    "input_features_normalized_test = h5f[\"input_features_normalized_test\"][:]\n",
    "y_train = h5f[\"y_train\"][:]\n",
    "y_val = h5f[\"y_val\"][:]\n",
    "y_test = h5f[\"y_test\"][:]\n",
    "h5f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

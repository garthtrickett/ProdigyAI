diff --git a/pipeline/tabl.py b/pipeline/tabl.py
index f0244ec..98e4ac3 100644
--- a/pipeline/tabl.py
+++ b/pipeline/tabl.py
@@ -90,12 +90,25 @@ if cwd == home + "/":
 yaml_path = path_adjust + "yaml/tabl.yaml"
 with open(yaml_path) as file:
     yaml_dict = yaml.load(file, Loader=yaml.FullLoader)
-
 config_dictionary = dict(yaml=yaml_path, params=yaml_dict)
+
+try:
+    with open("temp/run_name_and_status.txt", "r") as text_file:
+        run_name_and_status = text_file.read()
+    if "finished" not in run_name_and_status:
+        resume == True
+    else:
+        resume == False
+except:
+    resume == False
+
+import pdb
+pdb.set_trace()
+
 wandb.init(dir="~/ProdigyAI/",
            project="prodigyai",
            config=config_dictionary,
-           resume=True)
+           resume=resume)
 
 window_length = wandb.config['params']['window_length']['value']
 num_features = wandb.config['params']['num_features']['value']
@@ -440,8 +453,6 @@ model = Models.TABL(
 )
 model.summary()
 
-
-
 # Compile the model
 model.compile(loss=loss_function, optimizer=optimizer, metrics=["accuracy"])
 # Create a callback that saves the model's weights
@@ -490,17 +501,27 @@ val_generator = DataGenerator(checkpoint_path,
 steps_per_epoch = len(train_generator)
 validation_steps = len(val_generator)
 
+import pdb
+pdb.set_trace()
+
+with open(path_adjust + "temp/run_name_and_status.txt", "w+") as text_file:
+    text_file.write(wandb.run.id)
+
 # example sata training
 model.fit(
     train_generator,
     steps_per_epoch=steps_per_epoch,
     validation_steps=validation_steps,
-    epochs=epochs,
+    epochs=10000,
     validation_data=val_generator,
     callbacks=[cp_callback,
                WandbCallback(save_model=True, monitor="loss")])
 
 model.save(os.path.join(wandb.run.dir, "model.h5"))
+
+with open(path_adjust + "temp/run_name_and_status.txt", "w+") as text_file:
+    text_file.write(wandb.run.id + "___finished")
+
 # no class weight
 
 ## lob FI-2010 DATA PREPERATION
diff --git a/pipeline/~/ProdigyAI/wandb/wandb-resume.json b/pipeline/~/ProdigyAI/wandb/wandb-resume.json
deleted file mode 100644
index 6f8568f..0000000
--- a/pipeline/~/ProdigyAI/wandb/wandb-resume.json
+++ /dev/null
@@ -1 +0,0 @@
-{"run_id": "3g8o9tjh"}
\ No newline at end of file
diff --git a/temp/cp.ckpt b/temp/cp.ckpt
deleted file mode 100644
index b98d31b..0000000
Binary files a/temp/cp.ckpt and /dev/null differ
diff --git a/temp/cp_end.ckpt b/temp/cp_end.ckpt
deleted file mode 100644
index 62e0a67..0000000
Binary files a/temp/cp_end.ckpt and /dev/null differ
Submodule third_party_libraries/TABL contains modified content
Submodule third_party_libraries/TABL df3e92d..c573383:
diff --git a/third_party_libraries/TABL/Layers.py b/third_party_libraries/TABL/Layers.py
index 9020002..b299e57 100644
--- a/third_party_libraries/TABL/Layers.py
+++ b/third_party_libraries/TABL/Layers.py
@@ -3,8 +3,9 @@
 """
 @author: Dat Tran (dat.tranthanh@tut.fi)
 """
+import keras
 from keras import backend as K
-from keras.engine.topology import Layer
+from keras.layers import Layer
 from keras import activations as Activations
 from keras import initializers as Initializers
 
@@ -18,25 +19,25 @@ class Constraint(object):
 
     def get_config(self):
         return {}
-    
+
+
 class MinMax(Constraint):
     """
     Customized min-max constraint for scalar
     """
-
     def __init__(self, min_value=0.0, max_value=10.0):
         self.min_value = min_value
         self.max_value = max_value
+
     def __call__(self, w):
-        
-        return K.clip(w,self.min_value,self.max_value)
+
+        return K.clip(w, self.min_value, self.max_value)
 
     def get_config(self):
-        return {'min_value': self.min_value,
-                'max_value': self.max_value}
-        
+        return {'min_value': self.min_value, 'max_value': self.max_value}
 
-def nmodeproduct(x,w,mode):
+
+def nmodeproduct(x, w, mode):
     """
     n-mode product for 2D matrices
     x: NxHxW
@@ -45,74 +46,83 @@ def nmodeproduct(x,w,mode):
     
     output: NxhxW (mode1) or NxHxw (mode2)
     """
-    if mode==2:
-        x=K.dot(x,w)
+    if mode == 2:
+        x = K.dot(x, w)
     else:
-        x=K.permute_dimensions(x,(0,2,1))
-        x=K.dot(x,w)
-        x=K.permute_dimensions(x,(0,2,1))
+        x = K.permute_dimensions(x, (0, 2, 1))
+        x = K.dot(x, w)
+        x = K.permute_dimensions(x, (0, 2, 1))
     return x
 
+
 class BL(Layer):
     """
     Bilinear Layer
     """
-    def __init__(self, output_dim,
+    def __init__(self,
+                 output_dim,
                  kernel_regularizer=None,
-                 kernel_constraint=None,**kwargs):
+                 kernel_constraint=None,
+                 **kwargs):
         """
         output_dim : output dimensions of 2D tensor, should be a list of len 2, e.g. [30,20]
         kernel_regularizer : keras regularizer object
         kernel_constraint: keras constraint object
         """
-        
+
         self.output_dim = output_dim
-        self.kernel_regularizer=kernel_regularizer
-        self.kernel_constraint=kernel_constraint
-        
+        self.kernel_regularizer = kernel_regularizer
+        self.kernel_constraint = kernel_constraint
+
         super(BL, self).__init__(**kwargs)
 
     def build(self, input_shape):
-        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
-                                      initializer='he_uniform',
-                                      regularizer=self.kernel_regularizer,
-                                      constraint=self.kernel_constraint,
-                                      trainable=True)
-        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
-                                      initializer='he_uniform',
-                                      regularizer=self.kernel_regularizer,
-                                      constraint=self.kernel_constraint,
-                                      trainable=True)
-
-        self.bias=self.add_weight(name='bias',shape=(self.output_dim[0],self.output_dim[1]),
-                              initializer='zeros',trainable=True)
+        self.W1 = self.add_weight(name='W1',
+                                  shape=(input_shape[1], self.output_dim[0]),
+                                  initializer='he_uniform',
+                                  regularizer=self.kernel_regularizer,
+                                  constraint=self.kernel_constraint,
+                                  trainable=True)
+        self.W2 = self.add_weight(name='W2',
+                                  shape=(input_shape[2], self.output_dim[1]),
+                                  initializer='he_uniform',
+                                  regularizer=self.kernel_regularizer,
+                                  constraint=self.kernel_constraint,
+                                  trainable=True)
+
+        self.bias = self.add_weight(name='bias',
+                                    shape=(self.output_dim[0],
+                                           self.output_dim[1]),
+                                    initializer='zeros',
+                                    trainable=True)
         super(BL, self).build(input_shape)
 
     def call(self, x):
         print(K.int_shape(x))
-        x = nmodeproduct(x,self.W1,1)
-        x = nmodeproduct(x,self.W2,2)
-        x = K.bias_add(x,self.bias)
-        
-        if self.output_dim[1]==1:
-            x = K.squeeze(x,axis=-1)
+        x = nmodeproduct(x, self.W1, 1)
+        x = nmodeproduct(x, self.W2, 2)
+        x = K.bias_add(x, self.bias)
+
+        if self.output_dim[1] == 1:
+            x = K.squeeze(x, axis=-1)
         print(K.int_shape(x))
         return x
 
     def compute_output_shape(self, input_shape):
-        if self.output_dim[1]==1:
+        if self.output_dim[1] == 1:
             return (input_shape[0], self.output_dim[0])
         else:
             return (input_shape[0], self.output_dim[0], self.output_dim[1])
-        
-        
+
+
 class TABL(Layer):
     """
     Temporal Attention augmented Bilinear Layer
     https://arxiv.org/abs/1712.00975
     
     """
-    def __init__(self, output_dim,
+    def __init__(self,
+                 output_dim,
                  projection_regularizer=None,
                  projection_constraint=None,
                  attention_regularizer=None,
@@ -125,45 +135,52 @@ class TABL(Layer):
         attention_regularizer: keras regularizer object for attention matrix
         attention_constraint: keras constraint object for attention matrix
         """
-        
+
         self.output_dim = output_dim
         self.projection_regularizer = projection_regularizer
         self.projection_constraint = projection_constraint
         self.attention_regularizer = attention_regularizer
         self.attention_constraint = attention_constraint
-        
+
         super(TABL, self).__init__(**kwargs)
 
     def build(self, input_shape):
-        self.W1 = self.add_weight(name='W1',shape=(input_shape[1], self.output_dim[0]),
-                                      initializer='he_uniform',
-                                      regularizer=self.projection_regularizer,
-                                      constraint=self.projection_constraint,
-                                      trainable=True)
-        
-        self.W2 = self.add_weight(name='W2',shape=(input_shape[2], self.output_dim[1]),
-                                      initializer='he_uniform',
-                                      regularizer=self.projection_regularizer,
-                                      constraint=self.projection_constraint,
-                                      trainable=True)
-        
-        self.W = self.add_weight(name='W',shape=(input_shape[2], input_shape[2]),
-                                      initializer=Initializers.Constant(1.0/input_shape[2]),
-                                      regularizer=self.attention_regularizer,
-                                      constraint=self.attention_constraint,
-                                      trainable=True)
-        
-        self.alpha = self.add_weight(name='alpha',shape=(1,),
-                                      initializer=Initializers.Constant(0.5),
-                                      constraint=MinMax(),
-                                      trainable=True)
-
-
-        self.bias=self.add_weight(name='bias',shape=(1, self.output_dim[0],self.output_dim[1]),
-                              initializer='zeros',trainable=True)
-        
+        self.W1 = self.add_weight(name='W1',
+                                  shape=(input_shape[1], self.output_dim[0]),
+                                  initializer='he_uniform',
+                                  regularizer=self.projection_regularizer,
+                                  constraint=self.projection_constraint,
+                                  trainable=True)
+
+        self.W2 = self.add_weight(name='W2',
+                                  shape=(input_shape[2], self.output_dim[1]),
+                                  initializer='he_uniform',
+                                  regularizer=self.projection_regularizer,
+                                  constraint=self.projection_constraint,
+                                  trainable=True)
+
+        self.W = self.add_weight(name='W',
+                                 shape=(input_shape[2], input_shape[2]),
+                                 initializer=Initializers.Constant(
+                                     1.0 / input_shape[2]),
+                                 regularizer=self.attention_regularizer,
+                                 constraint=self.attention_constraint,
+                                 trainable=True)
+
+        self.alpha = self.add_weight(name='alpha',
+                                     shape=(1, ),
+                                     initializer=Initializers.Constant(0.5),
+                                     constraint=MinMax(),
+                                     trainable=True)
+
+        self.bias = self.add_weight(name='bias',
+                                    shape=(1, self.output_dim[0],
+                                           self.output_dim[1]),
+                                    initializer='zeros',
+                                    trainable=True)
+
         self.in_shape = input_shape
-        
+
         super(TABL, self).build(input_shape)
 
     def call(self, x):
@@ -174,27 +191,26 @@ class TABL(Layer):
         W: D2 x D2
         """
         # first mode projection
-        x = nmodeproduct(x,self.W1,1) # N x d1 x D2
-        # enforcing constant (1) on the diagonal 
-        W = self.W-self.W*K.eye(self.in_shape[2],dtype='float32')+K.eye(self.in_shape[2],dtype='float32')/self.in_shape[2]
-        # calculate attention 
-        attention = Activations.softmax(nmodeproduct(x,W,2),axis=-1) # N x d1 x D2
+        x = nmodeproduct(x, self.W1, 1)  # N x d1 x D2
+        # enforcing constant (1) on the diagonal
+        W = self.W - self.W * K.eye(self.in_shape[2], dtype='float32') + K.eye(
+            self.in_shape[2], dtype='float32') / self.in_shape[2]
+        # calculate attention
+        attention = Activations.softmax(nmodeproduct(x, W, 2),
+                                        axis=-1)  # N x d1 x D2
         # apply attention
-        x = self.alpha*x + (1.0 - self.alpha)*x*attention
+        x = self.alpha * x + (1.0 - self.alpha) * x * attention
         # second mode projection
-        x = nmodeproduct(x,self.W2,2)
+        x = nmodeproduct(x, self.W2, 2)
         # bias add
         x = x + self.bias
-        
-        if self.output_dim[1]==1:
-            x = K.squeeze(x,axis=-1)
+
+        if self.output_dim[1] == 1:
+            x = K.squeeze(x, axis=-1)
         return x
 
     def compute_output_shape(self, input_shape):
-        if self.output_dim[1]==1:
+        if self.output_dim[1] == 1:
             return (input_shape[0], self.output_dim[0])
         else:
             return (input_shape[0], self.output_dim[0], self.output_dim[1])
-
-
-
diff --git a/third_party_libraries/TABL/Models.py b/third_party_libraries/TABL/Models.py
index e2bf62c..d5c4c45 100644
--- a/third_party_libraries/TABL/Models.py
+++ b/third_party_libraries/TABL/Models.py
@@ -4,7 +4,6 @@
 @author: Dat Tran (dat.tranthanh@tut.fi)
 """
 
-
 from third_party_libraries.TABL import Layers
 import keras
 
@@ -74,7 +73,8 @@ def TABL(
 
     x = inputs
     for k in range(1, len(template) - 1):
-        x = Layers.BL(template[k], projection_regularizer, projection_constraint)(x)
+        x = Layers.BL(template[k], projection_regularizer,
+                      projection_constraint)(x)
         x = keras.layers.Activation("relu")(x)
         x = keras.layers.Dropout(dropout)(x)
 
@@ -94,4 +94,3 @@ def TABL(
     model.compile(optimizer, "categorical_crossentropy", ["acc"])
 
     return model
-
diff --git a/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc
index 12f31e7..ff0a753 100644
Binary files a/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc and b/third_party_libraries/TABL/__pycache__/Layers.cpython-37.pyc differ
diff --git a/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc
index d8950e3..523d214 100644
Binary files a/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc and b/third_party_libraries/TABL/__pycache__/Models.cpython-37.pyc differ
diff --git a/yaml/tabl.yaml b/yaml/tabl.yaml
index 456e287..a4361e3 100644
--- a/yaml/tabl.yaml
+++ b/yaml/tabl.yaml
@@ -1,7 +1,7 @@
 # sample config defaults file
 epochs:
   desc: Number of epochs to train over
-  value: 100
+  value: 200
 batch_size:
   desc: Size of each mini-batch
   value: 256

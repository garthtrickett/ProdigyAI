{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 0, 3)"
     ]
    }
   ],
   "source": [
    "print(\"script initiated\")\n",
    "import time\n",
    "very_start = time.time()\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import tables as tb\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from multiprocessing import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)  # don't use scientific notati\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "# from hanging_threads import start_monitoring\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler, StandardScaler\n",
    "import keras\n",
    "import third_party_libraries.finance_ml\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab as ml\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab.labeling.labeling as labeling\n",
    "import third_party_libraries.hudson_and_thames.mlfinlab.sample_weights.attribution as attribution\n",
    "import third_party_libraries.snippets as snp\n",
    "from third_party_libraries.finance_ml.stats.vol import *\n",
    "\n",
    "from library.core import *\n",
    "# monitoring_thread = start_monitoring(seconds_frozen=360, test_interval=100)\n",
    "# import googlecloudprofiler\n",
    "# try:\n",
    "#     googlecloudprofiler.start(\n",
    "#         service=\"preemp-cpu-big-full-jeff_in-max\",\n",
    "#         # verbose is the logging level. 0-error, 1-warning, 2-info,\n",
    "#         # 3-debug. It defaults to 0 (error) if not set.\n",
    "#         verbose=3,\n",
    "#     )\n",
    "# except (ValueError, NotImplementedError) as exc:\n",
    "#     print(exc)  # Handle errors here\n",
    "\n",
    "arg_parse_stage = None\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--is_finished\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    parser.add_argument(\"-r\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    args = parser.parse_args()\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "try:\n",
    "    with open(path_adjust + \"temp/data_name_gpu.txt\", \"r\") as text_file:\n",
    "        gpu_file_name = text_file.read()\n",
    "        stage = 2\n",
    "except:\n",
    "    stage = 1\n",
    "\n",
    "side = None\n",
    "\n",
    "with open(path_adjust + \"temp/is_script_finished.txt\", \"w+\") as text_file:\n",
    "    text_file.write(\"start_script_time\" + str(very_start))\n",
    "\n",
    "if arg_parse_stage == 1:\n",
    "    stage = int(args.stage)\n",
    "\n",
    "print(\"the stage\" + str(stage))\n",
    "# Overide model and stage for testing\n",
    "\n",
    "model = \"two_model\"\n",
    "stage = 1\n",
    "print(\"the overidden stage\" + str(stage))\n",
    "\n",
    "if stage == 2:\n",
    "    # size\n",
    "    h5f = h5py.File(\"data/gpu_output/\" + gpu_file_name + \".h5\", \"r\")\n",
    "    X = h5f[\"X\"][:]\n",
    "    P = h5f[\"P\"][:]\n",
    "    sample_weights = h5f[\"sample_weights\"][:]\n",
    "    sampled_idx_epoch = h5f[\"sampled_idx_epoch\"][:]\n",
    "    h5f.close()\n",
    "    data = pq.read_pandas(\"data/gpu_output/\" + gpu_file_name +\n",
    "                          \"_data.parquet\").to_pandas()\n",
    "    X_for_all_labels = data.dropna(subset=[\"bins\"])\n",
    "    sampled_idx = pd.DatetimeIndex(sampled_idx_epoch)\n",
    "    X_for_all_labels[\"predicted_bins\"] = P\n",
    "    side = X_for_all_labels[\"predicted_bins\"]\n",
    "    # Could use the probabilities (instead of [1,0,0] use [0.2,0.55,0.25]\n",
    "\n",
    "import yaml\n",
    "import wandb\n",
    "yaml_path = path_adjust + \"yaml/preprocessing.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "wandb.init(\n",
    "    dir=\"~/ProdigyAI/\",\n",
    "    project=\"prodigyai\",\n",
    "    config=config_dictionary,\n",
    ")\n",
    "\n",
    "minimum_return = eval(wandb.config['params']['minimum_return']['value'])\n",
    "\n",
    "vertical_barrier_seconds = eval(\n",
    "    wandb.config['params']['vertical_barrier_seconds']['value'])\n",
    "\n",
    "volume_max = (\n",
    "    minimum_return + wandb.config['params']['vol_max_modifier']['value']\n",
    ")  # The higher this is the more an increase in volatility requries an increase\n",
    "# in return to be considered buy/sell (Increasing this increases end barrier vertical touches)\n",
    "\n",
    "volume_min = minimum_return + wandb.config['params']['vol_min_modifier'][\n",
    "    'value']\n",
    "\n",
    "filter_type = wandb.config['params']['filter_type']['value']\n",
    "\n",
    "if filter_type == \"cm\":\n",
    "    cusum_filter_vol_modifier = wandb.config['params'][\n",
    "        'cusum_filter_volume_modifier']['value']\n",
    "else:\n",
    "    cusum_filter_vol_modifier = 0\n",
    "\n",
    "use_sample_weights = wandb.config['params']['use_sample_weights']['value']\n",
    "\n",
    "use_sample_weights = wandb.config['params']['use_fractional_differentiation'][\n",
    "    'value']\n",
    "\n",
    "input_type = wandb.config['params']['input_type']['value']\n",
    "# parameters[\"ntb\"] = True  # non time bars\n",
    "# if parameters[\"ntb\"] == True:\n",
    "#     # Pick whether you want to add in the time since last bar input feature\n",
    "#     # time since last bar column\n",
    "#     parameters[\"tslbc\"] = True  # time since last bar column\n",
    "# else:\n",
    "#     # Pick whether you want to add in the volume input feature\n",
    "#     parameters[\"vbc\"] = True  # volume bar column\n",
    "# Create the txt file string\n",
    "\n",
    "parameter_string = wandb.run.id\n",
    "\n",
    "pt_sl = [\n",
    "    wandb.config['params']['profit_taking_multiplier']['value'],\n",
    "    wandb.config['params']['stop_loss_multiplier']['value']\n",
    "]\n",
    "\n",
    "cpus = cpu_count() - 1\n",
    "regenerate_features_and_labels = True\n",
    "\n",
    "if regenerate_features_and_labels == True:\n",
    "    # READ THE DATA\n",
    "    if stage == 1:\n",
    "        # Side\n",
    "        print(\"starting data load\")\n",
    "        head = wandb.config['params']['head']['value']\n",
    "        # # read parquet file of dollar bars\n",
    "        if input_type == \"bars\":\n",
    "            # Mlfinlab bars\n",
    "            data = pq.read_pandas(\n",
    "                path_adjust + \"data/bars/\"\n",
    "                \"btcusdt_agg_trades_50_volume_bars.parquet\").to_pandas()\n",
    "            data = data.drop(columns=[\n",
    "                \"open\",\n",
    "                \"high\",\n",
    "                \"low\",\n",
    "                # \"volume\",\n",
    "                \"seconds_since_last_bar\",\n",
    "            ])\n",
    "            # 1 min ohlcv ready made bars\n",
    "            # data = pq.read_pandas(\"data/bars/BTCUSDT_1m.parquet\").to_pandas()\n",
    "            # data[\"date_time\"] = pd.to_datetime(data[\"date_time\"], unit='ms')\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "            data = data.set_index(\"date_time\")\n",
    "            data.index = pd.to_datetime(data.index, infer_datetime_format=True)\n",
    "        # read parquet file of raw ticks\n",
    "        if input_type == \"ticks\":\n",
    "            data = pq.read_pandas(\n",
    "                path_adjust + \"data/bars/\" +\n",
    "                \"btcusdt_agg_trades_raw_tick_data.parquet\").to_pandas()\n",
    "            data = data.rename(columns={\n",
    "                \"date\": \"date_time\",\n",
    "                \"price\": \"close\",\n",
    "                \"volume\": \"volume\"\n",
    "            })\n",
    "            data = data.drop(columns=[\"volume\"])\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "            data = data.set_index(\"date_time\")\n",
    "            print(\"converting index to date_time\")\n",
    "            data.index = pd.to_datetime(data.index,\n",
    "                                        format=\"%m/%d/%Y %H:%M:%S.%f\")\n",
    "            print(\"index converted\")\n",
    "            # Should do something else than drop the duplicates (maybe trades doesnt have duplicate indexes rather than aggtrades)\n",
    "            data = data.loc[~data.index.duplicated(keep=\"first\")]\n",
    "        if input_type == \"orderbook\":\n",
    "            with open(path_adjust + \"temp/orderbook_data_name.txt\",\n",
    "                      \"r\") as text_file:\n",
    "                orderbook_preprocessed_file_name = text_file.read()\n",
    "            h5f = h5py.File(\n",
    "                path_adjust + \"data/orderbook_preprocessed/\" +\n",
    "                orderbook_preprocessed_file_name + \".h5\",\n",
    "                \"r\",\n",
    "            )\n",
    "            volumes_index_as_epoch = h5f[\"volumes_index_as_epoch\"][:]\n",
    "            volumes_np_array = h5f[\"volumes_np_array\"][:]\n",
    "            df_index_as_epoch = h5f[\"df_index_as_epoch\"][:]\n",
    "            df_np_array = h5f[\"df_np_array\"][:]\n",
    "            h5f.close()\n",
    "            volumes = pd.DataFrame(data=volumes_np_array,\n",
    "                                   index=volumes_index_as_epoch)\n",
    "            volumes.index = pd.to_datetime(volumes.index, unit=\"ms\")\n",
    "            data = pd.DataFrame(data=df_np_array, index=df_index_as_epoch)\n",
    "            data.index = pd.to_datetime(data.index, unit=\"ms\")\n",
    "            data.columns = [\"close\"]\n",
    "            data.index.name = \"date_time\"\n",
    "            if head > 0:\n",
    "                data = data.head(head)\n",
    "        print(\"data load finished\")\n",
    "        # Checking for duplicates\n",
    "        # duplicate_fast_search(data.index.duplicated())\n",
    "        # Fractional differentiation\n",
    "        if use_sample_weights == \"on\":\n",
    "            data_series = data[\"close\"].to_frame()\n",
    "            # # generate 100 points\n",
    "            # nsample = 1000\n",
    "            # ## simulate a simple sinusoidal function\n",
    "            # x1 = np.linspace(0, 10, nsample)\n",
    "            # y = pd.Series(1*np.sin(2.0 * x1 + .5))\n",
    "            # y.plot()\n",
    "            # c_constant = 1.\n",
    "            # y_shifted = (y + c_constant).cumsum().rename('Shifted_series').to_frame()\n",
    "            # y_shifted.plot()\n",
    "            # df = y_shifted\n",
    "            # # df=(df-df.mean())/df.std()\n",
    "            # df['Shifted_series'][1:] = np.diff(df['Shifted_series'].values)\n",
    "            # df['Shifted_series'].plot()\n",
    "            kwargs = None\n",
    "            # data_series = np.log(data_series)  ## is it good to log this?\n",
    "            frac_diff_series, d = get_opt_d(  # reduces the number of rows and ends up with less vertical barriers touched\n",
    "                data_series,\n",
    "                ds=None,\n",
    "                maxlag=None,  # If we use raw tick data need at least head > 8000\n",
    "                thres=1e-5,\n",
    "                max_size=10000,\n",
    "                p_thres=1e-2,\n",
    "                autolag=None,\n",
    "                verbose=1,\n",
    "            )\n",
    "            data[\"close\"] = frac_diff_series\n",
    "            data = data.dropna(subset=[\"close\"])\n",
    "        data[\"window_volatility_level\"] = np.nan\n",
    "        start = time.time()\n",
    "        volatility_level_array = volatility_levels_numba(\n",
    "            np.ascontiguousarray(data.close.values),\n",
    "            wandb.config['params']['window_length']['value'])\n",
    "        data[\"window_volatility_level\"] = volatility_level_array\n",
    "        # Should adjust the max value\n",
    "        # To get more vertical touches we can\n",
    "        # either increase vol_max or\n",
    "        # decrease the window seconds\n",
    "        scaler = MinMaxScaler(feature_range=(volume_min,\n",
    "                                             volume_max))  # normalization\n",
    "        normed_window_volatility_level = scaler.fit_transform(\n",
    "            data[[\"window_volatility_level\"]])\n",
    "        data[\"window_volatility_level\"] = normed_window_volatility_level  #\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "        # CUSUM FILTER\n",
    "        volatility_threshold = data[\"window_volatility_level\"].mean()\n",
    "        close_copy = data.dropna().close.copy(deep=True)\n",
    "        close_np_array, close_index_np_array = pandas_series_to_numba_ready_np_arrays(\n",
    "            close_copy)\n",
    "        volatility_threshold = volatility_threshold * cusum_filter_vol_modifier\n",
    "        print(\"data_len = \" + str(len(data)))\n",
    "        start = time.time()\n",
    "        sampled_idx = filter_events(\n",
    "            data,\n",
    "            close_np_array,\n",
    "            close_index_np_array,\n",
    "            volatility_threshold,\n",
    "            filter_type,\n",
    "        )\n",
    "        print(\"sampled_idx_len = \" + str(len(sampled_idx)))\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "    if stage == 2:\n",
    "        # size\n",
    "        start = time.time()\n",
    "        volatility_level_array = volatility_levels_numba(\n",
    "            data.close.values,\n",
    "            wandb.config['params']['window_length']['value'])\n",
    "        data[\"window_volatility_level\"] = volatility_level_array\n",
    "    # This code runs for both first and second stage preprocessing\n",
    "    start = time.time()\n",
    "    vertical_barrier_timestamps = ml.labeling.add_vertical_barrier(\n",
    "        t_events=sampled_idx,\n",
    "        close=data[\"close\"],\n",
    "        num_seconds=vertical_barrier_seconds)\n",
    "    end = time.time()\n",
    "    print(\"vertical barrier\" + str(end - start))\n",
    "    start = time.time()\n",
    "    print(\"Getting triple barrier events\")\n",
    "    triple_barrier_events = ml.labeling.get_events(\n",
    "        close=data[\"close\"],\n",
    "        t_events=sampled_idx,\n",
    "        pt_sl=pt_sl,\n",
    "        target=data[\"window_volatility_level\"],\n",
    "        min_ret=minimum_return,\n",
    "        num_threads=cpus * 2,\n",
    "        vertical_barrier_times=vertical_barrier_timestamps,\n",
    "        side_prediction=side,\n",
    "        split_by=wandb.config['params']['split_by']\n",
    "        ['value']  # maybe we want this as large as we can while still fitting in ram\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"triple_barrier_events finished taking\" + str(end - start))\n",
    "    very_end = time.time()\n",
    "    with open(path_adjust + \"temp/is_script_finished.txt\", \"w+\") as text_file:\n",
    "        text_file.write(\"full_script_time\" + str(very_end - very_start))\n",
    "    start_time = time.time()\n",
    "    print(\"Returning Bins\")\n",
    "    labels = ml.labeling.get_bins(triple_barrier_events, data[\"close\"])\n",
    "    labels = ml.labeling.drop_labels(labels)\n",
    "    label_counts = labels.bin.value_counts()\n",
    "    print(\"label_counts\" + str(label_counts))\n",
    "    end_time = time.time()\n",
    "    print(\"returning bins finished taking\" + str(end_time - start_time))\n",
    "    # unique, counts = np.unique(y, return_counts=True)\n",
    "    sampled_idx_epoch = sampled_idx.astype(np.int64)\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/sampled_idx_epoch.h5\",\n",
    "        \"w\")\n",
    "    h5f.create_dataset(\"sampled_idx_epoch\", data=sampled_idx_epoch)\n",
    "    h5f.close()\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(labels)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/labels.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(data)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/data.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "    # save data dataframe\n",
    "    table = pa.Table.from_pandas(triple_barrier_events)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/triple_barrier_events.parquet\",\n",
    "        use_dictionary=True,\n",
    "        compression=\"snappy\",\n",
    "        use_deprecated_int96_timestamps=True,\n",
    "    )\n",
    "else:\n",
    "    labels = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/labels.parquet\").to_pandas()\n",
    "    data = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/data.parquet\").to_pandas()\n",
    "    triple_barrier_events = pq.read_pandas(\n",
    "        path_adjust +\n",
    "        \"data/inputs_and_barrier_labels/triple_barrier_events.parquet\"\n",
    "    ).to_pandas()\n",
    "    with open(path_adjust + \"temp/orderbook_data_name.txt\", \"r\") as text_file:\n",
    "        orderbook_preprocessed_file_name = text_file.read()\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/orderbook_preprocessed/\" +\n",
    "        orderbook_preprocessed_file_name + \".h5\", \"r\")\n",
    "    volumes_index_as_epoch = h5f[\"volumes_index_as_epoch\"][:]\n",
    "    volumes_np_array = h5f[\"volumes_np_array\"][:]\n",
    "    h5f.close()\n",
    "    volumes = pd.DataFrame(data=volumes_np_array, index=volumes_index_as_epoch)\n",
    "    volumes.index = pd.to_datetime(volumes.index, unit=\"ms\")\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/inputs_and_barrier_labels/sampled_idx_epoch.h5\",\n",
    "        \"r\")\n",
    "    sampled_idx_epoch = h5f[\"sampled_idx_epoch\"][:]\n",
    "    h5f.close()\n",
    "    sampled_idx = pd.DatetimeIndex(sampled_idx_epoch)\n",
    "\n",
    "if stage == 1:\n",
    "    # Get why from labels\n",
    "    y_dataframe = labels[\"bin\"]\n",
    "    data[\"bins\"] = labels[\"bin\"]\n",
    "    y = np.asarray(y_dataframe)\n",
    "    start_time = time.time()\n",
    "    # side\n",
    "    X_for_all_labels = data.loc[labels.index, :]\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    ### FOR HIGHWAY RNN\n",
    "    X = np.asarray(volumes.loc[labels.index, :])\n",
    "    h5f = h5py.File(\n",
    "        path_adjust + \"data/preprocessed/\" + parameter_string + \"_gam_rhn.h5\",\n",
    "        \"w\")\n",
    "    h5f.create_dataset(\"X\", data=X)\n",
    "    h5f.create_dataset(\"y\", data=y)\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1y7dtrfu'"
     ]
    }
   ],
   "source": [
    "parameter_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"script initiated\")\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from numba import njit, prange\n",
    "import math\n",
    "import numpy as np\n",
    "from third_party_libraries.TABL import Models\n",
    "from third_party_libraries.keras_lr_finder.lr_finder import LRFinder\n",
    "from third_party_libraries.CLR.clr_callbacks import CyclicLR\n",
    "import keras\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf\n",
    "# Init wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import yaml\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "    resuming = \"NA\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    args = parser.parse_args()\n",
    "    if args.resuming != None:\n",
    "        resuming = args.resuming\n",
    "    else:\n",
    "        resuming = \"NA\"\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "yaml_path = path_adjust + \"yaml/tabl.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "try:\n",
    "    with open(\"temp/run_in_progress.txt\", \"r\") as text_file:\n",
    "        run_in_progress = text_file.read()\n",
    "        resume = True\n",
    "except:\n",
    "    resume = False\n",
    "\n",
    "wandb.init(dir=\"~/ProdigyAI/\",\n",
    "           project=\"prodigyai\",\n",
    "           config=config_dictionary,\n",
    "           resume=resume)\n",
    "\n",
    "window_length = wandb.config['params']['window_length']['value']\n",
    "num_features = wandb.config['params']['num_features']['value']\n",
    "epochs = wandb.config['params']['epochs']['value']\n",
    "batch_size = wandb.config['params']['batch_size']['value']\n",
    "\n",
    "dropout = wandb.config['params']['dropout']['value']\n",
    "# # try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes\n",
    "# # example data\n",
    "# example_x = np.random.rand(1000, 40, 10)\n",
    "# np.min(example_x)\n",
    "# np.max(example_x)\n",
    "# np.mean(example_x)\n",
    "# example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)\n",
    "## PRODIGY AI HOCKUS POCKUS START\n",
    "\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "home = str(Path.home())\n",
    "file_name = \"1fdpldp4.h5\"\n",
    "wandb.config.update({'dataset': file_name})\n",
    "path = home + \"/ProdigyAI/data/preprocessed/\" + file_name\n",
    "h5f = h5py.File(path, \"r\")\n",
    "\n",
    "prices_for_window_index_array_train = h5f[\n",
    "    \"prices_for_window_index_array_train\"][:]\n",
    "\n",
    "prices_for_window_index_array_val = h5f[\"prices_for_window_index_array_val\"][:]\n",
    "\n",
    "prices_for_window_index_array_test = h5f[\n",
    "    \"prices_for_window_index_array_test\"][:]\n",
    "\n",
    "close_index_array_train = h5f[\"close_index_array_train\"][:]\n",
    "close_index_array_val = h5f[\"close_index_array_val\"][:]\n",
    "close_index_array_test = h5f[\"close_index_array_test\"][:]\n",
    "input_features_normalized_train = h5f[\"input_features_normalized_train\"][:]\n",
    "input_features_normalized_val = h5f[\"input_features_normalized_val\"][:]\n",
    "input_features_normalized_test = h5f[\"input_features_normalized_test\"][:]\n",
    "y_train = h5f[\"y_train\"][:]\n",
    "y_val = h5f[\"y_val\"][:]\n",
    "y_test = h5f[\"y_test\"][:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5204831"
     ]
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"script initiated\")\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from numba import njit, prange\n",
    "import math\n",
    "import numpy as np\n",
    "from third_party_libraries.TABL import Models\n",
    "from third_party_libraries.keras_lr_finder.lr_finder import LRFinder\n",
    "from third_party_libraries.CLR.clr_callbacks import CyclicLR\n",
    "import keras\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf\n",
    "# Init wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import yaml\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "    resuming = \"NA\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    args = parser.parse_args()\n",
    "    if args.resuming != None:\n",
    "        resuming = args.resuming\n",
    "    else:\n",
    "        resuming = \"NA\"\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "yaml_path = path_adjust + \"yaml/tabl.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "try:\n",
    "    with open(\"temp/run_in_progress.txt\", \"r\") as text_file:\n",
    "        run_in_progress = text_file.read()\n",
    "        resume = True\n",
    "except:\n",
    "    resume = False\n",
    "\n",
    "wandb.init(dir=\"~/ProdigyAI/\",\n",
    "           project=\"prodigyai\",\n",
    "           config=config_dictionary,\n",
    "           resume=resume)\n",
    "\n",
    "window_length = wandb.config['params']['window_length']['value']\n",
    "num_features = wandb.config['params']['num_features']['value']\n",
    "epochs = wandb.config['params']['epochs']['value']\n",
    "batch_size = wandb.config['params']['batch_size']['value']\n",
    "\n",
    "dropout = wandb.config['params']['dropout']['value']\n",
    "# # try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes\n",
    "# # example data\n",
    "# example_x = np.random.rand(1000, 40, 10)\n",
    "# np.min(example_x)\n",
    "# np.max(example_x)\n",
    "# np.mean(example_x)\n",
    "# example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)\n",
    "## PRODIGY AI HOCKUS POCKUS START\n",
    "\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "home = str(Path.home())\n",
    "file_name = \"1fdpldp4.h5\"\n",
    "wandb.config.update({'dataset': file_name})\n",
    "path = home + \"/ProdigyAI/data/preprocessed/\" + file_name\n",
    "h5f = h5py.File(path, \"r\")\n",
    "\n",
    "prices_for_window_index_array_train = h5f[\n",
    "    \"prices_for_window_index_array_train\"][:]\n",
    "\n",
    "prices_for_window_index_array_val = h5f[\"prices_for_window_index_array_val\"][:]\n",
    "\n",
    "prices_for_window_index_array_test = h5f[\n",
    "    \"prices_for_window_index_array_test\"][:]\n",
    "\n",
    "close_index_array_train = h5f[\"close_index_array_train\"][:]\n",
    "close_index_array_val = h5f[\"close_index_array_val\"][:]\n",
    "close_index_array_test = h5f[\"close_index_array_test\"][:]\n",
    "input_features_normalized_train = h5f[\"input_features_normalized_train\"][:]\n",
    "input_features_normalized_val = h5f[\"input_features_normalized_val\"][:]\n",
    "input_features_normalized_test = h5f[\"input_features_normalized_test\"][:]\n",
    "y_train = h5f[\"y_train\"][:]\n",
    "y_val = h5f[\"y_val\"][:]\n",
    "y_test = h5f[\"y_test\"][:]\n",
    "\n",
    "h5f.close()\n",
    "## PRODIGY AI HOCKUS POCKUS END\n",
    "\n",
    "for i in range(len(close_index_array_train)):\n",
    "    if close_index_array_train[i] == prices_for_window_index_array_train[0]:\n",
    "        offset = i\n",
    "        break\n",
    "\n",
    "checkpoint_path = os.path.join(wandb.run.dir, \"cp.ckpt\")\n",
    "\n",
    "@njit\n",
    "def generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                     input_features_normalized):\n",
    "    X = np.empty((batch_size, dim[1], dim[0]))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        for k in prange(len(input_features_normalized)):\n",
    "            for l in prange(dim[0]):\n",
    "                X[i][k][l] = input_features_normalized[k][ID + l]\n",
    "    # X[i, ] = self.input_features_normalized[ID+self.window_length, ]\n",
    "    return X\n",
    "\n",
    "@njit\n",
    "def generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data):\n",
    "    y = np.empty((batch_size, n_classes))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        y[i, :] = y_data[ID]\n",
    "    return y\n",
    "\n",
    "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
    "    def __init__(self,\n",
    "                 checkpoint_path,\n",
    "                 prices_for_window_index_array,\n",
    "                 input_features_normalized,\n",
    "                 y_data,\n",
    "                 batch_size,\n",
    "                 dim,\n",
    "                 n_classes,\n",
    "                 to_fit,\n",
    "                 shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.prices_for_window_index_array = prices_for_window_index_array\n",
    "        self.input_features_normalized = input_features_normalized\n",
    "        self.labels = y_data\n",
    "        self.y_data = y_data\n",
    "        self.to_fit = to_fit\n",
    "        self.n_classes = n_classes\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.n = 0\n",
    "        self.list_IDs = np.arange(len(self.prices_for_window_index_array))\n",
    "        self.on_epoch_end()\n",
    "    def __next__(self):\n",
    "        # Get one batch of data\n",
    "        data = self.__getitem__(self.n)\n",
    "        # Batch index\n",
    "        self.n += 1\n",
    "        # If we have processed the entire dataset then\n",
    "        if self.n >= self.__len__():\n",
    "            self.on_epoch_end\n",
    "            self.n = 0\n",
    "        return data\n",
    "    def __len__(self):\n",
    "        # Return the number of batches of the dataset\n",
    "        return math.ceil(len(self.indexes) / self.batch_size)\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                               self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X = self._generate_x(list_IDs_temp)\n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(list_IDs_temp)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    def on_epoch_end(self):\n",
    "        wandb.save(self.checkpoint_path)\n",
    "        self.indexes = np.arange(len(self.prices_for_window_index_array))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def _generate_x(self, list_IDs_temp):\n",
    "        dim = self.dim\n",
    "        batch_size = self.batch_size\n",
    "        input_features_normalized = self.input_features_normalized\n",
    "        X = generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                             input_features_normalized)\n",
    "        # X[i, ] = self.input_features_normalized[ID+self.window_length, ]\n",
    "        return X\n",
    "    def _generate_y(self, list_IDs_temp):\n",
    "        batch_size = self.batch_size\n",
    "        n_classes = self.n_classes\n",
    "        y_data = self.y_data\n",
    "        y = generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data)\n",
    "        return y\n",
    "\n",
    "n_classes = 3\n",
    "dim = (window_length, num_features)\n",
    "\n",
    "to_fit = True\n",
    "# for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,\n",
    "#                                       input_features_normalized_train,\n",
    "#                                       y_train,\n",
    "#                                       batch_size,\n",
    "#                                       dim,\n",
    "#                                       n_classes,\n",
    "#                                       to_fit,\n",
    "#                                       shuffle=True):\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1\n",
    "\n",
    "template = [[num_features, window_length], [60, 10], [120, 5], [3, 1]]\n",
    "# get Bilinear model\n",
    "\n",
    "projection_regularizer = None\n",
    "projection_constraint = keras.constraints.max_norm(3.0, axis=0)\n",
    "attention_regularizer = None\n",
    "\n",
    "attention_constraint = keras.constraints.max_norm(5.0, axis=1)\n",
    "# path = path_adjust + \"data/lob_2010/train_and_test_tabl.h5\"\n",
    "# h5f = h5py.File(path, \"r\")\n",
    "# trainX_CNN = h5f[\"trainX_CNN\"][:]\n",
    "# trainY_CNN = h5f[\"trainY_CNN\"][:]\n",
    "# testX_CNN = h5f[\"testX_CNN\"][:]\n",
    "# testY_CNN = h5f[\"testY_CNN\"][:]\n",
    "# h5f.close()\n",
    "# # train one epoch to find the learning rate\n",
    "# model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     batch_size=256,\n",
    "#     epochs=1,\n",
    "#     shuffle=False,\n",
    "# )  # no class weight\n",
    "# Model configuration\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "loss_function = categorical_crossentropy\n",
    "# no_epochs = 5\n",
    "# start_lr = 0.0001\n",
    "# end_lr = 1\n",
    "# moving_average = 20\n",
    "# # Determine tests you want to perform\n",
    "# tests = [\n",
    "#     (\"sgd\", \"SGD optimizer\"),\n",
    "#     (\"adam\", \"Adam optimizer\"),\n",
    "#     (\"rmsprop\", \"RMS Prop optimizer\"),\n",
    "# ]\n",
    "# # Set containers for tests\n",
    "# test_learning_rates = []\n",
    "# test_losses = []\n",
    "# test_loss_changes = []\n",
    "# labels = []\n",
    "# # Perform each test\n",
    "# for test_optimizer, label in tests:\n",
    "#     # Compile the model\n",
    "#     model.compile(loss=loss_function,\n",
    "#                   optimizer=test_optimizer,\n",
    "#                   metrics=[\"accuracy\"])\n",
    "#     # Instantiate the Learning Rate Range Test / LR Finder\n",
    "#     lr_finder = LRFinder(model)\n",
    "#     # Perform the Learning Rate Range Test\n",
    "#     outputs = lr_finder.find(\n",
    "#         trainX_CNN,\n",
    "#         trainY_CNN,\n",
    "#         start_lr=start_lr,\n",
    "#         end_lr=end_lr,\n",
    "#         batch_size=batch_size,\n",
    "#         epochs=no_epochs,\n",
    "#     )\n",
    "#     # Get values\n",
    "#     learning_rates = lr_finder.lrs\n",
    "#     losses = lr_finder.losses\n",
    "#     loss_changes = []\n",
    "#     # Compute smoothed loss changes\n",
    "#     # Inspired by Keras LR Finder: https://github.com/surmenok/keras_lr_finder/blob/master/keras_lr_finder/lr_finder.py\n",
    "#     for i in range(moving_average, len(learning_rates)):\n",
    "#         loss_changes.append(\n",
    "#             (losses[i] - losses[i - moving_average]) / moving_average)\n",
    "#     # Append values to container\n",
    "#     test_learning_rates.append(learning_rates)\n",
    "#     test_losses.append(losses)\n",
    "#     test_loss_changes.append(loss_changes)\n",
    "#     labels.append(label)\n",
    "# # Generate plot for Loss Deltas\n",
    "# for i in range(0, len(test_learning_rates)):\n",
    "#     plt.plot(test_learning_rates[i][moving_average:],\n",
    "#              test_loss_changes[i],\n",
    "#              label=labels[i])\n",
    "# plt.xscale(\"log\")\n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.ylabel(\"loss delta\")\n",
    "# plt.xlabel(\"learning rate (log scale)\")\n",
    "# plt.title(\n",
    "#     \"Results for Learning Rate Range Test / Loss Deltas for Learning Rate\")\n",
    "# plt.show()\n",
    "# # Generate plot for Loss Values\n",
    "# for i in range(0, len(test_learning_rates)):\n",
    "#     plt.plot(test_learning_rates[i], test_losses[i], label=labels[i])\n",
    "# plt.xscale(\"log\")\n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.ylabel(\"loss\")\n",
    "# plt.xlabel(\"learning rate (log scale)\")\n",
    "# plt.title(\n",
    "#     \"Results for Learning Rate Range Test / Loss Values for Learning Rate\")\n",
    "# plt.show()\n",
    "# # Configuration settings for LR finder\n",
    "# start_lr = 1e-4\n",
    "# end_lr = 1e0\n",
    "# no_epochs = 5\n",
    "# ##\n",
    "# ## LR Finder specific code\n",
    "# ##\n",
    "# optimizer = keras.optimizers.RMSprop()\n",
    "# # Compile the model\n",
    "# model.compile(loss=loss_function, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# # Define LR finder callback\n",
    "# from third_party_libraries.LRFT.keras_callbacks import LRFinder\n",
    "# lr_finder = LRFinder(min_lr=start_lr, max_lr=end_lr)\n",
    "# # Perform LR finder\n",
    "# model.fit(\n",
    "#     trainX_CNN,\n",
    "#     trainY_CNN,\n",
    "#     batch_size=batch_size,\n",
    "#     callbacks=[lr_finder],\n",
    "#     epochs=no_epochs,\n",
    "# )\n",
    "# print(\"learning rate found running cyclical learning\")\n",
    "# ### cyclical learning rate\n",
    "# Set CLR options\n",
    "\n",
    "clr_step_size = int(4 *\n",
    "                    (len(prices_for_window_index_array_train) / batch_size))\n",
    "\n",
    "base_lr = 1e-4\n",
    "max_lr = 1e-2\n",
    "\n",
    "mode = \"triangular\"\n",
    "# Define the callback\n",
    "\n",
    "clr = CyclicLR(base_lr=base_lr,\n",
    "               max_lr=max_lr,\n",
    "               step_size=clr_step_size,\n",
    "               mode=mode)\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "model = Models.TABL(\n",
    "    template,\n",
    "    dropout,\n",
    "    projection_regularizer,\n",
    "    projection_constraint,\n",
    "    attention_regularizer,\n",
    "    attention_constraint,\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "# Compile the model\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# Create a callback that saves the model's weights\n",
    "\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                              save_weights_only=True,\n",
    "                              verbose=1)\n",
    "\n",
    "if wandb.run.resumed:\n",
    "    weights = wandb.restore(\"cp.ckpt\",\n",
    "                            run_path=\"garthtrickett/prodigyai/\" + wandb.run.id)\n",
    "    model.load_weights(weights.name)\n",
    "# check_point_file = Path(checkpoint_path)\n",
    "# if check_point_file.exists() and resuming == \"resuming\":\n",
    "#     print(\"weights loaded\")\n",
    "#     model.load_weights(checkpoint_path)\n",
    "# Fit data to model\n",
    "# history = model.fit(trainX_CNN,\n",
    "#                     trainY_CNN,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=20,\n",
    "#                     callbacks=[clr, cp_callback])\n",
    "# create class weight\n",
    "# class_weight = {0: 1e6 / 300.0, 1: 1e6 / 400.0, 2: 1e6 / 300.0}\n",
    "\n",
    "train_generator = DataGenerator(checkpoint_path,\n",
    "                                prices_for_window_index_array_train,\n",
    "                                input_features_normalized_train,\n",
    "                                y_train,\n",
    "                                batch_size,\n",
    "                                dim,\n",
    "                                n_classes,\n",
    "                                to_fit,\n",
    "                                shuffle=True)\n",
    "\n",
    "val_generator = DataGenerator(checkpoint_path,\n",
    "                              prices_for_window_index_array_val,\n",
    "                              input_features_normalized_val,\n",
    "                              y_val,\n",
    "                              batch_size,\n",
    "                              dim,\n",
    "                              n_classes,\n",
    "                              to_fit,\n",
    "                              shuffle=True)\n",
    "\n",
    "steps_per_epoch = len(train_generator)\n",
    "validation_steps = len(val_generator)\n",
    "\n",
    "with open(path_adjust + \"temp/run_in_progress.txt\", \"w+\") as text_file:\n",
    "    text_file.write(wandb.run.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

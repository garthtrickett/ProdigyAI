{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"script initiated\")\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from numba import njit, prange\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "from third_party_libraries.TABL import Models\n",
    "from third_party_libraries.keras_lr_finder.lr_finder import LRFinder\n",
    "from third_party_libraries.CLR.clr_callbacks import CyclicLR\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflowkeras.callbacks import ModelCheckpoint\n",
    "# Init wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import yaml\n",
    "# import tensorflow.keras as keras\n",
    "# import tensorflow.keras.layers as layers\n",
    "# import tensorflow.keras.backend as K\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "    resuming = \"NA\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    args = parser.parse_args()\n",
    "    if args.resuming != None:\n",
    "        resuming = args.resuming\n",
    "    else:\n",
    "        resuming = \"NA\"\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "yaml_path = path_adjust + \"yaml/tabl.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "wandb.init(dir=\"~/ProdigyAI/\",\n",
    "           project=\"prodigyai\",\n",
    "           config=config_dictionary,\n",
    "           resume=True)\n",
    "\n",
    "window_length = wandb.config['params']['window_length']['value']\n",
    "num_features = wandb.config['params']['num_features']['value']\n",
    "epochs = wandb.config['params']['epochs']['value']\n",
    "batch_size = wandb.config['params']['batch_size']['value']\n",
    "\n",
    "dropout = wandb.config['params']['dropout']['value']\n",
    "# # try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes\n",
    "# # example data\n",
    "# example_x = np.random.rand(1000, 40, 10)\n",
    "# np.min(example_x)\n",
    "# np.max(example_x)\n",
    "# np.mean(example_x)\n",
    "# example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)\n",
    "## PRODIGY AI HOCKUS POCKUS START\n",
    "\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "home = str(Path.home())\n",
    "file_name = \"esugj36b.h5\"\n",
    "wandb.config.update({'dataset': file_name})\n",
    "path = home + \"/ProdigyAI/data/preprocessed/\" + file_name\n",
    "h5f = h5py.File(path, \"r\")\n",
    "\n",
    "prices_for_window_index_array_train = h5f[\n",
    "    \"prices_for_window_index_array_train\"][:]\n",
    "\n",
    "prices_for_window_index_array_val = h5f[\"prices_for_window_index_array_val\"][:]\n",
    "\n",
    "prices_for_window_index_array_test = h5f[\n",
    "    \"prices_for_window_index_array_test\"][:]\n",
    "\n",
    "close_index_array_train = h5f[\"close_index_array_train\"][:]\n",
    "close_index_array_val = h5f[\"close_index_array_val\"][:]\n",
    "close_index_array_test = h5f[\"close_index_array_test\"][:]\n",
    "input_features_normalized_train = h5f[\"input_features_normalized_train\"][:]\n",
    "input_features_normalized_val = h5f[\"input_features_normalized_val\"][:]\n",
    "input_features_normalized_test = h5f[\"input_features_normalized_test\"][:]\n",
    "y_train = h5f[\"y_train\"][:]\n",
    "y_val = h5f[\"y_val\"][:]\n",
    "y_test = h5f[\"y_test\"][:]\n",
    "\n",
    "h5f.close()\n",
    "## PRODIGY AI HOCKUS POCKUS END\n",
    "\n",
    "for i in range(len(close_index_array_train)):\n",
    "    if close_index_array_train[i] == prices_for_window_index_array_train[0]:\n",
    "        offset = i\n",
    "        break\n",
    "\n",
    "checkpoint_path = os.path.join(wandb.run.dir, \"cp.ckpt\")\n",
    "\n",
    "@njit\n",
    "def generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                     input_features_normalized):\n",
    "    X = np.empty((batch_size, dim[1], dim[0]))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        for k in prange(len(input_features_normalized)):\n",
    "            for l in prange(dim[0]):\n",
    "                X[i][k][l] = input_features_normalized[k][ID + l]\n",
    "    # X[i, ] = self.input_features_normalized[ID+self.window_length, ]\n",
    "    return X\n",
    "\n",
    "@njit\n",
    "def generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data):\n",
    "    y = np.empty((batch_size, n_classes))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        y[i, :] = y_data[ID]\n",
    "    return y\n",
    "\n",
    "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
    "    def __init__(self,\n",
    "                 checkpoint_path,\n",
    "                 prices_for_window_index_array,\n",
    "                 input_features_normalized,\n",
    "                 y_data,\n",
    "                 batch_size,\n",
    "                 dim,\n",
    "                 n_classes,\n",
    "                 to_fit,\n",
    "                 shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.prices_for_window_index_array = prices_for_window_index_array\n",
    "        self.input_features_normalized = input_features_normalized\n",
    "        self.labels = y_data\n",
    "        self.y_data = y_data\n",
    "        self.to_fit = to_fit\n",
    "        self.n_classes = n_classes\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.n = 0\n",
    "        self.list_IDs = np.arange(len(self.prices_for_window_index_array))\n",
    "        self.on_epoch_end()\n",
    "    def __next__(self):\n",
    "        # Get one batch of data\n",
    "        data = self.__getitem__(self.n)\n",
    "        # Batch index\n",
    "        self.n += 1\n",
    "        # If we have processed the entire dataset then\n",
    "        if self.n >= self.__len__():\n",
    "            self.on_epoch_end\n",
    "            self.n = 0\n",
    "        return data\n",
    "    def __len__(self):\n",
    "        # Return the number of batches of the dataset\n",
    "        return math.ceil(len(self.indexes) / self.batch_size)\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                               self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X = self._generate_x(list_IDs_temp)\n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(list_IDs_temp)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    def on_epoch_end(self):\n",
    "        wandb.save(self.checkpoint_path)\n",
    "        self.indexes = np.arange(len(self.prices_for_window_index_array))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def _generate_x(self, list_IDs_temp):\n",
    "        dim = self.dim\n",
    "        batch_size = self.batch_size\n",
    "        input_features_normalized = self.input_features_normalized\n",
    "        X = generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                             input_features_normalized)\n",
    "        # X[i, ] = self.input_features_normalized[ID+self.window_length, ]\n",
    "        return X\n",
    "    def _generate_y(self, list_IDs_temp):\n",
    "        batch_size = self.batch_size\n",
    "        n_classes = self.n_classes\n",
    "        y_data = self.y_data\n",
    "        y = generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data)\n",
    "        return y\n",
    "\n",
    "n_classes = 3\n",
    "dim = (window_length, num_features)\n",
    "\n",
    "to_fit = True\n",
    "# for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,\n",
    "#                                       input_features_normalized_train,\n",
    "#                                       y_train,\n",
    "#                                       batch_size,\n",
    "#                                       dim,\n",
    "#                                       n_classes,\n",
    "#                                       to_fit,\n",
    "#                                       shuffle=True):\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1\n",
    "\n",
    "template = [[num_features, window_length], [60, 10], [120, 5], [3, 1]]\n",
    "# get Bilinear model\n",
    "\n",
    "projection_regularizer = None\n",
    "projection_constraint = tensorflow.keras.constraints.max_norm(3.0, axis=0)\n",
    "attention_regularizer = None\n",
    "attention_constraint = tensorflow.keras.constraints.max_norm(5.0, axis=1)\n",
    "\n",
    "model = Models.TABL(\n",
    "    template,\n",
    "    dropout,\n",
    "    projection_regularizer,\n",
    "    projection_constraint,\n",
    "    attention_regularizer,\n",
    "    attention_constraint,\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "# path = path_adjust + \"data/lob_2010/train_and_test_tabl.h5\"\n",
    "# h5f = h5py.File(path, \"r\")\n",
    "# trainX_CNN = h5f[\"trainX_CNN\"][:]\n",
    "# trainY_CNN = h5f[\"trainY_CNN\"][:]\n",
    "# testX_CNN = h5f[\"testX_CNN\"][:]\n",
    "# testY_CNN = h5f[\"testY_CNN\"][:]\n",
    "# h5f.close()\n",
    "# # train one epoch to find the learning rate\n",
    "# model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     batch_size=256,\n",
    "#     epochs=1,\n",
    "#     shuffle=False,\n",
    "# )  # no class weight\n",
    "# Model configuration\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "loss_function = categorical_crossentropy\n",
    "# no_epochs = 5\n",
    "# start_lr = 0.0001\n",
    "# end_lr = 1\n",
    "# moving_average = 20\n",
    "# # Determine tests you want to perform\n",
    "# tests = [\n",
    "#     (\"sgd\", \"SGD optimizer\"),\n",
    "#     (\"adam\", \"Adam optimizer\"),\n",
    "#     (\"rmsprop\", \"RMS Prop optimizer\"),\n",
    "# ]\n",
    "# # Set containers for tests\n",
    "# test_learning_rates = []\n",
    "# test_losses = []\n",
    "# test_loss_changes = []\n",
    "# labels = []\n",
    "# # Perform each test\n",
    "# for test_optimizer, label in tests:\n",
    "#     # Compile the model\n",
    "#     model.compile(loss=loss_function,\n",
    "#                   optimizer=test_optimizer,\n",
    "#                   metrics=[\"accuracy\"])\n",
    "#     # Instantiate the Learning Rate Range Test / LR Finder\n",
    "#     lr_finder = LRFinder(model)\n",
    "#     # Perform the Learning Rate Range Test\n",
    "#     outputs = lr_finder.find(\n",
    "#         trainX_CNN,\n",
    "#         trainY_CNN,\n",
    "#         start_lr=start_lr,\n",
    "#         end_lr=end_lr,\n",
    "#         batch_size=batch_size,\n",
    "#         epochs=no_epochs,\n",
    "#     )\n",
    "#     # Get values\n",
    "#     learning_rates = lr_finder.lrs\n",
    "#     losses = lr_finder.losses\n",
    "#     loss_changes = []\n",
    "#     # Compute smoothed loss changes\n",
    "#     # Inspired by Keras LR Finder: https://github.com/surmenok/keras_lr_finder/blob/master/keras_lr_finder/lr_finder.py\n",
    "#     for i in range(moving_average, len(learning_rates)):\n",
    "#         loss_changes.append(\n",
    "#             (losses[i] - losses[i - moving_average]) / moving_average)\n",
    "#     # Append values to container\n",
    "#     test_learning_rates.append(learning_rates)\n",
    "#     test_losses.append(losses)\n",
    "#     test_loss_changes.append(loss_changes)\n",
    "#     labels.append(label)\n",
    "# # Generate plot for Loss Deltas\n",
    "# for i in range(0, len(test_learning_rates)):\n",
    "#     plt.plot(test_learning_rates[i][moving_average:],\n",
    "#              test_loss_changes[i],\n",
    "#              label=labels[i])\n",
    "# plt.xscale(\"log\")\n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.ylabel(\"loss delta\")\n",
    "# plt.xlabel(\"learning rate (log scale)\")\n",
    "# plt.title(\n",
    "#     \"Results for Learning Rate Range Test / Loss Deltas for Learning Rate\")\n",
    "# plt.show()\n",
    "# # Generate plot for Loss Values\n",
    "# for i in range(0, len(test_learning_rates)):\n",
    "#     plt.plot(test_learning_rates[i], test_losses[i], label=labels[i])\n",
    "# plt.xscale(\"log\")\n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.ylabel(\"loss\")\n",
    "# plt.xlabel(\"learning rate (log scale)\")\n",
    "# plt.title(\n",
    "#     \"Results for Learning Rate Range Test / Loss Values for Learning Rate\")\n",
    "# plt.show()\n",
    "# # Configuration settings for LR finder\n",
    "# start_lr = 1e-4\n",
    "# end_lr = 1e0\n",
    "# no_epochs = 5\n",
    "# ##\n",
    "# ## LR Finder specific code\n",
    "# ##\n",
    "# optimizer = keras.optimizers.RMSprop()\n",
    "# # Compile the model\n",
    "# model.compile(loss=loss_function, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# # Define LR finder callback\n",
    "# from third_party_libraries.LRFT.keras_callbacks import LRFinder\n",
    "# lr_finder = LRFinder(min_lr=start_lr, max_lr=end_lr)\n",
    "# # Perform LR finder\n",
    "# model.fit(\n",
    "#     trainX_CNN,\n",
    "#     trainY_CNN,\n",
    "#     batch_size=batch_size,\n",
    "#     callbacks=[lr_finder],\n",
    "#     epochs=no_epochs,\n",
    "# )\n",
    "# print(\"learning rate found running cyclical learning\")\n",
    "# ### cyclical learning rate\n",
    "# Set CLR options\n",
    "\n",
    "clr_step_size = int(4 *\n",
    "                    (len(prices_for_window_index_array_train) / batch_size))\n",
    "\n",
    "base_lr = 1e-4\n",
    "max_lr = 1e-2\n",
    "\n",
    "mode = \"triangular\"\n",
    "# Define the callback\n",
    "\n",
    "clr = CyclicLR(base_lr=base_lr,\n",
    "               max_lr=max_lr,\n",
    "               step_size=clr_step_size,\n",
    "               mode=mode)\n",
    "\n",
    "optimizer = tensorflow.keras.optimizers.Adam()\n",
    "# Compile the model\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# Create a callback that saves the model's weights\n",
    "\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                              save_weights_only=True,\n",
    "                              verbose=1)\n",
    "\n",
    "check_point_file = Path(checkpoint_path)\n",
    "\n",
    "if check_point_file.exists() and resuming == \"resuming\":\n",
    "    print(\"weights loaded\")\n",
    "    model.load_weights(checkpoint_path)\n",
    "# Fit data to model\n",
    "# history = model.fit(trainX_CNN,\n",
    "#                     trainY_CNN,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=20,\n",
    "#                     callbacks=[clr, cp_callback])\n",
    "\n",
    "finished_weights_path = path_adjust + \"temp/cp_end.ckpt\"\n",
    "\n",
    "model.save_weights(finished_weights_path)\n",
    "# create class weight\n",
    "# class_weight = {0: 1e6 / 300.0, 1: 1e6 / 400.0, 2: 1e6 / 300.0}\n",
    "\n",
    "train_generator = DataGenerator(checkpoint_path,\n",
    "                                prices_for_window_index_array_train,\n",
    "                                input_features_normalized_train,\n",
    "                                y_train,\n",
    "                                batch_size,\n",
    "                                dim,\n",
    "                                n_classes,\n",
    "                                to_fit,\n",
    "                                shuffle=True)\n",
    "\n",
    "val_generator = DataGenerator(checkpoint_path,\n",
    "                              prices_for_window_index_array_val,\n",
    "                              input_features_normalized_val,\n",
    "                              y_val,\n",
    "                              batch_size,\n",
    "                              dim,\n",
    "                              n_classes,\n",
    "                              to_fit,\n",
    "                              shuffle=True)\n",
    "\n",
    "steps_per_epoch = len(train_generator)\n",
    "\n",
    "validation_steps = len(val_generator)\n",
    "# example sata training\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[cp_callback,\n",
    "               WandbCallback(save_model=True, monitor=\"loss\")])\n",
    "\n",
    "model.save(os.path.join(wandb.run.dir, \"model.h5\"))\n",
    "# no class weight\n",
    "## lob FI-2010 DATA PREPERATION\n",
    "# The first 40 columns of the FI-2010 dataset are 10 levels ask and bid information\n",
    "# for a limit order book and we only use these 40 features in our network.\n",
    "# The last 5 columns of the FI-2010 dataset are the labels with different prediction horizons.\n",
    "# def prepare_x(data):\n",
    "#     df1 = data[:40, :].T\n",
    "#     return np.array(df1)\n",
    "# def get_label(data):\n",
    "#     lob = data[-5:, :].T\n",
    "#     return lob\n",
    "# def data_classification(X, Y, T):\n",
    "#     [N, D] = X.shape\n",
    "#     df = np.array(X)\n",
    "#     dY = np.array(Y)\n",
    "#     dataY = dY[T - 1 : N]\n",
    "#     dataX = np.zeros((N - T + 1, T, D))\n",
    "#     for i in range(T, N + 1):\n",
    "#         dataX[i - T] = df[i - T : i, :]\n",
    "#     return dataX.reshape(dataX.shape + (1,)), dataY\n",
    "# # please change the data_path to your local path\n",
    "# data_path = (\n",
    "#     \"../third_party_libraries/gam_rhn/95-FI2010/data/BenchmarkDatasets/NoAuction\"\n",
    "# )\n",
    "# dec_train = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Training/Train_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "# )\n",
    "# dec_test1 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "# )\n",
    "# dec_test2 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_8.txt\"\n",
    "# )\n",
    "# dec_test3 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_9.txt\"\n",
    "# )\n",
    "# dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "# # extract limit order book data from the FI-2010 dataset\n",
    "# train_lob = prepare_x(dec_train)\n",
    "# test_lob = prepare_x(dec_test)\n",
    "# # extract label from the FI-2010 dataset\n",
    "# train_label = get_label(dec_train)\n",
    "# test_label = get_label(dec_test)\n",
    "# # prepare training data. We feed past 100 observations into our algorithms and choose the prediction horizon.\n",
    "# trainX_CNN, trainY_CNN = data_classification(train_lob, train_label, T=10)\n",
    "# trainY_CNN = trainY_CNN[:, 3] - 1\n",
    "# trainY_CNN = np_utils.to_categorical(trainY_CNN, 3)\n",
    "# # trainX_CNN.shape (254651, 100, 40, 1)\n",
    "# # prepare test data.\n",
    "# testX_CNN, testY_CNN = data_classification(test_lob, test_label, T=10)\n",
    "# testY_CNN = testY_CNN[:, 3] - 1\n",
    "# testY_CNN = np_utils.to_categorical(testY_CNN, 3)\n",
    "# trainX_CNN = np.swapaxes(trainX_CNN, 1, 2)\n",
    "# trainX_CNN = trainX_CNN.reshape(\n",
    "#     trainX_CNN.shape[0], trainX_CNN.shape[1], trainX_CNN.shape[2]\n",
    "# )\n",
    "# testX_CNN = testX_CNN = np.swapaxes(testX_CNN, 1, 2)\n",
    "# testX_CNN = testX_CNN.reshape(\n",
    "#     testX_CNN.shape[0], testX_CNN.shape[1], testX_CNN.shape[2]\n",
    "# )\n",
    "# path = path_adjust + \"data/lob_2010/train_and_test_tabl.h5\"\n",
    "# h5f = h5py.File(path, \"w\")\n",
    "# h5f.create_dataset(\"trainX_CNN\", data=trainX_CNN)\n",
    "# h5f.create_dataset(\"trainY_CNN\", data=trainY_CNN)\n",
    "# h5f.create_dataset(\"testX_CNN\", data=testX_CNN)\n",
    "# h5f.create_dataset(\"testY_CNN\", data=testY_CNN)\n",
    "# h5f.close()\n",
    "# fi-02010 data\n",
    "# balancing seems to help abit (0.64 accuracy after 950 epochs), next try norm 0,1 rather than -1,1\n",
    "# after applying normalization not sure if ill need this\n",
    "# two_d_X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "# two_d_X_val = X_val.reshape(X_val.shape[0], X_val.shape[1] * X_val.shape[2])\n",
    "# oversample = SMOTE()\n",
    "# X_train, y_train = oversample.fit_resample(two_d_X_train, y_train)\n",
    "# X_val, y_val = oversample.fit_resample(two_d_X_val, y_val)\n",
    "# X_train = X_train.reshape((X_train.shape[0], 40, 10))\n",
    "# X_val = X_val.reshape((X_val.shape[0], 40, 10))\n",
    "# train one epoch to find the learning rate\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=256,\n",
    "    epochs=1,\n",
    "    shuffle=False,\n",
    ")  # no class weight\n",
    "\n",
    "score = model.evaluate(x=X_test, y=y_test, batch_size=256)\n",
    "# Save model to wandb\n",
    "# print(score)\n",
    "# score on deeplob fi-2010 loss: 0.7469 - acc: 0.6760 - val_loss: 0.6772 - val_acc: 0.7248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 0, 3)"
     ]
    }
   ],
   "source": [
    "print(\"script initiated\")\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from numba import njit, prange\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "from third_party_libraries.TABL import Models\n",
    "from third_party_libraries.keras_lr_finder.lr_finder import LRFinder\n",
    "from third_party_libraries.CLR.clr_callbacks import CyclicLR\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflowkeras.callbacks import ModelCheckpoint\n",
    "# Init wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import yaml\n",
    "# import tensorflow.keras as keras\n",
    "# import tensorflow.keras.layers as layers\n",
    "# import tensorflow.keras.backend as K\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "    resuming = \"NA\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    args = parser.parse_args()\n",
    "    if args.resuming != None:\n",
    "        resuming = args.resuming\n",
    "    else:\n",
    "        resuming = \"NA\"\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "yaml_path = path_adjust + \"yaml/tabl.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "wandb.init(dir=\"~/ProdigyAI/\",\n",
    "           project=\"prodigyai\",\n",
    "           config=config_dictionary,\n",
    "           resume=True)\n",
    "\n",
    "window_length = wandb.config['params']['window_length']['value']\n",
    "num_features = wandb.config['params']['num_features']['value']\n",
    "epochs = wandb.config['params']['epochs']['value']\n",
    "batch_size = wandb.config['params']['batch_size']['value']\n",
    "\n",
    "dropout = wandb.config['params']['dropout']['value']\n",
    "# # try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes\n",
    "# # example data\n",
    "# example_x = np.random.rand(1000, 40, 10)\n",
    "# np.min(example_x)\n",
    "# np.max(example_x)\n",
    "# np.mean(example_x)\n",
    "# example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)\n",
    "## PRODIGY AI HOCKUS POCKUS START\n",
    "\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "home = str(Path.home())\n",
    "file_name = \"esugj36b.h5\"\n",
    "wandb.config.update({'dataset': file_name})\n",
    "path = home + \"/ProdigyAI/data/preprocessed/\" + file_name\n",
    "h5f = h5py.File(path, \"r\")\n",
    "\n",
    "prices_for_window_index_array_train = h5f[\n",
    "    \"prices_for_window_index_array_train\"][:]\n",
    "\n",
    "prices_for_window_index_array_val = h5f[\"prices_for_window_index_array_val\"][:]\n",
    "\n",
    "prices_for_window_index_array_test = h5f[\n",
    "    \"prices_for_window_index_array_test\"][:]\n",
    "\n",
    "close_index_array_train = h5f[\"close_index_array_train\"][:]\n",
    "close_index_array_val = h5f[\"close_index_array_val\"][:]\n",
    "close_index_array_test = h5f[\"close_index_array_test\"][:]\n",
    "input_features_normalized_train = h5f[\"input_features_normalized_train\"][:]\n",
    "input_features_normalized_val = h5f[\"input_features_normalized_val\"][:]\n",
    "input_features_normalized_test = h5f[\"input_features_normalized_test\"][:]\n",
    "y_train = h5f[\"y_train\"][:]\n",
    "y_val = h5f[\"y_val\"][:]\n",
    "y_test = h5f[\"y_test\"][:]\n",
    "\n",
    "h5f.close()\n",
    "## PRODIGY AI HOCKUS POCKUS END\n",
    "\n",
    "for i in range(len(close_index_array_train)):\n",
    "    if close_index_array_train[i] == prices_for_window_index_array_train[0]:\n",
    "        offset = i\n",
    "        break\n",
    "\n",
    "checkpoint_path = os.path.join(wandb.run.dir, \"cp.ckpt\")\n",
    "\n",
    "@njit\n",
    "def generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                     input_features_normalized):\n",
    "    X = np.empty((batch_size, dim[1], dim[0]))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        for k in prange(len(input_features_normalized)):\n",
    "            for l in prange(dim[0]):\n",
    "                X[i][k][l] = input_features_normalized[k][ID + l]\n",
    "    # X[i, ] = self.input_features_normalized[ID+self.window_length, ]\n",
    "    return X\n",
    "\n",
    "@njit\n",
    "def generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data):\n",
    "    y = np.empty((batch_size, n_classes))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        y[i, :] = y_data[ID]\n",
    "    return y\n",
    "\n",
    "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
    "    def __init__(self,\n",
    "                 checkpoint_path,\n",
    "                 prices_for_window_index_array,\n",
    "                 input_features_normalized,\n",
    "                 y_data,\n",
    "                 batch_size,\n",
    "                 dim,\n",
    "                 n_classes,\n",
    "                 to_fit,\n",
    "                 shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.prices_for_window_index_array = prices_for_window_index_array\n",
    "        self.input_features_normalized = input_features_normalized\n",
    "        self.labels = y_data\n",
    "        self.y_data = y_data\n",
    "        self.to_fit = to_fit\n",
    "        self.n_classes = n_classes\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.n = 0\n",
    "        self.list_IDs = np.arange(len(self.prices_for_window_index_array))\n",
    "        self.on_epoch_end()\n",
    "    def __next__(self):\n",
    "        # Get one batch of data\n",
    "        data = self.__getitem__(self.n)\n",
    "        # Batch index\n",
    "        self.n += 1\n",
    "        # If we have processed the entire dataset then\n",
    "        if self.n >= self.__len__():\n",
    "            self.on_epoch_end\n",
    "            self.n = 0\n",
    "        return data\n",
    "    def __len__(self):\n",
    "        # Return the number of batches of the dataset\n",
    "        return math.ceil(len(self.indexes) / self.batch_size)\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                               self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X = self._generate_x(list_IDs_temp)\n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(list_IDs_temp)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    def on_epoch_end(self):\n",
    "        wandb.save(self.checkpoint_path)\n",
    "        self.indexes = np.arange(len(self.prices_for_window_index_array))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def _generate_x(self, list_IDs_temp):\n",
    "        dim = self.dim\n",
    "        batch_size = self.batch_size\n",
    "        input_features_normalized = self.input_features_normalized\n",
    "        X = generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                             input_features_normalized)\n",
    "        # X[i, ] = self.input_features_normalized[ID+self.window_length, ]\n",
    "        return X\n",
    "    def _generate_y(self, list_IDs_temp):\n",
    "        batch_size = self.batch_size\n",
    "        n_classes = self.n_classes\n",
    "        y_data = self.y_data\n",
    "        y = generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data)\n",
    "        return y\n",
    "\n",
    "n_classes = 3\n",
    "dim = (window_length, num_features)\n",
    "\n",
    "to_fit = True\n",
    "# for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,\n",
    "#                                       input_features_normalized_train,\n",
    "#                                       y_train,\n",
    "#                                       batch_size,\n",
    "#                                       dim,\n",
    "#                                       n_classes,\n",
    "#                                       to_fit,\n",
    "#                                       shuffle=True):\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1\n",
    "\n",
    "template = [[num_features, window_length], [60, 10], [120, 5], [3, 1]]\n",
    "# get Bilinear model\n",
    "\n",
    "projection_regularizer = None\n",
    "projection_constraint = tensorflow.keras.constraints.max_norm(3.0, axis=0)\n",
    "attention_regularizer = None\n",
    "attention_constraint = tensorflow.keras.constraints.max_norm(5.0, axis=1)\n",
    "\n",
    "model = Models.TABL(\n",
    "    template,\n",
    "    dropout,\n",
    "    projection_regularizer,\n",
    "    projection_constraint,\n",
    "    attention_regularizer,\n",
    "    attention_constraint,\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "# path = path_adjust + \"data/lob_2010/train_and_test_tabl.h5\"\n",
    "# h5f = h5py.File(path, \"r\")\n",
    "# trainX_CNN = h5f[\"trainX_CNN\"][:]\n",
    "# trainY_CNN = h5f[\"trainY_CNN\"][:]\n",
    "# testX_CNN = h5f[\"testX_CNN\"][:]\n",
    "# testY_CNN = h5f[\"testY_CNN\"][:]\n",
    "# h5f.close()\n",
    "# # train one epoch to find the learning rate\n",
    "# model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     batch_size=256,\n",
    "#     epochs=1,\n",
    "#     shuffle=False,\n",
    "# )  # no class weight\n",
    "# Model configuration\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "loss_function = categorical_crossentropy\n",
    "# no_epochs = 5\n",
    "# start_lr = 0.0001\n",
    "# end_lr = 1\n",
    "# moving_average = 20\n",
    "# # Determine tests you want to perform\n",
    "# tests = [\n",
    "#     (\"sgd\", \"SGD optimizer\"),\n",
    "#     (\"adam\", \"Adam optimizer\"),\n",
    "#     (\"rmsprop\", \"RMS Prop optimizer\"),\n",
    "# ]\n",
    "# # Set containers for tests\n",
    "# test_learning_rates = []\n",
    "# test_losses = []\n",
    "# test_loss_changes = []\n",
    "# labels = []\n",
    "# # Perform each test\n",
    "# for test_optimizer, label in tests:\n",
    "#     # Compile the model\n",
    "#     model.compile(loss=loss_function,\n",
    "#                   optimizer=test_optimizer,\n",
    "#                   metrics=[\"accuracy\"])\n",
    "#     # Instantiate the Learning Rate Range Test / LR Finder\n",
    "#     lr_finder = LRFinder(model)\n",
    "#     # Perform the Learning Rate Range Test\n",
    "#     outputs = lr_finder.find(\n",
    "#         trainX_CNN,\n",
    "#         trainY_CNN,\n",
    "#         start_lr=start_lr,\n",
    "#         end_lr=end_lr,\n",
    "#         batch_size=batch_size,\n",
    "#         epochs=no_epochs,\n",
    "#     )\n",
    "#     # Get values\n",
    "#     learning_rates = lr_finder.lrs\n",
    "#     losses = lr_finder.losses\n",
    "#     loss_changes = []\n",
    "#     # Compute smoothed loss changes\n",
    "#     # Inspired by Keras LR Finder: https://github.com/surmenok/keras_lr_finder/blob/master/keras_lr_finder/lr_finder.py\n",
    "#     for i in range(moving_average, len(learning_rates)):\n",
    "#         loss_changes.append(\n",
    "#             (losses[i] - losses[i - moving_average]) / moving_average)\n",
    "#     # Append values to container\n",
    "#     test_learning_rates.append(learning_rates)\n",
    "#     test_losses.append(losses)\n",
    "#     test_loss_changes.append(loss_changes)\n",
    "#     labels.append(label)\n",
    "# # Generate plot for Loss Deltas\n",
    "# for i in range(0, len(test_learning_rates)):\n",
    "#     plt.plot(test_learning_rates[i][moving_average:],\n",
    "#              test_loss_changes[i],\n",
    "#              label=labels[i])\n",
    "# plt.xscale(\"log\")\n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.ylabel(\"loss delta\")\n",
    "# plt.xlabel(\"learning rate (log scale)\")\n",
    "# plt.title(\n",
    "#     \"Results for Learning Rate Range Test / Loss Deltas for Learning Rate\")\n",
    "# plt.show()\n",
    "# # Generate plot for Loss Values\n",
    "# for i in range(0, len(test_learning_rates)):\n",
    "#     plt.plot(test_learning_rates[i], test_losses[i], label=labels[i])\n",
    "# plt.xscale(\"log\")\n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.ylabel(\"loss\")\n",
    "# plt.xlabel(\"learning rate (log scale)\")\n",
    "# plt.title(\n",
    "#     \"Results for Learning Rate Range Test / Loss Values for Learning Rate\")\n",
    "# plt.show()\n",
    "# # Configuration settings for LR finder\n",
    "# start_lr = 1e-4\n",
    "# end_lr = 1e0\n",
    "# no_epochs = 5\n",
    "# ##\n",
    "# ## LR Finder specific code\n",
    "# ##\n",
    "# optimizer = keras.optimizers.RMSprop()\n",
    "# # Compile the model\n",
    "# model.compile(loss=loss_function, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# # Define LR finder callback\n",
    "# from third_party_libraries.LRFT.keras_callbacks import LRFinder\n",
    "# lr_finder = LRFinder(min_lr=start_lr, max_lr=end_lr)\n",
    "# # Perform LR finder\n",
    "# model.fit(\n",
    "#     trainX_CNN,\n",
    "#     trainY_CNN,\n",
    "#     batch_size=batch_size,\n",
    "#     callbacks=[lr_finder],\n",
    "#     epochs=no_epochs,\n",
    "# )\n",
    "# print(\"learning rate found running cyclical learning\")\n",
    "# ### cyclical learning rate\n",
    "# Set CLR options\n",
    "\n",
    "clr_step_size = int(4 *\n",
    "                    (len(prices_for_window_index_array_train) / batch_size))\n",
    "\n",
    "base_lr = 1e-4\n",
    "max_lr = 1e-2\n",
    "\n",
    "mode = \"triangular\"\n",
    "# Define the callback\n",
    "\n",
    "clr = CyclicLR(base_lr=base_lr,\n",
    "               max_lr=max_lr,\n",
    "               step_size=clr_step_size,\n",
    "               mode=mode)\n",
    "\n",
    "optimizer = tensorflow.keras.optimizers.Adam()\n",
    "# Compile the model\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# Create a callback that saves the model's weights\n",
    "\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                              save_weights_only=True,\n",
    "                              verbose=1)\n",
    "\n",
    "check_point_file = Path(checkpoint_path)\n",
    "\n",
    "if check_point_file.exists() and resuming == \"resuming\":\n",
    "    print(\"weights loaded\")\n",
    "    model.load_weights(checkpoint_path)\n",
    "# Fit data to model\n",
    "# history = model.fit(trainX_CNN,\n",
    "#                     trainY_CNN,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=20,\n",
    "#                     callbacks=[clr, cp_callback])\n",
    "\n",
    "finished_weights_path = path_adjust + \"temp/cp_end.ckpt\"\n",
    "\n",
    "model.save_weights(finished_weights_path)\n",
    "# create class weight\n",
    "# class_weight = {0: 1e6 / 300.0, 1: 1e6 / 400.0, 2: 1e6 / 300.0}\n",
    "\n",
    "train_generator = DataGenerator(checkpoint_path,\n",
    "                                prices_for_window_index_array_train,\n",
    "                                input_features_normalized_train,\n",
    "                                y_train,\n",
    "                                batch_size,\n",
    "                                dim,\n",
    "                                n_classes,\n",
    "                                to_fit,\n",
    "                                shuffle=True)\n",
    "\n",
    "val_generator = DataGenerator(checkpoint_path,\n",
    "                              prices_for_window_index_array_val,\n",
    "                              input_features_normalized_val,\n",
    "                              y_val,\n",
    "                              batch_size,\n",
    "                              dim,\n",
    "                              n_classes,\n",
    "                              to_fit,\n",
    "                              shuffle=True)\n",
    "\n",
    "steps_per_epoch = len(train_generator)\n",
    "\n",
    "validation_steps = len(val_generator)\n",
    "# example sata training\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[cp_callback,\n",
    "               WandbCallback(save_model=True, monitor=\"loss\")])\n",
    "\n",
    "model.save(os.path.join(wandb.run.dir, \"model.h5\"))\n",
    "# no class weight\n",
    "## lob FI-2010 DATA PREPERATION\n",
    "# The first 40 columns of the FI-2010 dataset are 10 levels ask and bid information\n",
    "# for a limit order book and we only use these 40 features in our network.\n",
    "# The last 5 columns of the FI-2010 dataset are the labels with different prediction horizons.\n",
    "# def prepare_x(data):\n",
    "#     df1 = data[:40, :].T\n",
    "#     return np.array(df1)\n",
    "# def get_label(data):\n",
    "#     lob = data[-5:, :].T\n",
    "#     return lob\n",
    "# def data_classification(X, Y, T):\n",
    "#     [N, D] = X.shape\n",
    "#     df = np.array(X)\n",
    "#     dY = np.array(Y)\n",
    "#     dataY = dY[T - 1 : N]\n",
    "#     dataX = np.zeros((N - T + 1, T, D))\n",
    "#     for i in range(T, N + 1):\n",
    "#         dataX[i - T] = df[i - T : i, :]\n",
    "#     return dataX.reshape(dataX.shape + (1,)), dataY\n",
    "# # please change the data_path to your local path\n",
    "# data_path = (\n",
    "#     \"../third_party_libraries/gam_rhn/95-FI2010/data/BenchmarkDatasets/NoAuction\"\n",
    "# )\n",
    "# dec_train = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Training/Train_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "# )\n",
    "# dec_test1 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "# )\n",
    "# dec_test2 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_8.txt\"\n",
    "# )\n",
    "# dec_test3 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_9.txt\"\n",
    "# )\n",
    "# dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "# # extract limit order book data from the FI-2010 dataset\n",
    "# train_lob = prepare_x(dec_train)\n",
    "# test_lob = prepare_x(dec_test)\n",
    "# # extract label from the FI-2010 dataset\n",
    "# train_label = get_label(dec_train)\n",
    "# test_label = get_label(dec_test)\n",
    "# # prepare training data. We feed past 100 observations into our algorithms and choose the prediction horizon.\n",
    "# trainX_CNN, trainY_CNN = data_classification(train_lob, train_label, T=10)\n",
    "# trainY_CNN = trainY_CNN[:, 3] - 1\n",
    "# trainY_CNN = np_utils.to_categorical(trainY_CNN, 3)\n",
    "# # trainX_CNN.shape (254651, 100, 40, 1)\n",
    "# # prepare test data.\n",
    "# testX_CNN, testY_CNN = data_classification(test_lob, test_label, T=10)\n",
    "# testY_CNN = testY_CNN[:, 3] - 1\n",
    "# testY_CNN = np_utils.to_categorical(testY_CNN, 3)\n",
    "# trainX_CNN = np.swapaxes(trainX_CNN, 1, 2)\n",
    "# trainX_CNN = trainX_CNN.reshape(\n",
    "#     trainX_CNN.shape[0], trainX_CNN.shape[1], trainX_CNN.shape[2]\n",
    "# )\n",
    "# testX_CNN = testX_CNN = np.swapaxes(testX_CNN, 1, 2)\n",
    "# testX_CNN = testX_CNN.reshape(\n",
    "#     testX_CNN.shape[0], testX_CNN.shape[1], testX_CNN.shape[2]\n",
    "# )\n",
    "# path = path_adjust + \"data/lob_2010/train_and_test_tabl.h5\"\n",
    "# h5f = h5py.File(path, \"w\")\n",
    "# h5f.create_dataset(\"trainX_CNN\", data=trainX_CNN)\n",
    "# h5f.create_dataset(\"trainY_CNN\", data=trainY_CNN)\n",
    "# h5f.create_dataset(\"testX_CNN\", data=testX_CNN)\n",
    "# h5f.create_dataset(\"testY_CNN\", data=testY_CNN)\n",
    "# h5f.close()\n",
    "# fi-02010 data\n",
    "# balancing seems to help abit (0.64 accuracy after 950 epochs), next try norm 0,1 rather than -1,1\n",
    "# after applying normalization not sure if ill need this\n",
    "# two_d_X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "# two_d_X_val = X_val.reshape(X_val.shape[0], X_val.shape[1] * X_val.shape[2])\n",
    "# oversample = SMOTE()\n",
    "# X_train, y_train = oversample.fit_resample(two_d_X_train, y_train)\n",
    "# X_val, y_val = oversample.fit_resample(two_d_X_val, y_val)\n",
    "# X_train = X_train.reshape((X_train.shape[0], 40, 10))\n",
    "# X_val = X_val.reshape((X_val.shape[0], 40, 10))\n",
    "# train one epoch to find the learning rate\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=256,\n",
    "    epochs=1,\n",
    "    shuffle=False,\n",
    ")  # no class weight\n",
    "\n",
    "score = model.evaluate(x=X_test, y=y_test, batch_size=256)\n",
    "# Save model to wandb\n",
    "# print(score)\n",
    "# score on deeplob fi-2010 loss: 0.7469 - acc: 0.6760 - val_loss: 0.6772 - val_acc: 0.7248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"script initiated\")\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "cwd = os.getcwd()\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "sys.path.append(home + \"/ProdigyAI\")\n",
    "from numba import njit, prange\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "from third_party_libraries.TABL import Models\n",
    "from third_party_libraries.keras_lr_finder.lr_finder import LRFinder\n",
    "from third_party_libraries.CLR.clr_callbacks import CyclicLR\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# Init wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import yaml\n",
    "# import tensorflow.keras as keras\n",
    "# import tensorflow.keras.layers as layers\n",
    "# import tensorflow.keras.backend as K\n",
    "# Sorting out whether we are using the ipython kernel or not\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    check_if_ipython = True\n",
    "    path_adjust = \"../\"\n",
    "    resuming = \"NA\"\n",
    "except Exception as e:\n",
    "    check_if_ipython = False\n",
    "    split_cwd = cwd.split(\"/\")\n",
    "    last_string = split_cwd.pop(-1)\n",
    "    cwd = cwd.replace(last_string, \"\")\n",
    "    os.chdir(cwd)\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocessing\")\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--stage\",\n",
    "                        type=str,\n",
    "                        help=\"Stage of Preprocesssing\")\n",
    "    parser.add_argument(\"-f\",\n",
    "                        \"--resuming\",\n",
    "                        type=str,\n",
    "                        help=\"Is this a continuation of preempted instance?\")\n",
    "    parser.add_argument(\"-m\",\n",
    "                        \"--model\",\n",
    "                        type=str,\n",
    "                        help=\"one_model or two_model\")\n",
    "    args = parser.parse_args()\n",
    "    if args.resuming != None:\n",
    "        resuming = args.resuming\n",
    "    else:\n",
    "        resuming = \"NA\"\n",
    "    if args.stage != None:\n",
    "        arg_parse_stage = 1\n",
    "        if int(args.stage) == 1:\n",
    "            if os.path.exists(path_adjust + \"temp/data_name_gpu.txt\"):\n",
    "                os.remove(path_adjust + \"temp/data_name_gpu.txt\")\n",
    "                print(\"removed temp/data_name_gpu.txt\")\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "    if args.model != None:\n",
    "        model = args.model\n",
    "    path_adjust = \"\"\n",
    "\n",
    "if cwd == home + \"/\":\n",
    "    cwd = cwd + \"/ProdigyAI\"\n",
    "    path_adjust = cwd\n",
    "\n",
    "yaml_path = path_adjust + \"yaml/tabl.yaml\"\n",
    "\n",
    "with open(yaml_path) as file:\n",
    "    yaml_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dictionary = dict(yaml=yaml_path, params=yaml_dict)\n",
    "\n",
    "wandb.init(dir=\"~/ProdigyAI/\",\n",
    "           project=\"prodigyai\",\n",
    "           config=config_dictionary,\n",
    "           resume=True)\n",
    "\n",
    "window_length = wandb.config['params']['window_length']['value']\n",
    "num_features = wandb.config['params']['num_features']['value']\n",
    "epochs = wandb.config['params']['epochs']['value']\n",
    "batch_size = wandb.config['params']['batch_size']['value']\n",
    "\n",
    "dropout = wandb.config['params']['dropout']['value']\n",
    "# # try with 1000 samples, 10 periods and then also with 0,1 normalization and balanced classes\n",
    "# # example data\n",
    "# example_x = np.random.rand(1000, 40, 10)\n",
    "# np.min(example_x)\n",
    "# np.max(example_x)\n",
    "# np.mean(example_x)\n",
    "# example_y = keras.utils.to_categorical(np.random.randint(0, 3, (1000, )), 3)\n",
    "## PRODIGY AI HOCKUS POCKUS START\n",
    "\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "home = str(Path.home())\n",
    "file_name = \"esugj36b.h5\"\n",
    "wandb.config.update({'dataset': file_name})\n",
    "path = home + \"/ProdigyAI/data/preprocessed/\" + file_name\n",
    "h5f = h5py.File(path, \"r\")\n",
    "\n",
    "prices_for_window_index_array_train = h5f[\n",
    "    \"prices_for_window_index_array_train\"][:]\n",
    "\n",
    "prices_for_window_index_array_val = h5f[\"prices_for_window_index_array_val\"][:]\n",
    "\n",
    "prices_for_window_index_array_test = h5f[\n",
    "    \"prices_for_window_index_array_test\"][:]\n",
    "\n",
    "close_index_array_train = h5f[\"close_index_array_train\"][:]\n",
    "close_index_array_val = h5f[\"close_index_array_val\"][:]\n",
    "close_index_array_test = h5f[\"close_index_array_test\"][:]\n",
    "input_features_normalized_train = h5f[\"input_features_normalized_train\"][:]\n",
    "input_features_normalized_val = h5f[\"input_features_normalized_val\"][:]\n",
    "input_features_normalized_test = h5f[\"input_features_normalized_test\"][:]\n",
    "y_train = h5f[\"y_train\"][:]\n",
    "y_val = h5f[\"y_val\"][:]\n",
    "y_test = h5f[\"y_test\"][:]\n",
    "\n",
    "h5f.close()\n",
    "## PRODIGY AI HOCKUS POCKUS END\n",
    "\n",
    "for i in range(len(close_index_array_train)):\n",
    "    if close_index_array_train[i] == prices_for_window_index_array_train[0]:\n",
    "        offset = i\n",
    "        break\n",
    "\n",
    "checkpoint_path = os.path.join(wandb.run.dir, \"cp.ckpt\")\n",
    "\n",
    "@njit\n",
    "def generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                     input_features_normalized):\n",
    "    X = np.empty((batch_size, dim[1], dim[0]))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        for k in prange(len(input_features_normalized)):\n",
    "            for l in prange(dim[0]):\n",
    "                X[i][k][l] = input_features_normalized[k][ID + l]\n",
    "    # X[i, ] = self.input_features_normalized[ID+self.window_length, ]\n",
    "    return X\n",
    "\n",
    "@njit\n",
    "def generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data):\n",
    "    y = np.empty((batch_size, n_classes))\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        y[i, :] = y_data[ID]\n",
    "    return y\n",
    "\n",
    "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
    "    def __init__(self,\n",
    "                 checkpoint_path,\n",
    "                 prices_for_window_index_array,\n",
    "                 input_features_normalized,\n",
    "                 y_data,\n",
    "                 batch_size,\n",
    "                 dim,\n",
    "                 n_classes,\n",
    "                 to_fit,\n",
    "                 shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.prices_for_window_index_array = prices_for_window_index_array\n",
    "        self.input_features_normalized = input_features_normalized\n",
    "        self.labels = y_data\n",
    "        self.y_data = y_data\n",
    "        self.to_fit = to_fit\n",
    "        self.n_classes = n_classes\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.n = 0\n",
    "        self.list_IDs = np.arange(len(self.prices_for_window_index_array))\n",
    "        self.on_epoch_end()\n",
    "    def __next__(self):\n",
    "        # Get one batch of data\n",
    "        data = self.__getitem__(self.n)\n",
    "        # Batch index\n",
    "        self.n += 1\n",
    "        # If we have processed the entire dataset then\n",
    "        if self.n >= self.__len__():\n",
    "            self.on_epoch_end\n",
    "            self.n = 0\n",
    "        return data\n",
    "    def __len__(self):\n",
    "        # Return the number of batches of the dataset\n",
    "        return math.ceil(len(self.indexes) / self.batch_size)\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                               self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X = self._generate_x(list_IDs_temp)\n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(list_IDs_temp)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    def on_epoch_end(self):\n",
    "        wandb.save(self.checkpoint_path)\n",
    "        self.indexes = np.arange(len(self.prices_for_window_index_array))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def _generate_x(self, list_IDs_temp):\n",
    "        dim = self.dim\n",
    "        batch_size = self.batch_size\n",
    "        input_features_normalized = self.input_features_normalized\n",
    "        X = generate_x_numba(batch_size, dim, list_IDs_temp,\n",
    "                             input_features_normalized)\n",
    "        # X[i, ] = self.input_features_normalized[ID+self.window_length, ]\n",
    "        return X\n",
    "    def _generate_y(self, list_IDs_temp):\n",
    "        batch_size = self.batch_size\n",
    "        n_classes = self.n_classes\n",
    "        y_data = self.y_data\n",
    "        y = generate_y_numba(batch_size, n_classes, list_IDs_temp, y_data)\n",
    "        return y\n",
    "\n",
    "n_classes = 3\n",
    "dim = (window_length, num_features)\n",
    "\n",
    "to_fit = True\n",
    "# for X_batch, y_batch in DataGenerator(prices_for_window_index_array_train,\n",
    "#                                       input_features_normalized_train,\n",
    "#                                       y_train,\n",
    "#                                       batch_size,\n",
    "#                                       dim,\n",
    "#                                       n_classes,\n",
    "#                                       to_fit,\n",
    "#                                       shuffle=True):\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "# 1 hidden layer network with input: 40x10, hidden 120x5, output 3x1\n",
    "\n",
    "template = [[num_features, window_length], [60, 10], [120, 5], [3, 1]]\n",
    "# get Bilinear model\n",
    "\n",
    "projection_regularizer = None\n",
    "projection_constraint = tensorflow.keras.constraints.max_norm(3.0, axis=0)\n",
    "attention_regularizer = None\n",
    "attention_constraint = tensorflow.keras.constraints.max_norm(5.0, axis=1)\n",
    "\n",
    "model = Models.TABL(\n",
    "    template,\n",
    "    dropout,\n",
    "    projection_regularizer,\n",
    "    projection_constraint,\n",
    "    attention_regularizer,\n",
    "    attention_constraint,\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "# path = path_adjust + \"data/lob_2010/train_and_test_tabl.h5\"\n",
    "# h5f = h5py.File(path, \"r\")\n",
    "# trainX_CNN = h5f[\"trainX_CNN\"][:]\n",
    "# trainY_CNN = h5f[\"trainY_CNN\"][:]\n",
    "# testX_CNN = h5f[\"testX_CNN\"][:]\n",
    "# testY_CNN = h5f[\"testY_CNN\"][:]\n",
    "# h5f.close()\n",
    "# # train one epoch to find the learning rate\n",
    "# model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     batch_size=256,\n",
    "#     epochs=1,\n",
    "#     shuffle=False,\n",
    "# )  # no class weight\n",
    "# Model configuration\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "loss_function = categorical_crossentropy\n",
    "# no_epochs = 5\n",
    "# start_lr = 0.0001\n",
    "# end_lr = 1\n",
    "# moving_average = 20\n",
    "# # Determine tests you want to perform\n",
    "# tests = [\n",
    "#     (\"sgd\", \"SGD optimizer\"),\n",
    "#     (\"adam\", \"Adam optimizer\"),\n",
    "#     (\"rmsprop\", \"RMS Prop optimizer\"),\n",
    "# ]\n",
    "# # Set containers for tests\n",
    "# test_learning_rates = []\n",
    "# test_losses = []\n",
    "# test_loss_changes = []\n",
    "# labels = []\n",
    "# # Perform each test\n",
    "# for test_optimizer, label in tests:\n",
    "#     # Compile the model\n",
    "#     model.compile(loss=loss_function,\n",
    "#                   optimizer=test_optimizer,\n",
    "#                   metrics=[\"accuracy\"])\n",
    "#     # Instantiate the Learning Rate Range Test / LR Finder\n",
    "#     lr_finder = LRFinder(model)\n",
    "#     # Perform the Learning Rate Range Test\n",
    "#     outputs = lr_finder.find(\n",
    "#         trainX_CNN,\n",
    "#         trainY_CNN,\n",
    "#         start_lr=start_lr,\n",
    "#         end_lr=end_lr,\n",
    "#         batch_size=batch_size,\n",
    "#         epochs=no_epochs,\n",
    "#     )\n",
    "#     # Get values\n",
    "#     learning_rates = lr_finder.lrs\n",
    "#     losses = lr_finder.losses\n",
    "#     loss_changes = []\n",
    "#     # Compute smoothed loss changes\n",
    "#     # Inspired by Keras LR Finder: https://github.com/surmenok/keras_lr_finder/blob/master/keras_lr_finder/lr_finder.py\n",
    "#     for i in range(moving_average, len(learning_rates)):\n",
    "#         loss_changes.append(\n",
    "#             (losses[i] - losses[i - moving_average]) / moving_average)\n",
    "#     # Append values to container\n",
    "#     test_learning_rates.append(learning_rates)\n",
    "#     test_losses.append(losses)\n",
    "#     test_loss_changes.append(loss_changes)\n",
    "#     labels.append(label)\n",
    "# # Generate plot for Loss Deltas\n",
    "# for i in range(0, len(test_learning_rates)):\n",
    "#     plt.plot(test_learning_rates[i][moving_average:],\n",
    "#              test_loss_changes[i],\n",
    "#              label=labels[i])\n",
    "# plt.xscale(\"log\")\n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.ylabel(\"loss delta\")\n",
    "# plt.xlabel(\"learning rate (log scale)\")\n",
    "# plt.title(\n",
    "#     \"Results for Learning Rate Range Test / Loss Deltas for Learning Rate\")\n",
    "# plt.show()\n",
    "# # Generate plot for Loss Values\n",
    "# for i in range(0, len(test_learning_rates)):\n",
    "#     plt.plot(test_learning_rates[i], test_losses[i], label=labels[i])\n",
    "# plt.xscale(\"log\")\n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.ylabel(\"loss\")\n",
    "# plt.xlabel(\"learning rate (log scale)\")\n",
    "# plt.title(\n",
    "#     \"Results for Learning Rate Range Test / Loss Values for Learning Rate\")\n",
    "# plt.show()\n",
    "# # Configuration settings for LR finder\n",
    "# start_lr = 1e-4\n",
    "# end_lr = 1e0\n",
    "# no_epochs = 5\n",
    "# ##\n",
    "# ## LR Finder specific code\n",
    "# ##\n",
    "# optimizer = keras.optimizers.RMSprop()\n",
    "# # Compile the model\n",
    "# model.compile(loss=loss_function, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# # Define LR finder callback\n",
    "# from third_party_libraries.LRFT.keras_callbacks import LRFinder\n",
    "# lr_finder = LRFinder(min_lr=start_lr, max_lr=end_lr)\n",
    "# # Perform LR finder\n",
    "# model.fit(\n",
    "#     trainX_CNN,\n",
    "#     trainY_CNN,\n",
    "#     batch_size=batch_size,\n",
    "#     callbacks=[lr_finder],\n",
    "#     epochs=no_epochs,\n",
    "# )\n",
    "# print(\"learning rate found running cyclical learning\")\n",
    "# ### cyclical learning rate\n",
    "# Set CLR options\n",
    "\n",
    "clr_step_size = int(4 *\n",
    "                    (len(prices_for_window_index_array_train) / batch_size))\n",
    "\n",
    "base_lr = 1e-4\n",
    "max_lr = 1e-2\n",
    "\n",
    "mode = \"triangular\"\n",
    "# Define the callback\n",
    "\n",
    "clr = CyclicLR(base_lr=base_lr,\n",
    "               max_lr=max_lr,\n",
    "               step_size=clr_step_size,\n",
    "               mode=mode)\n",
    "\n",
    "optimizer = tensorflow.keras.optimizers.Adam()\n",
    "# Compile the model\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# Create a callback that saves the model's weights\n",
    "\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                              save_weights_only=True,\n",
    "                              verbose=1)\n",
    "\n",
    "check_point_file = Path(checkpoint_path)\n",
    "\n",
    "if check_point_file.exists() and resuming == \"resuming\":\n",
    "    print(\"weights loaded\")\n",
    "    model.load_weights(checkpoint_path)\n",
    "# Fit data to model\n",
    "# history = model.fit(trainX_CNN,\n",
    "#                     trainY_CNN,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=20,\n",
    "#                     callbacks=[clr, cp_callback])\n",
    "\n",
    "finished_weights_path = path_adjust + \"temp/cp_end.ckpt\"\n",
    "\n",
    "model.save_weights(finished_weights_path)\n",
    "# create class weight\n",
    "# class_weight = {0: 1e6 / 300.0, 1: 1e6 / 400.0, 2: 1e6 / 300.0}\n",
    "\n",
    "train_generator = DataGenerator(checkpoint_path,\n",
    "                                prices_for_window_index_array_train,\n",
    "                                input_features_normalized_train,\n",
    "                                y_train,\n",
    "                                batch_size,\n",
    "                                dim,\n",
    "                                n_classes,\n",
    "                                to_fit,\n",
    "                                shuffle=True)\n",
    "\n",
    "val_generator = DataGenerator(checkpoint_path,\n",
    "                              prices_for_window_index_array_val,\n",
    "                              input_features_normalized_val,\n",
    "                              y_val,\n",
    "                              batch_size,\n",
    "                              dim,\n",
    "                              n_classes,\n",
    "                              to_fit,\n",
    "                              shuffle=True)\n",
    "\n",
    "steps_per_epoch = len(train_generator)\n",
    "\n",
    "validation_steps = len(val_generator)\n",
    "# example sata training\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[cp_callback,\n",
    "               WandbCallback(save_model=True, monitor=\"loss\")])\n",
    "\n",
    "model.save(os.path.join(wandb.run.dir, \"model.h5\"))\n",
    "# no class weight\n",
    "## lob FI-2010 DATA PREPERATION\n",
    "# The first 40 columns of the FI-2010 dataset are 10 levels ask and bid information\n",
    "# for a limit order book and we only use these 40 features in our network.\n",
    "# The last 5 columns of the FI-2010 dataset are the labels with different prediction horizons.\n",
    "# def prepare_x(data):\n",
    "#     df1 = data[:40, :].T\n",
    "#     return np.array(df1)\n",
    "# def get_label(data):\n",
    "#     lob = data[-5:, :].T\n",
    "#     return lob\n",
    "# def data_classification(X, Y, T):\n",
    "#     [N, D] = X.shape\n",
    "#     df = np.array(X)\n",
    "#     dY = np.array(Y)\n",
    "#     dataY = dY[T - 1 : N]\n",
    "#     dataX = np.zeros((N - T + 1, T, D))\n",
    "#     for i in range(T, N + 1):\n",
    "#         dataX[i - T] = df[i - T : i, :]\n",
    "#     return dataX.reshape(dataX.shape + (1,)), dataY\n",
    "# # please change the data_path to your local path\n",
    "# data_path = (\n",
    "#     \"../third_party_libraries/gam_rhn/95-FI2010/data/BenchmarkDatasets/NoAuction\"\n",
    "# )\n",
    "# dec_train = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Training/Train_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "# )\n",
    "# dec_test1 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "# )\n",
    "# dec_test2 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_8.txt\"\n",
    "# )\n",
    "# dec_test3 = np.loadtxt(\n",
    "#     data_path\n",
    "#     + \"/3.NoAuction_DecPre/NoAuction_DecPre_Testing/Test_Dst_NoAuction_DecPre_CF_9.txt\"\n",
    "# )\n",
    "# dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "# # extract limit order book data from the FI-2010 dataset\n",
    "# train_lob = prepare_x(dec_train)\n",
    "# test_lob = prepare_x(dec_test)\n",
    "# # extract label from the FI-2010 dataset\n",
    "# train_label = get_label(dec_train)\n",
    "# test_label = get_label(dec_test)\n",
    "# # prepare training data. We feed past 100 observations into our algorithms and choose the prediction horizon.\n",
    "# trainX_CNN, trainY_CNN = data_classification(train_lob, train_label, T=10)\n",
    "# trainY_CNN = trainY_CNN[:, 3] - 1\n",
    "# trainY_CNN = np_utils.to_categorical(trainY_CNN, 3)\n",
    "# # trainX_CNN.shape (254651, 100, 40, 1)\n",
    "# # prepare test data.\n",
    "# testX_CNN, testY_CNN = data_classification(test_lob, test_label, T=10)\n",
    "# testY_CNN = testY_CNN[:, 3] - 1\n",
    "# testY_CNN = np_utils.to_categorical(testY_CNN, 3)\n",
    "# trainX_CNN = np.swapaxes(trainX_CNN, 1, 2)\n",
    "# trainX_CNN = trainX_CNN.reshape(\n",
    "#     trainX_CNN.shape[0], trainX_CNN.shape[1], trainX_CNN.shape[2]\n",
    "# )\n",
    "# testX_CNN = testX_CNN = np.swapaxes(testX_CNN, 1, 2)\n",
    "# testX_CNN = testX_CNN.reshape(\n",
    "#     testX_CNN.shape[0], testX_CNN.shape[1], testX_CNN.shape[2]\n",
    "# )\n",
    "# path = path_adjust + \"data/lob_2010/train_and_test_tabl.h5\"\n",
    "# h5f = h5py.File(path, \"w\")\n",
    "# h5f.create_dataset(\"trainX_CNN\", data=trainX_CNN)\n",
    "# h5f.create_dataset(\"trainY_CNN\", data=trainY_CNN)\n",
    "# h5f.create_dataset(\"testX_CNN\", data=testX_CNN)\n",
    "# h5f.create_dataset(\"testY_CNN\", data=testY_CNN)\n",
    "# h5f.close()\n",
    "# fi-02010 data\n",
    "# balancing seems to help abit (0.64 accuracy after 950 epochs), next try norm 0,1 rather than -1,1\n",
    "# after applying normalization not sure if ill need this\n",
    "# two_d_X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "# two_d_X_val = X_val.reshape(X_val.shape[0], X_val.shape[1] * X_val.shape[2])\n",
    "# oversample = SMOTE()\n",
    "# X_train, y_train = oversample.fit_resample(two_d_X_train, y_train)\n",
    "# X_val, y_val = oversample.fit_resample(two_d_X_val, y_val)\n",
    "# X_train = X_train.reshape((X_train.shape[0], 40, 10))\n",
    "# X_val = X_val.reshape((X_val.shape[0], 40, 10))\n",
    "# train one epoch to find the learning rate\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=256,\n",
    "    epochs=1,\n",
    "    shuffle=False,\n",
    ")  # no class weight\n",
    "\n",
    "score = model.evaluate(x=X_test, y=y_test, batch_size=256)\n",
    "# Save model to wandb\n",
    "# print(score)\n",
    "# score on deeplob fi-2010 loss: 0.7469 - acc: 0.6760 - val_loss: 0.6772 - val_acc: 0.7248"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
